{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d989e9a7-a98e-4aa8-86c6-d784d751a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a4964a3-0ed4-48f3-9364-25dc84b551ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import boolean_argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14fb9ea3-5497-4691-9d97-b97c4f3002fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(rest_args):\n",
    "    \"\"\"PPOAgent.\n",
    "        Parameters:\n",
    "            device: cpu or gpu acelator.\n",
    "            make_env: factory that produce environment.\n",
    "            continuous: True of environments with continuous action space.\n",
    "            obs_dim: dimension od observaion.\n",
    "            act_dim: dimension of action.\n",
    "            gamma: coef for discount factor.\n",
    "            lamda: coef for general adversial estimator (GAE).\n",
    "            entropy_coef: coef of weighting entropy in objective loss.\n",
    "            epsilon: clipping range for actor objective loss.\n",
    "            actor_lr: learnig rate for actor optimizer.\n",
    "            critic_lr: learnig rate for critic optimizer.\n",
    "            value_range: clipping range for critic objective loss.\n",
    "            rollout_len: num t-steps per one rollout.\n",
    "            total_rollouts: num rollouts.\n",
    "            num_epochs: num weights updation iteration for one policy update.\n",
    "            batch_size: data batch size for weights updating\n",
    "            actor: model for predction action.\n",
    "            critic: model for prediction state values.\n",
    "            plot_interval: interval for plotting train history.\n",
    "            solved_reward: desired reward.\n",
    "            plot_interval: plot history log every plot_interval rollouts.\n",
    "            path2save_train_history: path to save training history logs.\n",
    "            \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--env_name', type=str, default='olympics-running', help='name of environment')\n",
    "\n",
    "    # device\n",
    "    parser.add_argument('--device', type=str, default='cpu', help='cpu or gpu acelator')\n",
    "\n",
    "    # coefficients\n",
    "    parser.add_argument('--gamma', type=float, default=0.9, help='coef for discount factor')\n",
    "    parser.add_argument('--lamda', type=float, default=0.95, help='coef for general adversial estimator (GAE)')\n",
    "    parser.add_argument('--entropy_coef', type=float, default=0.005, help='coef for general adversial estimator (GAE)')\n",
    "    parser.add_argument('--epsilon', type=float, default=0.2, help='clipping range for actor objective loss')\n",
    "    parser.add_argument('--value_range', type=float, default=0.5, help='clipping range for critic objective loss')\n",
    "\n",
    "    # other hyperparameters\n",
    "    parser.add_argument('--rollout_len', type=int, default=4000, help='num t-steps per one rollout')\n",
    "    parser.add_argument('--total_rollouts', type=int, default=1000, help='num rollouts')\n",
    "    parser.add_argument('--num_epochs', type=int, default=30,\n",
    "                        help='num weights updation iteration for one policy update')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='data batch size for weights updating')\n",
    "\n",
    "    # agent net\n",
    "    parser.add_argument('--obs_dim', type=tuple, default=(4, 40, 40), help='dimension od observaion')\n",
    "    parser.add_argument('--continuous', type=boolean_argument, default=True,\n",
    "                        help='True of environments with continuous action space')\n",
    "    parser.add_argument('--act_dim', type=int, default=2, help='dimension of action')\n",
    "\n",
    "    # agent nets optimizers\n",
    "    parser.add_argument('--actor_lr', type=float, default=1e-4, help='learning rate for actor optimizer')\n",
    "    parser.add_argument('--critic_lr', type=float, default=5e-4, help='learning rate for actor optimizer')\n",
    "\n",
    "    # etc.\n",
    "    parser.add_argument('--is_evaluate', type=boolean_argument, default=False, help='for evaluation')\n",
    "    parser.add_argument('--solved_reward', type=int, default=-50, help='desired reward')\n",
    "    parser.add_argument('--plot_interval', type=int, default=1, help='interval for plotting train history')\n",
    "    parser.add_argument('--print_episode_interval', type=int, default=10, help='interval for printing train history')\n",
    "\n",
    "    # olympic.\n",
    "    parser.add_argument('--render_over_train', type=boolean_argument, default=True, help='render over train')\n",
    "    parser.add_argument('--controlled_agent_index', type=int, default=1, help='controlled agent index')\n",
    "    parser.add_argument('--frame_stack', type=int, default=4, help='frame stack')\n",
    "    parser.add_argument('--wandb_use', type=boolean_argument, default=False, help='wandb_use')\n",
    "    parser.add_argument('--load_model', type=boolean_argument, default=False, help='load previous model')\n",
    "    parser.add_argument('--load_model_time', type=str, default=\"10_25_15_33\", help='month_day_hour_minute')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491650f-f6e6-4d57-8d02-7e0b965842f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympic",
   "language": "python",
   "name": "olympic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ffec765-5814-4418-9abe-0e959d5d253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ppo.ipynb\n",
      "/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_07_Olymic/PPO\n",
      "importing Jupyter notebook from utils.ipynb\n",
      "/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_07_Olymic/ActorCritic\n",
      "importing Jupyter notebook from cnn.ipynb\n",
      "importing Jupyter notebook from actor.ipynb\n",
      "importing Jupyter notebook from critic.ipynb\n",
      "importing Jupyter notebook from memory.ipynb\n",
      "importing Jupyter notebook from running_args.ipynb\n",
      "before: /Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_07_Olymic/ENV\n",
      "Namespace(env_name='olympics-running', device='cpu', gamma=0.9, lamda=0.95, entropy_coef=0.005, epsilon=0.2, value_range=0.5, rollout_len=400000, total_rollouts=1000, num_epochs=100, batch_size=256, obs_dim=(4, 40, 40), continuous=True, act_dim=2, actor_lr=0.0001, critic_lr=0.0005, is_evaluate=False, solved_reward=500, plot_interval=1, print_episode_interval=10, render_over_train=True, controlled_agent_index=1, frame_stack=4, wandb_use=False, load_model=False, load_model_time='10_25_15_33')\n",
      "importing Jupyter notebook from wrapper.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import import_ipynb\n",
    "import gymnasium as gym\n",
    "\n",
    "os.chdir('/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_07_Olymic/PPO')\n",
    "from ppo import PPOAgent\n",
    "os.chdir('/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_07_Olymic/ENV')\n",
    "import running_args\n",
    "from wrapper import CompetitionOlympicsEnvWrapper\n",
    "os.chdir('/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/termproject_olympic')\n",
    "from env.chooseenv import make\n",
    "\n",
    "DEVICE = None\n",
    "\n",
    "class GlobalConfig:\n",
    "    def __init__(self):\n",
    "        self.seed = 555\n",
    "        self.path2save_train_history = \"train_history\"\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "config = GlobalConfig()\n",
    "seed_everything(config.seed)\n",
    "\n",
    "\n",
    "def make_env(env_name, agent=None, config=None):\n",
    "    # environment\n",
    "    env = make(env_type=\"olympics-integrated\", game_name=env_name)\n",
    "    env = CompetitionOlympicsEnvWrapper(env, args=config)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "def main(args, evaluation=False):\n",
    "    if not evaluation:\n",
    "        # ppo agent\n",
    "        agent = PPOAgent(make_env, args)\n",
    "        # ppo train\n",
    "        agent.train()\n",
    "    else:\n",
    "        agent = PPOAgent(make_env, args)\n",
    "        agent.load_predtrain_model(f\"{args.path2save_train_history}/actor.pth\", f\"{args.path2save_train_history}/critic.pth\")\n",
    "        agent.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8551e4-cba2-4eb2-a558-5159bbbcf97e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_type = 'olympics-integrated'\n",
      "!!olympics-running\n",
      "game_name = 'olympics-running'\n",
      "device: cpu\n",
      "[Episode  10, Steps  3,135] Episode Reward:   174.000, Elapsed Time: 00:00:19\n",
      "[Episode  20, Steps  6,866] Episode Reward:   192.650, Elapsed Time: 00:00:40\n",
      "[Episode  30, Steps  9,943] Episode Reward:   170.067, Elapsed Time: 00:00:57\n",
      "[Episode  40, Steps 13,698] Episode Reward:   169.767, Elapsed Time: 00:01:18\n",
      "[Episode  50, Steps 17,282] Episode Reward: 500161.200, Elapsed Time: 00:01:38\n",
      "[Episode  60, Steps 20,511] Episode Reward: 500168.467, Elapsed Time: 00:01:52\n",
      "[Episode  70, Steps 24,074] Episode Reward: 500165.667, Elapsed Time: 00:02:12\n",
      "[Episode  80, Steps 27,751] Episode Reward:   178.900, Elapsed Time: 00:02:37\n",
      "[Episode  90, Steps 31,181] Episode Reward:   189.933, Elapsed Time: 00:02:56\n",
      "[Episode 100, Steps 34,238] Episode Reward:   198.433, Elapsed Time: 00:03:12\n",
      "[Episode 110, Steps 37,539] Episode Reward:   179.333, Elapsed Time: 00:03:30\n",
      "[Episode 120, Steps 40,867] Episode Reward:   157.833, Elapsed Time: 00:03:46\n",
      "[Episode 130, Steps 44,321] Episode Reward: 266825.067, Elapsed Time: 00:04:06\n",
      "[Episode 140, Steps 47,946] Episode Reward: 266836.600, Elapsed Time: 00:04:28\n",
      "[Episode 150, Steps 51,414] Episode Reward: 266863.200, Elapsed Time: 00:04:49\n",
      "[Episode 160, Steps 55,141] Episode Reward:   209.600, Elapsed Time: 00:05:10\n",
      "[Episode 170, Steps 58,602] Episode Reward:   210.433, Elapsed Time: 00:05:30\n",
      "[Episode 180, Steps 62,126] Episode Reward:   206.033, Elapsed Time: 00:05:50\n",
      "[Episode 190, Steps 65,809] Episode Reward:   193.167, Elapsed Time: 00:06:10\n",
      "[Episode 200, Steps 69,509] Episode Reward:   178.833, Elapsed Time: 00:06:32\n",
      "[Episode 210, Steps 73,285] Episode Reward:   192.467, Elapsed Time: 00:06:57\n",
      "[Episode 220, Steps 77,099] Episode Reward:   182.633, Elapsed Time: 00:07:16\n",
      "[Episode 230, Steps 80,763] Episode Reward:   191.833, Elapsed Time: 00:07:40\n",
      "[Episode 240, Steps 84,194] Episode Reward:   180.600, Elapsed Time: 00:08:03\n",
      "[Episode 250, Steps 87,857] Episode Reward:   190.233, Elapsed Time: 00:08:26\n",
      "[Episode 260, Steps 91,591] Episode Reward:   196.633, Elapsed Time: 00:08:45\n",
      "[Episode 270, Steps 95,039] Episode Reward:   200.000, Elapsed Time: 00:09:04\n",
      "[Episode 280, Steps 98,566] Episode Reward:   190.967, Elapsed Time: 00:09:22\n",
      "[Episode 290, Steps 102,566] Episode Reward:   200.400, Elapsed Time: 00:09:54\n",
      "[Episode 300, Steps 106,082] Episode Reward:   205.867, Elapsed Time: 00:10:14\n",
      "[Episode 310, Steps 109,844] Episode Reward:   216.967, Elapsed Time: 00:10:40\n",
      "[Episode 320, Steps 113,517] Episode Reward:   188.700, Elapsed Time: 00:11:03\n",
      "[Episode 330, Steps 116,714] Episode Reward: 400165.933, Elapsed Time: 00:11:21\n",
      "[Episode 340, Steps 120,498] Episode Reward: 666827.333, Elapsed Time: 00:11:40\n",
      "[Episode 350, Steps 124,169] Episode Reward: 666838.967, Elapsed Time: 00:11:59\n",
      "[Episode 360, Steps 127,786] Episode Reward: 366853.567, Elapsed Time: 00:12:22\n",
      "[Episode 370, Steps 131,786] Episode Reward: 100190.600, Elapsed Time: 00:12:45\n",
      "[Episode 380, Steps 135,683] Episode Reward: 600199.467, Elapsed Time: 00:13:07\n",
      "[Episode 390, Steps 139,373] Episode Reward: 500208.467, Elapsed Time: 00:13:29\n",
      "[Episode 400, Steps 142,943] Episode Reward: 500207.633, Elapsed Time: 00:13:49\n",
      "[Episode 410, Steps 146,881] Episode Reward:   206.500, Elapsed Time: 00:14:15\n",
      "[Episode 420, Steps 150,359] Episode Reward:   197.000, Elapsed Time: 00:14:34\n",
      "[Episode 430, Steps 153,897] Episode Reward:   189.367, Elapsed Time: 00:14:55\n",
      "[Episode 440, Steps 157,435] Episode Reward:   169.867, Elapsed Time: 00:15:19\n",
      "[Episode 450, Steps 161,162] Episode Reward:   171.333, Elapsed Time: 00:15:40\n",
      "[Episode 460, Steps 164,738] Episode Reward: 100178.933, Elapsed Time: 00:15:58\n",
      "[Episode 470, Steps 167,946] Episode Reward: 100188.167, Elapsed Time: 00:16:15\n",
      "[Episode 480, Steps 171,832] Episode Reward: 100199.967, Elapsed Time: 00:16:38\n",
      "[Episode 490, Steps 175,248] Episode Reward:   191.833, Elapsed Time: 00:16:54\n",
      "[Episode 500, Steps 178,931] Episode Reward:   193.533, Elapsed Time: 00:17:14\n",
      "[Episode 510, Steps 182,692] Episode Reward:   175.067, Elapsed Time: 00:17:36\n",
      "[Episode 520, Steps 186,634] Episode Reward:   172.667, Elapsed Time: 00:17:58\n",
      "[Episode 530, Steps 190,362] Episode Reward:   164.967, Elapsed Time: 00:18:17\n",
      "[Episode 540, Steps 194,039] Episode Reward: 233502.100, Elapsed Time: 00:18:38\n",
      "[Episode 550, Steps 197,691] Episode Reward: 233510.800, Elapsed Time: 00:19:06\n",
      "[Episode 560, Steps 201,435] Episode Reward: 233522.100, Elapsed Time: 00:19:28\n",
      "[Episode 570, Steps 205,161] Episode Reward:   195.433, Elapsed Time: 00:19:52\n",
      "[Episode 580, Steps 209,036] Episode Reward:   199.533, Elapsed Time: 00:20:15\n",
      "[Episode 590, Steps 211,807] Episode Reward:   184.600, Elapsed Time: 00:20:30\n",
      "[Episode 600, Steps 215,259] Episode Reward:   187.900, Elapsed Time: 00:26:15\n",
      "[Episode 610, Steps 218,939] Episode Reward:   188.167, Elapsed Time: 00:30:47\n",
      "[Episode 620, Steps 222,574] Episode Reward:   215.167, Elapsed Time: 00:31:24\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = running_args.get_args()\n",
    "    # parser.add_argument(\"--env_type\", default=\"olympics-wrestling\")\n",
    "    # # \"olympics-wrestling\", \"olympics-running\"\n",
    "    # args, rest_args = parser.parse_known_args()\n",
    "    # env_name = args.env_type\n",
    "\n",
    "    # if env_name == \"olympics-running\":\n",
    "    #     args = args_olympic_running.get_args(rest_args)\n",
    "    # elif env_name == \"olympics-wrestling\":\n",
    "    #     args = args_olympic_wrestling.get_args(rest_args)\n",
    "    # elif env_name == \"olympics-integrated\":\n",
    "    #     args = args_olympic_integrated.get_args(rest_args)\n",
    "    # else:\n",
    "    #     raise Exception(\"Invalid Environment\")\n",
    "\n",
    "    args.path2save_train_history = config.path2save_train_history\n",
    "\n",
    "    if not os.path.exists(args.path2save_train_history):\n",
    "        try:\n",
    "            os.mkdir(args.path2save_train_history)\n",
    "        except:\n",
    "            dir_path_head, dir_path_tail = os.path.split(args.path2save_train_history)\n",
    "            if len(dir_path_tail) == 0:\n",
    "                dir_path_head, dir_path_tail = os.path.split(dir_path_head)\n",
    "            os.mkdir(dir_path_head)\n",
    "            os.mkdir(args.path2save_train_history)\n",
    "\n",
    "    main(args, args.is_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9522b-0412-4672-9157-1485936b5c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f3c4a-42a8-4f86-af64-db1992d12c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympic",
   "language": "python",
   "name": "olympic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e9298b-920f-4797-81ae-475aacd8c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5357e8b0-76a1-44ce-8aca-487d60a69df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_07_Olymic/PPO\n",
      "/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_07_Olymic/ActorCritic\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_07_Olymic/PPO')\n",
    "print(os.getcwd())\n",
    "from utils import get_gae, trajectories_data_generator\n",
    "os.chdir('/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_07_Olymic/ActorCritic')\n",
    "print(os.getcwd())\n",
    "from cnn import CnnEncoder\n",
    "from actor import ContinuousActor\n",
    "from critic import Critic\n",
    "from memory import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46e78764-df97-4078-89f9-dbcb595fe786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0facb9-6a31-4e80-838e-014f28d70585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) in (nn.Linear, nn.Conv2d):\n",
    "        nn.init.orthogonal_(m.weight.data, np.sqrt(float(2)))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a982ff6-70fa-4d78-9342-bcd3af5fa03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(object):\n",
    "    \"\"\"PPOAgent.\n",
    "    Parameters:\n",
    "        device: cpu or gpu acelator.\n",
    "        make_env: factory that produce environment.\n",
    "        continuous: True of environments with continuous action space.\n",
    "        obs_dim: dimension od observaion.\n",
    "        act_dim: dimension of action.\n",
    "        gamma: coef for discount factor.\n",
    "        lamda: coef for general adversial estimator (GAE).\n",
    "        entropy_coef: coef of weighting entropy in objective loss.\n",
    "        epsilon: clipping range for actor objective loss.\n",
    "        actor_lr: learnig rate for actor optimizer.\n",
    "        critic_lr: learnig rate for critic optimizer.\n",
    "        value_range: clipping range for critic objective loss.\n",
    "        rollout_len: num t-steps per one rollout.\n",
    "        total_rollouts: num rollouts.\n",
    "        num_epochs: num weights updation iteration for one policy update.\n",
    "        batch_size: data batch size for weights updating\n",
    "        actor: model for predction action.\n",
    "        critic: model for prediction state values.\n",
    "        plot_interval: interval for plotting train history.\n",
    "        solved_reward: desired reward.\n",
    "        plot_interval: plot history log every plot_interval rollouts.\n",
    "        path2save_train_history: path to save training history logs.\n",
    "        \"\"\"\n",
    "    def __init__(self, make_env, args):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        \"\"\"\n",
    "        self.device = args.device\n",
    "        self.env_name = args.env_name\n",
    "        self.env = make_env(args.env_name, config=args)\n",
    "        print(\"device:\", self.device)\n",
    "        # self.env = make_env(args.env_name, args)\n",
    "\n",
    "        # coeffs\n",
    "        self.gamma = args.gamma\n",
    "        self.lamda = args.lamda\n",
    "        self.entropy_coef = args.entropy_coef\n",
    "        self.epsilon = args.epsilon\n",
    "        self.value_range = args.value_range\n",
    "        \n",
    "        # other hyperparameters\n",
    "        self.rollout_len = args.rollout_len\n",
    "        self.total_rollouts = args.total_rollouts\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        # agent nets\n",
    "        self.obs_dim = args.obs_dim\n",
    "        self.encoder = CnnEncoder().apply(init_weights).to(self.device)\n",
    "        self.actor = ContinuousActor(self.encoder, self.device).apply(init_weights).to(self.device)\n",
    "        self.critic = Critic(self.encoder, self.device).apply(init_weights).to(self.device)\n",
    "\n",
    "        # agent nets optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=args.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=args.critic_lr)\n",
    "\n",
    "        # memory of trajectory (s, a, r ...)\n",
    "        self.memory = Memory()\n",
    "\n",
    "        # memory of the train history\n",
    "        self.actor_loss_history = []\n",
    "        self.critic_loss_history = []\n",
    "        self.scores = []\n",
    "\n",
    "        self.is_evaluate = args.is_evaluate\n",
    "        self.solved_reward = args.solved_reward\n",
    "        self.plot_interval = args.plot_interval\n",
    "        self.print_episode_interval = args.print_episode_interval\n",
    "        self.path2save_train_history = args.path2save_train_history\n",
    "\n",
    "        # load model\n",
    "        if args.load_model:\n",
    "            self.actor.load_state_dict(torch.load(\n",
    "                f\"{self.path2save_train_history}/{self.env_name}/{args.load_model_time}/actor.pth\",\n",
    "                map_location=self.device))\n",
    "            self.critic.load_state_dict(torch.load(\n",
    "                f\"{self.path2save_train_history}/{self.env_name}/{args.load_model_time}/critic.pth\",\n",
    "                map_location=self.device))\n",
    "            self.encoder.load_state_dict(torch.load(\n",
    "                f\"{self.path2save_train_history}/{self.env_name}/{args.load_model_time}/encoder.pth\",\n",
    "                map_location=self.device))\n",
    "\n",
    "            print(\"COMPLETE MODEL LOAD!!!!\")\n",
    "\n",
    "        self.load_model_time = args.load_model_time\n",
    "\n",
    "        # wandb\n",
    "        self.wandb_use = args.wandb_use\n",
    "        self.train_flag = False\n",
    "        self.time_step = 0\n",
    "        self.num_episode = 0\n",
    "        self.num_train = 0\n",
    "        self.episode_reward_list = deque(maxlen=30)\n",
    "        self.actor_loss_list = deque(maxlen=30)\n",
    "        self.entropy_loss_list = deque(maxlen=30)\n",
    "        self.critic_loss_list = deque(maxlen=30)\n",
    "        self.ratio_list = deque(maxlen=30)\n",
    "\n",
    "        if self.wandb_use and not self.is_evaluate:\n",
    "            now_time = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "            wandb_name = f\"{args.env_name}_{now_time}\"\n",
    "            self.wandb = wandb.init(\n",
    "                project=f\"{args.env_name}\",\n",
    "                name=wandb_name,\n",
    "                save_code=False\n",
    "            )\n",
    "\n",
    "    def _get_action(self, state: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Get action from actor, and if not test -  \n",
    "        get state value from critic, collect elements of trajectory.\n",
    "        \"\"\"\n",
    "        state = np.array(state)\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "\n",
    "        # print(f\"{state.shape = }, {state.dim() = }, *************************\")\n",
    "\n",
    "        action, dist = self.actor(state)\n",
    "\n",
    "        if not self.is_evaluate:\n",
    "            value = self.critic(state)\n",
    "           \n",
    "            # collect elements of trajectory\n",
    "            self.memory.states.append(state)\n",
    "            self.memory.actions.append(action)\n",
    "            self.memory.log_probs.append(dist.log_prob(action))\n",
    "            self.memory.values.append(value)\n",
    "\n",
    "        return list(action.detach().cpu().numpy()).pop()\n",
    "            \n",
    "    def _step(self, action: float):\n",
    "        \"\"\"\n",
    "        Make action in enviroment chosen by current policy,\n",
    "        if not evaluate - collect elements of trajectory.\n",
    "        \"\"\"\n",
    "        #print(action)\n",
    "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        if any([terminated, truncated]):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # add fake dim to match dimension with batch size\n",
    "        next_state = np.reshape(next_state, (1, 4, 40, 40)).astype(np.float64)\n",
    "        reward = np.reshape(reward, (1, -1)).astype(np.float64)\n",
    "        done = np.reshape(done, (1, -1))\n",
    "\n",
    "        if not self.is_evaluate:\n",
    "            # convert np.ndarray return from enviroment to torch tensor. \n",
    "            # collect elements of trajectory.\n",
    "            reward = np.array(reward)\n",
    "            done_memory = np.array(1 - done)\n",
    "\n",
    "            self.memory.rewards.append(torch.FloatTensor(reward).to(self.device))\n",
    "            self.memory.is_terminals.append(torch.FloatTensor(done_memory).to(self.device))\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Interaction process in enviroment for collect trajectory,\n",
    "        train process by agent nets after each rollout.\n",
    "        \"\"\"\n",
    "        total_train_start_time = time.time()\n",
    "\n",
    "        score = 0\n",
    "        state, _ = self.env.reset()\n",
    "        state = np.asarray(state)\n",
    "        self.num_episode = 0\n",
    "        self.time_step = 0\n",
    "        episode_reward = 0\n",
    "        print_episode_flag = False\n",
    "\n",
    "        for step_ in range(self.total_rollouts):\n",
    "            for _ in range(self.rollout_len):\n",
    "                action = self._get_action(state)\n",
    "                next_state, reward, done = self._step(action)\n",
    "                state = next_state\n",
    "                score += reward[0][0]\n",
    "                episode_reward += reward[0][0]\n",
    "\n",
    "                # print(f\"{test.shape = } $@!#$%$#*&&**()*)(*(*)()*\")\n",
    "\n",
    "                if done[0][0]:\n",
    "                    self.scores.append(score)\n",
    "                    score = 0\n",
    "                    state, _ = self.env.reset()\n",
    "                    self.num_episode += 1\n",
    "                    self.episode_reward_list.append(episode_reward)\n",
    "                    episode_reward = 0\n",
    "                    print_episode_flag = True\n",
    "\n",
    "                self.time_step += 1\n",
    "\n",
    "                total_training_time = time.time() - total_train_start_time\n",
    "                total_training_time = time.strftime('%H:%M:%S', time.gmtime(total_training_time))\n",
    "                if self.num_episode % self.print_episode_interval == 0 and print_episode_flag:\n",
    "                    print(\n",
    "                        \"[Episode {:3,}, Steps {:6,}]\".format(self.num_episode, self.time_step),\n",
    "                        \"Episode Reward: {:>9.3f},\".format(np.mean(np.asarray(self.episode_reward_list))),\n",
    "                        \"Elapsed Time: {}\".format(total_training_time)\n",
    "                    )\n",
    "                    print_episode_flag = False\n",
    "\n",
    "            if step_ % self.plot_interval == 0 and self.wandb_use and self.train_flag and not self.is_evaluate:\n",
    "                # self._plot_train_history()\n",
    "                self.log_wandb()\n",
    "\n",
    "            # if we have achieved the desired score - stop the process.\n",
    "            if self.solved_reward is not None:\n",
    "                if np.mean(self.scores[-10:]) > self.solved_reward:\n",
    "                    print(f\"It's solved! 10 episode reward mean = {np.mean(self.scores[-10:])}\")\n",
    "                    break\n",
    "\n",
    "            next_state = np.array(next_state)\n",
    "            value = self.critic(torch.FloatTensor(next_state).to(self.device))\n",
    "            self.memory.values.append(value)\n",
    "            # update policy\n",
    "            self._update_weights()\n",
    "            self.num_train += 1\n",
    "\n",
    "            if self.smart_competition:\n",
    "                if self.num_train % self.target_update_interval == 0:\n",
    "                    self.actor_target = deepcopy(self.actor)\n",
    "\n",
    "        self._save_train_history()\n",
    "        self.env.close()\n",
    "\n",
    "    def _update_weights(self):\n",
    "\n",
    "        returns = get_gae(\n",
    "            self.memory.rewards,\n",
    "            self.memory.values,\n",
    "            self.memory.is_terminals,\n",
    "            self.gamma,\n",
    "            self.lamda,\n",
    "        )\n",
    "        actor_losses, critic_losses = [], []\n",
    "        actor_wo_entropy_losses, entropy_losses = [], []\n",
    "        ratio_list = []\n",
    "\n",
    "        # flattening a list of torch.tensors into vectors\n",
    "        states = torch.cat(self.memory.states).view(-1, 4, 40, 40)\n",
    "        actions = torch.cat(self.memory.actions)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        log_probs = torch.cat(self.memory.log_probs).detach()\n",
    "        values = torch.cat(self.memory.values).detach()\n",
    "        advantages = returns - values[:-1]\n",
    "\n",
    "        for state, action, return_, old_log_prob, old_value, advantage in trajectories_data_generator(\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            returns=returns,\n",
    "            log_probs=log_probs,\n",
    "            values=values,\n",
    "            advantages=advantages,\n",
    "            batch_size=self.batch_size,\n",
    "            num_epochs=self.num_epochs,\n",
    "            ):\n",
    "\n",
    "            # compute ratio (pi_theta / pi_theta__old)\n",
    "            _, dist = self.actor(state)\n",
    "            cur_log_prob = dist.log_prob(action)\n",
    "            ratio = torch.exp(cur_log_prob - old_log_prob)\n",
    "\n",
    "            ratio_list.append(ratio.mean().item())\n",
    "\n",
    "            # compute entropy\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            # compute actor loss\n",
    "            loss =  advantage * ratio\n",
    "            clipped_loss = (\n",
    "                torch.clamp(ratio, 1. - self.epsilon, 1. + self.epsilon)\n",
    "                 * advantage\n",
    "                )\n",
    "            actor_loss = (\n",
    "                -torch.mean(torch.min(loss, clipped_loss))\n",
    "                - entropy * self.entropy_coef)\n",
    "\n",
    "            actor_loss_wo_entropy = -torch.mean(torch.min(loss, clipped_loss))\n",
    "            entropy_loss = -torch.mean(entropy * self.entropy_coef)\n",
    "\n",
    "            # critic loss, uncoment for clipped value loss too.\n",
    "            cur_value = self.critic(state)\n",
    "\n",
    "            critic_loss = (return_ - cur_value).pow(2).mean()\n",
    "\n",
    "            # actor optimizer step\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            actor_losses.append(actor_loss.item())\n",
    "            critic_losses.append(critic_loss.item())\n",
    "\n",
    "            actor_wo_entropy_losses.append(actor_loss_wo_entropy.item())\n",
    "            entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "            self.actor_loss_list.append(actor_loss_wo_entropy.item())\n",
    "            self.entropy_loss_list.append(entropy_loss.item())\n",
    "            self.critic_loss_list.append(critic_loss.item())\n",
    "\n",
    "        # clean memory of trajectory\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "        # write mean losses in train history logs\n",
    "        actor_loss = sum(actor_losses) / len(actor_losses)\n",
    "        critic_loss = sum(critic_losses) / len(critic_losses)\n",
    "\n",
    "        actor_wo_entropy_loss = sum(actor_wo_entropy_losses) / len(actor_wo_entropy_losses)\n",
    "        entropy_loss = sum(entropy_losses) / len(entropy_losses)\n",
    "        ratio = sum(ratio_list) / len(ratio_list)\n",
    "\n",
    "        self.actor_loss_history.append(actor_loss)\n",
    "        self.critic_loss_history.append(critic_loss)\n",
    "\n",
    "        self.actor_loss_list.append(actor_wo_entropy_loss)\n",
    "        self.entropy_loss_list.append(entropy_loss)\n",
    "        self.critic_loss_list.append(critic_loss)\n",
    "        self.ratio_list.append(ratio)\n",
    "\n",
    "        self.train_flag = True\n",
    "\n",
    "    def _plot_train_history(self):\n",
    "        data = [self.scores, self.actor_loss_history, self.critic_loss_history]\n",
    "        labels = [f\"score {np.mean(self.scores[-10:])}\",\n",
    "                  f\"actor loss {np.mean(self.actor_loss_history[-10:])}\", \n",
    "                  f\"critic loss {np.mean(self.critic_loss_history[-10:])}\",\n",
    "                  ]\n",
    "        clear_output(True)\n",
    "        with plt.style.context(\"seaborn-bright\"):\n",
    "            fig, axes = plt.subplots(3, 1, figsize=(6, 8))\n",
    "            for i, ax in enumerate(axes):\n",
    "                ax.plot(data[i], c=\"crimson\")\n",
    "                ax.set_title(labels[i])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def _save_train_history(self):\n",
    "        \"\"\"writing model weights and training logs to files.\"\"\"\n",
    "        data_time = datetime.now()\n",
    "        if not os.path.exists(f\"{self.path2save_train_history}/{self.env_name}/{data_time.month}_{data_time.day}_{data_time.hour}_{data_time.minute}\"):\n",
    "            os.makedirs(f\"{self.path2save_train_history}/{self.env_name}/{data_time.month}_{data_time.day}_{data_time.hour}_{data_time.minute}\")\n",
    "\n",
    "        torch.save(self.actor.state_dict(),\n",
    "                   f\"{self.path2save_train_history}/{self.env_name}/{data_time.month}_{data_time.day}_{data_time.hour}_{data_time.minute}/actor.pth\")\n",
    "        torch.save(self.critic.state_dict(),\n",
    "                   f\"{self.path2save_train_history}/{self.env_name}/{data_time.month}_{data_time.day}_{data_time.hour}_{data_time.minute}/critic.pth\")\n",
    "        torch.save(self.encoder.state_dict(),\n",
    "                   f\"{self.path2save_train_history}/{self.env_name}/{data_time.month}_{data_time.day}_{data_time.hour}_{data_time.minute}/encoder.pth\")\n",
    "        \n",
    "        pd.DataFrame({\"actor loss\": self.actor_loss_history, \n",
    "                      \"critic loss\": self.critic_loss_history}\n",
    "                     ).to_csv(f\"{self.path2save_train_history}/loss_logs.csv\")\n",
    "\n",
    "        pd.DataFrame(\n",
    "            data=self.scores, columns=[\"scores\"]\n",
    "            ).to_csv(f\"{self.path2save_train_history}/score_logs.csv\")\n",
    "\n",
    "        print(f\"MODEL SAVE SUCCESS!!! MODEL_DIRECTORY: {data_time.month}_{data_time.day}_{data_time.hour}_{data_time.minute}\")\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.is_evaluate = True\n",
    "\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        for _ in range(self.rollout_len):\n",
    "            while not done:\n",
    "                action = self._get_action(state)\n",
    "                next_state, reward, done = self._step(action)\n",
    "                state = next_state\n",
    "\n",
    "            self.env.close()\n",
    "\n",
    "    def load_predtrain_model(self,\n",
    "                             actor_weights_path: str,\n",
    "                             critic_weights_path: str):\n",
    "\n",
    "        self.actor.load_state_dict(torch.load(\n",
    "            f\"{self.path2save_train_history}/{self.env_name}/{self.load_model_time}/actor.pth\",\n",
    "            map_location=self.device))\n",
    "        self.critic.load_state_dict(torch.load(\n",
    "            f\"{self.path2save_train_history}/{self.env_name}/{self.load_model_time}/critic.pth\",\n",
    "            map_location=self.device))\n",
    "        self.encoder.load_state_dict(torch.load(\n",
    "            f\"{self.path2save_train_history}/{self.env_name}/{self.load_model_time}/encoder.pth\",\n",
    "            map_location=self.device))\n",
    "        print(\"Predtrain models loaded\")\n",
    "\n",
    "    def log_wandb(self):\n",
    "        log_dict = {\n",
    "            \"Num/train\": self.num_train,\n",
    "            \"Num/episodes\": self.num_episode,\n",
    "            \"Num/timesteps\": self.time_step,\n",
    "            \"Train/episode_reward_mean\": sum(self.episode_reward_list) / len(self.episode_reward_list),\n",
    "            \"Train/ratio\": sum(self.ratio_list) / len(self.ratio_list),\n",
    "            \"Loss/actor_loss\": sum(self.actor_loss_list) / len(self.actor_loss_list),\n",
    "            \"Loss/critic_loss\": sum(self.critic_loss_list) / len(self.critic_loss_list),\n",
    "            \"Loss/entropy_loss\": sum(self.entropy_loss_list) / len(self.entropy_loss_list),\n",
    "        }\n",
    "\n",
    "        wandb.log(log_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be613b0-38be-47ba-b6dd-8ed15ccd216d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympic",
   "language": "python",
   "name": "olympic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

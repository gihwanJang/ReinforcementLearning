{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999f5e79-adc8-41fe-8830-a9963da76bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c_qnet.ipynb\n",
      "TORCH VERSION: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import import_ipynb\n",
    "from datetime import datetime\n",
    "from shutil import copyfile\n",
    "\n",
    "from c_qnet import QNet, ReplayBuffer, Transition, MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a0e25a-f708-4bfd-af10-b3b84491e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, test_env, config, use_wandb):\n",
    "        self.env = env\n",
    "        self.test_env = test_env\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        self.env_name = config[\"env_name\"]\n",
    "\n",
    "        self.current_time = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "        if self.use_wandb:\n",
    "            self.wandb = wandb.init(\n",
    "                project=\"DQN_{0}\".format(self.env_name),\n",
    "                name=self.current_time,\n",
    "                config=config\n",
    "            )\n",
    "\n",
    "        self.max_num_episodes = config[\"max_num_episodes\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.steps_between_train = config[\"steps_between_train\"]\n",
    "        self.target_sync_step_interval = config[\"target_sync_step_interval\"]\n",
    "        self.replay_buffer_size = config[\"replay_buffer_size\"]\n",
    "        self.epsilon_start = config[\"epsilon_start\"]\n",
    "        self.epsilon_end = config[\"epsilon_end\"]\n",
    "        self.epsilon_final_scheduled_percent = config[\"epsilon_final_scheduled_percent\"]\n",
    "        self.print_episode_interval = config[\"print_episode_interval\"]\n",
    "        self.train_num_episodes_before_next_test = config[\"train_num_episodes_before_next_test\"]\n",
    "        self.validation_num_episodes = config[\"validation_num_episodes\"]\n",
    "        self.episode_reward_avg_solved = config[\"episode_reward_avg_solved\"]\n",
    "\n",
    "        self.epsilon_scheduled_last_episode = self.max_num_episodes * self.epsilon_final_scheduled_percent\n",
    "\n",
    "        # network\n",
    "        self.q = QNet(n_features=4, n_actions=2)\n",
    "        self.target_q = QNet(n_features=4, n_actions=2)\n",
    "        self.target_q.load_state_dict(self.q.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # agent\n",
    "        self.replay_buffer = ReplayBuffer(self.replay_buffer_size)\n",
    "\n",
    "        self.time_steps = 0\n",
    "        self.total_time_steps = 0\n",
    "        self.training_time_steps = 0\n",
    "\n",
    "    def epsilon_scheduled(self, current_episode):\n",
    "        fraction = min(current_episode / self.epsilon_scheduled_last_episode, 1.0)\n",
    "\n",
    "        epsilon = min(\n",
    "            self.epsilon_start + fraction * (self.epsilon_end - self.epsilon_start),\n",
    "            self.epsilon_start\n",
    "        )\n",
    "        return epsilon\n",
    "\n",
    "    def train_loop(self):\n",
    "        loss = 0.0\n",
    "\n",
    "        total_train_start_time = time.time()\n",
    "\n",
    "        validation_episode_reward_avg = 0.0\n",
    "\n",
    "        is_terminated = False\n",
    "\n",
    "        for n_episode in range(1, self.max_num_episodes + 1):\n",
    "            epsilon = self.epsilon_scheduled(n_episode)\n",
    "\n",
    "            episode_reward = 0\n",
    "\n",
    "            observation, _ = self.env.reset()\n",
    "\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                self.time_steps += 1\n",
    "                self.total_time_steps += 1\n",
    "\n",
    "                action = self.q.get_action(observation, epsilon)\n",
    "\n",
    "                next_observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                transition = Transition(observation, action, next_observation, reward, terminated)\n",
    "\n",
    "                self.replay_buffer.append(transition)\n",
    "\n",
    "                episode_reward += reward\n",
    "                observation = next_observation\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if self.total_time_steps % self.steps_between_train == 0 and self.time_steps > self.batch_size:\n",
    "                    loss = self.train()\n",
    "\n",
    "            total_training_time = time.time() - total_train_start_time\n",
    "            total_training_time = time.strftime('%H:%M:%S', time.gmtime(total_training_time))\n",
    "\n",
    "            if n_episode % self.print_episode_interval == 0:\n",
    "                print(\n",
    "                    \"[Episode {:3,}, Time Steps {:6,}]\".format(n_episode, self.time_steps),\n",
    "                    \"Episode Reward: {:>5},\".format(episode_reward),\n",
    "                    \"Replay buffer: {:>6,},\".format(self.replay_buffer.size()),\n",
    "                    \"Loss: {:6.3f},\".format(loss),\n",
    "                    \"Epsilon: {:4.2f},\".format(epsilon),\n",
    "                    \"Training Steps: {:5,},\".format(self.training_time_steps),\n",
    "                    \"Elapsed Time: {}\".format(total_training_time)\n",
    "                )\n",
    "\n",
    "            if n_episode % self.train_num_episodes_before_next_test == 0:\n",
    "                validation_episode_reward_lst, validation_episode_reward_avg = self.validate()\n",
    "\n",
    "                print(\"[Validation Episode Reward: {0}] Average: {1:.3f}\".format(\n",
    "                    validation_episode_reward_lst, validation_episode_reward_avg\n",
    "                ))\n",
    "\n",
    "                if validation_episode_reward_avg > self.episode_reward_avg_solved:\n",
    "                    print(\"Solved in {0:,} steps ({1:,} training steps)!\".format(\n",
    "                        self.time_steps, self.training_time_steps\n",
    "                    ))\n",
    "                    self.model_save(validation_episode_reward_avg)\n",
    "                    is_terminated = True\n",
    "\n",
    "            if self.use_wandb:\n",
    "                self.wandb.log({\n",
    "                    \"[VALIDATION] Mean Episode Reward ({0} Episodes)\".format(self.validation_num_episodes): validation_episode_reward_avg,\n",
    "                    \"[TRAIN] Episode Reward\": episode_reward,\n",
    "                    \"[TRAIN] Loss\": loss if loss != 0.0 else 0.0,\n",
    "                    \"[TRAIN] Epsilon\": epsilon,\n",
    "                    \"[TRAIN] Replay buffer\": self.replay_buffer.size(),\n",
    "                    \"Training Episode\": n_episode,\n",
    "                    \"Training Steps\": self.training_time_steps\n",
    "                })\n",
    "\n",
    "            if is_terminated:\n",
    "                break\n",
    "\n",
    "        total_training_time = time.time() - total_train_start_time\n",
    "        total_training_time = time.strftime('%H:%M:%S', time.gmtime(total_training_time))\n",
    "        print(\"Total Training End : {}\".format(total_training_time))\n",
    "        self.wandb.finish()\n",
    "\n",
    "    def train(self):\n",
    "        self.training_time_steps += 1\n",
    "\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # observations.shape: torch.Size([32, 4]),\n",
    "        # actions.shape: torch.Size([32, 1]),\n",
    "        # next_observations.shape: torch.Size([32, 4]),\n",
    "        # rewards.shape: torch.Size([32, 1]),\n",
    "        # dones.shape: torch.Size([32])\n",
    "        observations, actions, next_observations, rewards, dones = batch\n",
    "\n",
    "        # state_action_values.shape: torch.Size([32, 1])\n",
    "        q_out = self.q(observations)\n",
    "        q_values = q_out.gather(dim=-1, index=actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_prime_out = self.target_q(next_observations)\n",
    "            # next_state_values.shape: torch.Size([32, 1])\n",
    "            max_q_prime = q_prime_out.max(dim=1, keepdim=True).values\n",
    "            max_q_prime[dones] = 0.0\n",
    "\n",
    "            # target_state_action_values.shape: torch.Size([32, 1])\n",
    "            targets = rewards + self.gamma * max_q_prime\n",
    "\n",
    "        # loss is just scalar torch value\n",
    "        loss = F.mse_loss(targets.detach(), q_values)\n",
    "\n",
    "        # print(\"observations.shape: {0}, actions.shape: {1}, \"\n",
    "        #       \"next_observations.shape: {2}, rewards.shape: {3}, dones.shape: {4}\".format(\n",
    "        #     observations.shape, actions.shape,\n",
    "        #     next_observations.shape, rewards.shape, dones.shape\n",
    "        # ))\n",
    "        # print(\"state_action_values.shape: {0}\".format(state_action_values.shape))\n",
    "        # print(\"next_state_values.shape: {0}\".format(next_state_values.shape))\n",
    "        # print(\"target_state_action_values.shape: {0}\".format(\n",
    "        #     target_state_action_values.shape\n",
    "        # ))\n",
    "        # print(\"loss.shape: {0}\".format(loss.shape))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # sync\n",
    "        if self.time_steps % self.target_sync_step_interval == 0:\n",
    "            self.target_q.load_state_dict(self.q.state_dict())\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def model_save(self, validation_episode_reward_avg):\n",
    "        filename = \"dqn_{0}_{1:4.1f}_{2}.pth\".format(\n",
    "            self.env_name, validation_episode_reward_avg, self.current_time\n",
    "        )\n",
    "        torch.save(self.q.state_dict(), os.path.join(MODEL_DIR, filename))\n",
    "\n",
    "        copyfile(\n",
    "            src=os.path.join(MODEL_DIR, filename),\n",
    "            dst=os.path.join(MODEL_DIR, \"dqn_{0}_latest.pth\".format(self.env_name))\n",
    "        )\n",
    "\n",
    "    def validate(self):\n",
    "        episode_reward_lst = np.zeros(shape=(self.validation_num_episodes,), dtype=float)\n",
    "\n",
    "        for i in range(self.validation_num_episodes):\n",
    "            episode_reward = 0\n",
    "\n",
    "            observation, _ = self.test_env.reset()\n",
    "\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self.q.get_action(observation, epsilon=0.0)\n",
    "\n",
    "                next_observation, reward, terminated, truncated, _ = self.test_env.step(action)\n",
    "\n",
    "                episode_reward += reward\n",
    "                observation = next_observation\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episode_reward_lst[i] = episode_reward\n",
    "\n",
    "        return episode_reward_lst, np.average(episode_reward_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f274e1-cf55-468b-9f55-68078fa6f085",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ae6196-83b7-4d41-9d5a-6b29a3fc792c",
   "metadata": {},
   "source": [
    "### __init__function\n",
    "This function is initialization DQN class\n",
    "\n",
    "```python\n",
    "def __init__(self, env, test_env, config, use_wandb):\n",
    "    self.env = env\n",
    "    self.test_env = test_env\n",
    "    self.use_wandb = use_wandb\n",
    "\n",
    "    self.env_name = config[\"env_name\"]\n",
    "\n",
    "    self.current_time = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    if self.use_wandb:\n",
    "        self.wandb = wandb.init(\n",
    "            project=\"DQN_{0}\".format(self.env_name),\n",
    "            name=self.current_time,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "    self.max_num_episodes = config[\"max_num_episodes\"]\n",
    "    self.batch_size = config[\"batch_size\"]\n",
    "    self.learning_rate = config[\"learning_rate\"]\n",
    "    self.gamma = config[\"gamma\"]\n",
    "    self.steps_between_train = config[\"steps_between_train\"]\n",
    "    self.target_sync_step_interval = config[\"target_sync_step_interval\"]\n",
    "    self.replay_buffer_size = config[\"replay_buffer_size\"]\n",
    "    self.epsilon_start = config[\"epsilon_start\"]\n",
    "    self.epsilon_end = config[\"epsilon_end\"]\n",
    "    self.epsilon_final_scheduled_percent = config[\"epsilon_final_scheduled_percent\"]\n",
    "    self.print_episode_interval = config[\"print_episode_interval\"]\n",
    "    self.train_num_episodes_before_next_test = config[\"train_num_episodes_before_next_test\"]\n",
    "    self.validation_num_episodes = config[\"validation_num_episodes\"]\n",
    "    self.episode_reward_avg_solved = config[\"episode_reward_avg_solved\"]\n",
    "\n",
    "    self.epsilon_scheduled_last_episode = self.max_num_episodes * self.epsilon_final_scheduled_percent\n",
    "\n",
    "    # network\n",
    "    self.q = QNet(n_features=4, n_actions=2)\n",
    "    self.target_q = QNet(n_features=4, n_actions=2)\n",
    "    self.target_q.load_state_dict(self.q.state_dict())\n",
    "\n",
    "    self.optimizer = optim.Adam(self.q.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    # agent\n",
    "    self.replay_buffer = ReplayBuffer(self.replay_buffer_size)\n",
    "\n",
    "    self.time_steps = 0\n",
    "    self.total_time_steps = 0\n",
    "    self.training_time_steps = 0\n",
    "```\n",
    "\n",
    "### environment initialization\n",
    "\n",
    "- env : this variable means training environment\n",
    "- test_env : this variable mean validation environment\n",
    "- use_wandb : this variable has two value true or false\n",
    "    - if value is true, using wandb\n",
    "    - else doesn't use wandb\n",
    "- env_name : in this case using CartPole-v1\n",
    "- current_time : this variable means current time / when using this value, can see episode info\n",
    "\n",
    "### hyper parameter initialization\n",
    "\n",
    "- max_num_episodes : this variable means how many do episode\n",
    "- batch_size : this variable means how many using step info when do gradient decent\n",
    "- learning_rate : this variable means how much new information will you accept\n",
    "- gamma : this variable means To what extent will future values be reflected\n",
    "- steps_between_train : this variable means how many steps will you train during the episode\n",
    "- epsilon_start : this variable means how much do exploration when start\n",
    "- epsilon_end : this variable means how much do exploration when end\n",
    "- epsilon_final_scheduled_percent : percentage of last episode scheduled as epsilon final value\n",
    "- print_episode_interval : episode interval for output statistics\n",
    "- train_num_episodes_before_next_test : interval between each training episode between verifications\n",
    "- validation_num_episodes : number of episodes performed for validation\n",
    "- episode_reward_avg_solved : average of verification episode rewards for completing training\n",
    "\n",
    "### QNet initialization\n",
    "\n",
    "- self.q = QNet(n_features=4, n_actions=2) : current q-net\n",
    "    - observation space is 4 ans action space is 2\n",
    "- self.target_q = QNet(n_features=4, n_actions=2) : target q-net\n",
    "- self.target_q.load_state_dict(self.q.state_dict())\n",
    "    - set the initial weights of the target Q-net to be the same as the current Q-net\n",
    "- self.optimizer = optim.Adam(self.q.parameters(), lr=self.learning_rate) : optimizer is adam optimizer\n",
    "\n",
    "### agent initialization\n",
    "\n",
    "- self.replay_buffer = ReplayBuffer(self.replay_buffer_size) : set replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a7f2e3-4d86-4d3f-a42e-5012b2c440bd",
   "metadata": {},
   "source": [
    "## epsilon_scheduled function\n",
    "This function selecte action by decaying_epsilon_greedy method\n",
    "\n",
    "```python\n",
    "def epsilon_scheduled(self, current_episode):\n",
    "    fraction = min(current_episode / self.epsilon_scheduled_last_episode, 1.0)\n",
    "\n",
    "    epsilon = min(\n",
    "        self.epsilon_start + fraction * (self.epsilon_end - self.epsilon_start),\n",
    "        self.epsilon_start\n",
    "    )\n",
    "    return epsilon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae9b19-3c4e-4a2b-ae47-1e87ce26cbd6",
   "metadata": {},
   "source": [
    "## train_loop function\n",
    "\n",
    "``` python\n",
    "def train_loop(self):\n",
    "    loss = 0.0\n",
    "    validation_episode_reward_avg = 0.0\n",
    "    is_terminated = False\n",
    "\n",
    "    for n_episode in range(1, self.max_num_episodes + 1):\n",
    "        epsilon = self.epsilon_scheduled(n_episode)\n",
    "        episode_reward = 0\n",
    "        observation, _ = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            self.time_steps += 1\n",
    "            self.total_time_steps += 1\n",
    "\n",
    "            action = self.q.get_action(observation, epsilon)\n",
    "            next_observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            transition = Transition(observation, action, next_observation, reward, terminated)\n",
    "            self.replay_buffer.append(transition)\n",
    "\n",
    "            episode_reward += reward\n",
    "            observation = next_observation\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if self.total_time_steps % self.steps_between_train == 0 and self.time_steps > self.batch_size:\n",
    "                loss = self.train()\n",
    "\n",
    "        if n_episode % self.train_num_episodes_before_next_test == 0:\n",
    "            validation_episode_reward_lst, validation_episode_reward_avg = self.validate()\n",
    "\n",
    "            if validation_episode_reward_avg > self.episode_reward_avg_solved:\n",
    "                self.model_save(validation_episode_reward_avg)\n",
    "                is_terminated = True\n",
    "\n",
    "        if is_terminated:\n",
    "            break\n",
    "```\n",
    "\n",
    "### initialization train loop\n",
    "\n",
    "- loss = 0.0\n",
    "- validation_episode_reward_avg = 0.0\n",
    "    - if validation_episode_reward_avg is same episode_reward_avg_solved, train loop is done\n",
    "- is_terminated = False\n",
    "    - if this value is true, train loop is done\n",
    "\n",
    "### doing episode\n",
    "#### initialization episode\n",
    "- epsilon = self.epsilon_scheduled(n_episode) : this episode epsilon\n",
    "- episode_reward = 0 : this episode reward\n",
    "- observation, _ = self.env.reset() : start observation\n",
    "- done = False : done is false\n",
    "#### doing step\n",
    "- self.time_steps += 1 : counting step\n",
    "- action = self.q.get_action(observation, epsilon) : get action\n",
    "- next_observation, reward, terminated, truncated, _ = self.env.step(action) : get next_observation\n",
    "- transition = Transition(observation, action, next_observation, reward, terminated) : make transition\n",
    "- self.replay_buffer.append(transition) : insert transition in replay_buffer\n",
    "- episode_reward += reward : cumulative sum reward in this episode\n",
    "- observation = next_observation : set current observation to next observation\n",
    "- done = terminated or truncated : done is (terminated | truncated)\n",
    "- if self.total_time_steps % self.steps_between_train == 0 and self.time_steps > self.batch_size -> loss = self.train()\n",
    "    - Test is performed every steps_between_train but buffer size is smaller then batch_size doesn't do this\n",
    "#### doing validation \n",
    "- validation_episode_reward_lst, validation_episode_reward_avg = self.validate()\n",
    "    - validate is performed every train_num_episodes_before_next_test\n",
    "    - if get validation_episode_reward_avg value is more than episode_reward_avg_solved\n",
    "        - save model and terminated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44279575-9cf4-466f-bc36-1e793fd66330",
   "metadata": {},
   "source": [
    "## train function\n",
    "This function update policy\n",
    "\n",
    "``` python\n",
    "def train(self):\n",
    "    self.training_time_steps += 1\n",
    "\n",
    "    batch = self.replay_buffer.sample(self.batch_size)\n",
    "    observations, actions, next_observations, rewards, dones = batch\n",
    "\n",
    "    q_out = self.q(observations)\n",
    "    q_values = q_out.gather(dim=-1, index=actions)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_prime_out = self.target_q(next_observations)\n",
    "        max_q_prime = q_prime_out.max(dim=1, keepdim=True).values\n",
    "        max_q_prime[dones] = 0.0\n",
    "\n",
    "        targets = rewards + self.gamma * max_q_prime\n",
    "\n",
    "    loss = F.mse_loss(targets.detach(), q_values)\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    if self.time_steps % self.target_sync_step_interval == 0:\n",
    "        self.target_q.load_state_dict(self.q.state_dict())\n",
    "\n",
    "    return loss.item()\n",
    "```\n",
    "\n",
    "### get current policy batch\n",
    "- self.training_time_steps += 1 : counting train step\n",
    "- batch = self.replay_buffer.sample(self.batch_size) : get batch at replay_buffer\n",
    "- observations, actions, next_observations, rewards, dones = batch : divide each into batch sizes\n",
    "- q_out = self.q(observations) : get q_value each batch sizes observations\n",
    "- q_values = q_out.gather(dim=-1, index=actions) : get action index value\n",
    "### get target policy batch\n",
    "- q_prime_out = self.target_q(next_observations) : get q_value each batch sizes next observations\n",
    "- max_q_prime = q_prime_out.max(dim=1, keepdim=True).values : get best action index value\n",
    "- targets = rewards + self.gamma * max_q_prime : update target value\n",
    "### update current policy\n",
    "- oss = F.mse_loss(targets.detach(), q_values) : get loss\n",
    "- self.optimizer.zero_grad() : initialization qnet\n",
    "- loss.backward() : gradient decent\n",
    "- self.optimizer.step() : update qnet\n",
    "### sync\n",
    "- if self.time_steps % self.target_sync_step_interval == 0 -> self.target_q.load_state_dict(self.q.state_dict())\n",
    "    - target_q syncs for each target_sync_step_interval "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74f975-73c9-46ac-bf84-c4327bc4214b",
   "metadata": {},
   "source": [
    "## model_save function\n",
    "This function save model\n",
    "\n",
    "``` python\n",
    "def model_save(self, validation_episode_reward_avg):\n",
    "    filename = \"dqn_{0}_{1:4.1f}_{2}.pth\".format(\n",
    "        self.env_name, validation_episode_reward_avg, self.current_time\n",
    "    )\n",
    "    torch.save(self.q.state_dict(), os.path.join(MODEL_DIR, filename))\n",
    "\n",
    "    copyfile(\n",
    "        src=os.path.join(MODEL_DIR, filename),\n",
    "        dst=os.path.join(MODEL_DIR, \"dqn_{0}_latest.pth\".format(self.env_name))\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca80bbf-4023-4be8-8d1f-e5a560cbee88",
   "metadata": {},
   "source": [
    "## validate function\n",
    "This function validate model\n",
    "\n",
    "```python\n",
    "def validate(self):\n",
    "    episode_reward_lst = np.zeros(shape=(self.validation_num_episodes,), dtype=float)\n",
    "\n",
    "    for i in range(self.validation_num_episodes):\n",
    "        episode_reward = 0\n",
    "\n",
    "        observation, _ = self.test_env.reset()\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = self.q.get_action(observation, epsilon=0.0)\n",
    "\n",
    "            next_observation, reward, terminated, truncated, _ = self.test_env.step(action)\n",
    "\n",
    "            episode_reward += reward\n",
    "            observation = next_observation\n",
    "            done = terminated or truncated\n",
    "\n",
    "        episode_reward_lst[i] = episode_reward\n",
    "\n",
    "    return episode_reward_lst, np.average(episode_reward_lst)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41626da4-f509-48dd-af6d-be1855d03d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "    env = gym.make(ENV_NAME)\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "\n",
    "    config = {\n",
    "        \"env_name\": ENV_NAME,                       # 환경의 이름\n",
    "        \"max_num_episodes\": 1_500,                  # 훈련을 위한 최대 에피소드 횟수\n",
    "        \"batch_size\": 32,                           # 훈련시 배치에서 한번에 가져오는 랜덤 배치 사이즈\n",
    "        \"learning_rate\": 0.0001,                    # 학습율\n",
    "        \"gamma\": 0.99,                              # 감가율\n",
    "        \"steps_between_train\": 1,                   # 훈련 사이의 환경 스텝 수\n",
    "        \"target_sync_step_interval\": 500,           # 기존 Q 모델을 타깃 Q 모델로 동기화시키는 step 간격\n",
    "        \"replay_buffer_size\": 30_000,               # 리플레이 버퍼 사이즈\n",
    "        \"epsilon_start\": 0.95,                      # Epsilon 초기 값\n",
    "        \"epsilon_end\": 0.01,                        # Epsilon 최종 값\n",
    "        \"epsilon_final_scheduled_percent\": 0.75,    # Epsilon 최종 값으로 스케줄되는 마지막 에피소드 비율\n",
    "        \"print_episode_interval\": 10,               # Episode 통계 출력에 관한 에피소드 간격\n",
    "        \"train_num_episodes_before_next_test\": 50,  # 검증 사이 마다 각 훈련 episode 간격\n",
    "        \"validation_num_episodes\": 3,               # 검증에 수행하는 에피소드 횟수\n",
    "        \"episode_reward_avg_solved\": 490,           # 훈련 종료를 위한 검증 에피소드 리워드의 Average\n",
    "    }\n",
    "\n",
    "    use_wandb = True\n",
    "    dqn = DQN(\n",
    "        env=env, test_env=test_env, config=config, use_wandb=use_wandb\n",
    "    )\n",
    "    dqn.train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d44651-967e-4738-936f-3a383c3e952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgihwan319\u001b[0m (\u001b[33mgihwanjang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_03_DQN/wandb/run-20231107_080505-25z3o3rv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gihwanjang/DQN_CartPole-v1/runs/25z3o3rv' target=\"_blank\">2023-11-07_08-05-02</a></strong> to <a href='https://wandb.ai/gihwanjang/DQN_CartPole-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gihwanjang/DQN_CartPole-v1' target=\"_blank\">https://wandb.ai/gihwanjang/DQN_CartPole-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gihwanjang/DQN_CartPole-v1/runs/25z3o3rv' target=\"_blank\">https://wandb.ai/gihwanjang/DQN_CartPole-v1/runs/25z3o3rv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode  10, Time Steps    156] Episode Reward:  13.0, Replay buffer:    156, Loss:  0.158, Epsilon: 0.94, Training Steps:   124, Elapsed Time: 00:00:00\n",
      "[Episode  20, Time Steps    380] Episode Reward:  32.0, Replay buffer:    380, Loss:  0.011, Epsilon: 0.93, Training Steps:   348, Elapsed Time: 00:00:00\n",
      "[Episode  30, Time Steps    554] Episode Reward:   9.0, Replay buffer:    554, Loss:  0.016, Epsilon: 0.92, Training Steps:   522, Elapsed Time: 00:00:00\n",
      "[Episode  40, Time Steps    754] Episode Reward:  11.0, Replay buffer:    754, Loss:  0.008, Epsilon: 0.92, Training Steps:   722, Elapsed Time: 00:00:00\n",
      "[Episode  50, Time Steps    971] Episode Reward:  22.0, Replay buffer:    971, Loss:  0.011, Epsilon: 0.91, Training Steps:   939, Elapsed Time: 00:00:00\n",
      "[Validation Episode Reward: [11. 10. 11.]] Average: 10.667\n",
      "[Episode  60, Time Steps  1,158] Episode Reward:  16.0, Replay buffer:  1,158, Loss:  0.329, Epsilon: 0.90, Training Steps: 1,126, Elapsed Time: 00:00:00\n",
      "[Episode  70, Time Steps  1,380] Episode Reward:  21.0, Replay buffer:  1,380, Loss:  0.227, Epsilon: 0.89, Training Steps: 1,348, Elapsed Time: 00:00:00\n",
      "[Episode  80, Time Steps  1,591] Episode Reward:  14.0, Replay buffer:  1,591, Loss:  0.031, Epsilon: 0.88, Training Steps: 1,559, Elapsed Time: 00:00:01\n",
      "[Episode  90, Time Steps  1,862] Episode Reward:  54.0, Replay buffer:  1,862, Loss:  0.182, Epsilon: 0.87, Training Steps: 1,830, Elapsed Time: 00:00:01\n",
      "[Episode 100, Time Steps  2,110] Episode Reward:  26.0, Replay buffer:  2,110, Loss:  0.465, Epsilon: 0.87, Training Steps: 2,078, Elapsed Time: 00:00:01\n",
      "[Validation Episode Reward: [126. 102.  86.]] Average: 104.667\n",
      "[Episode 110, Time Steps  2,430] Episode Reward:  35.0, Replay buffer:  2,430, Loss:  0.332, Epsilon: 0.86, Training Steps: 2,398, Elapsed Time: 00:00:01\n",
      "[Episode 120, Time Steps  2,724] Episode Reward:  22.0, Replay buffer:  2,724, Loss:  0.162, Epsilon: 0.85, Training Steps: 2,692, Elapsed Time: 00:00:01\n",
      "[Episode 130, Time Steps  3,013] Episode Reward:  45.0, Replay buffer:  3,013, Loss:  0.687, Epsilon: 0.84, Training Steps: 2,981, Elapsed Time: 00:00:02\n",
      "[Episode 140, Time Steps  3,300] Episode Reward:  10.0, Replay buffer:  3,300, Loss:  0.302, Epsilon: 0.83, Training Steps: 3,268, Elapsed Time: 00:00:02\n",
      "[Episode 150, Time Steps  3,637] Episode Reward:  16.0, Replay buffer:  3,637, Loss:  1.163, Epsilon: 0.82, Training Steps: 3,605, Elapsed Time: 00:00:02\n",
      "[Validation Episode Reward: [78. 85. 94.]] Average: 85.667\n",
      "[Episode 160, Time Steps  4,070] Episode Reward:  14.0, Replay buffer:  4,070, Loss:  0.182, Epsilon: 0.82, Training Steps: 4,038, Elapsed Time: 00:00:02\n",
      "[Episode 170, Time Steps  4,395] Episode Reward:  41.0, Replay buffer:  4,395, Loss:  0.677, Epsilon: 0.81, Training Steps: 4,363, Elapsed Time: 00:00:03\n",
      "[Episode 180, Time Steps  5,044] Episode Reward: 106.0, Replay buffer:  5,044, Loss:  0.089, Epsilon: 0.80, Training Steps: 5,012, Elapsed Time: 00:00:03\n",
      "[Episode 190, Time Steps  5,347] Episode Reward:  38.0, Replay buffer:  5,347, Loss:  1.175, Epsilon: 0.79, Training Steps: 5,315, Elapsed Time: 00:00:03\n",
      "[Episode 200, Time Steps  5,796] Episode Reward:  83.0, Replay buffer:  5,796, Loss:  4.806, Epsilon: 0.78, Training Steps: 5,764, Elapsed Time: 00:00:04\n",
      "[Validation Episode Reward: [500. 379. 500.]] Average: 459.667\n",
      "[Episode 210, Time Steps  6,137] Episode Reward:  45.0, Replay buffer:  6,137, Loss:  0.140, Epsilon: 0.77, Training Steps: 6,105, Elapsed Time: 00:00:04\n",
      "[Episode 220, Time Steps  6,435] Episode Reward:  12.0, Replay buffer:  6,435, Loss:  1.649, Epsilon: 0.77, Training Steps: 6,403, Elapsed Time: 00:00:04\n",
      "[Episode 230, Time Steps  6,996] Episode Reward:  19.0, Replay buffer:  6,996, Loss:  0.306, Epsilon: 0.76, Training Steps: 6,964, Elapsed Time: 00:00:05\n",
      "[Episode 240, Time Steps  7,310] Episode Reward:  25.0, Replay buffer:  7,310, Loss:  0.733, Epsilon: 0.75, Training Steps: 7,278, Elapsed Time: 00:00:05\n",
      "[Episode 250, Time Steps  7,631] Episode Reward:  23.0, Replay buffer:  7,631, Loss:  3.706, Epsilon: 0.74, Training Steps: 7,599, Elapsed Time: 00:00:05\n",
      "[Validation Episode Reward: [278. 319. 303.]] Average: 300.000\n",
      "[Episode 260, Time Steps  8,045] Episode Reward:  17.0, Replay buffer:  8,045, Loss:  0.283, Epsilon: 0.73, Training Steps: 8,013, Elapsed Time: 00:00:05\n",
      "[Episode 270, Time Steps  8,579] Episode Reward:  34.0, Replay buffer:  8,579, Loss:  6.644, Epsilon: 0.72, Training Steps: 8,547, Elapsed Time: 00:00:06\n",
      "[Episode 280, Time Steps  8,924] Episode Reward:  13.0, Replay buffer:  8,924, Loss:  0.316, Epsilon: 0.72, Training Steps: 8,892, Elapsed Time: 00:00:06\n",
      "[Episode 290, Time Steps  9,318] Episode Reward:  16.0, Replay buffer:  9,318, Loss:  0.378, Epsilon: 0.71, Training Steps: 9,286, Elapsed Time: 00:00:06\n",
      "[Episode 300, Time Steps  9,908] Episode Reward:  34.0, Replay buffer:  9,908, Loss:  5.195, Epsilon: 0.70, Training Steps: 9,876, Elapsed Time: 00:00:07\n",
      "[Validation Episode Reward: [261. 344. 294.]] Average: 299.667\n",
      "[Episode 310, Time Steps 10,426] Episode Reward:  28.0, Replay buffer: 10,426, Loss:  6.960, Epsilon: 0.69, Training Steps: 10,394, Elapsed Time: 00:00:07\n",
      "[Episode 320, Time Steps 11,054] Episode Reward:  86.0, Replay buffer: 11,054, Loss:  0.269, Epsilon: 0.68, Training Steps: 11,022, Elapsed Time: 00:00:08\n",
      "[Episode 330, Time Steps 11,456] Episode Reward:  22.0, Replay buffer: 11,456, Loss:  0.236, Epsilon: 0.67, Training Steps: 11,424, Elapsed Time: 00:00:08\n",
      "[Episode 340, Time Steps 11,839] Episode Reward:  52.0, Replay buffer: 11,839, Loss:  0.827, Epsilon: 0.67, Training Steps: 11,807, Elapsed Time: 00:00:08\n",
      "[Episode 350, Time Steps 12,388] Episode Reward:  18.0, Replay buffer: 12,388, Loss:  9.620, Epsilon: 0.66, Training Steps: 12,356, Elapsed Time: 00:00:09\n",
      "[Validation Episode Reward: [227. 196. 164.]] Average: 195.667\n",
      "[Episode 360, Time Steps 12,773] Episode Reward:  17.0, Replay buffer: 12,773, Loss:  0.707, Epsilon: 0.65, Training Steps: 12,741, Elapsed Time: 00:00:09\n",
      "[Episode 370, Time Steps 13,557] Episode Reward:  58.0, Replay buffer: 13,557, Loss:  4.605, Epsilon: 0.64, Training Steps: 13,525, Elapsed Time: 00:00:10\n",
      "[Episode 380, Time Steps 14,046] Episode Reward:  17.0, Replay buffer: 14,046, Loss:  0.500, Epsilon: 0.63, Training Steps: 14,014, Elapsed Time: 00:00:10\n",
      "[Episode 390, Time Steps 14,978] Episode Reward:  34.0, Replay buffer: 14,978, Loss:  2.128, Epsilon: 0.62, Training Steps: 14,946, Elapsed Time: 00:00:11\n",
      "[Episode 400, Time Steps 15,965] Episode Reward:  14.0, Replay buffer: 15,965, Loss:  0.934, Epsilon: 0.62, Training Steps: 15,933, Elapsed Time: 00:00:12\n",
      "[Validation Episode Reward: [241. 197. 233.]] Average: 223.667\n",
      "[Episode 410, Time Steps 16,776] Episode Reward: 184.0, Replay buffer: 16,776, Loss:  3.003, Epsilon: 0.61, Training Steps: 16,744, Elapsed Time: 00:00:13\n",
      "[Episode 420, Time Steps 17,740] Episode Reward:  58.0, Replay buffer: 17,740, Loss:  4.859, Epsilon: 0.60, Training Steps: 17,708, Elapsed Time: 00:00:13\n",
      "[Episode 430, Time Steps 18,302] Episode Reward:  80.0, Replay buffer: 18,302, Loss:  1.127, Epsilon: 0.59, Training Steps: 18,270, Elapsed Time: 00:00:14\n",
      "[Episode 440, Time Steps 19,073] Episode Reward:  63.0, Replay buffer: 19,073, Loss:  1.221, Epsilon: 0.58, Training Steps: 19,041, Elapsed Time: 00:00:15\n",
      "[Episode 450, Time Steps 20,076] Episode Reward: 192.0, Replay buffer: 20,076, Loss:  1.742, Epsilon: 0.57, Training Steps: 20,044, Elapsed Time: 00:00:16\n",
      "[Validation Episode Reward: [265. 349. 277.]] Average: 297.000\n",
      "[Episode 460, Time Steps 21,310] Episode Reward: 142.0, Replay buffer: 21,310, Loss: 12.486, Epsilon: 0.57, Training Steps: 21,278, Elapsed Time: 00:00:17\n",
      "[Episode 470, Time Steps 22,497] Episode Reward:  31.0, Replay buffer: 22,497, Loss: 50.269, Epsilon: 0.56, Training Steps: 22,465, Elapsed Time: 00:00:18\n",
      "[Episode 480, Time Steps 23,335] Episode Reward: 154.0, Replay buffer: 23,335, Loss:  0.673, Epsilon: 0.55, Training Steps: 23,303, Elapsed Time: 00:00:19\n",
      "[Episode 490, Time Steps 24,671] Episode Reward:  51.0, Replay buffer: 24,671, Loss:  3.217, Epsilon: 0.54, Training Steps: 24,639, Elapsed Time: 00:00:20\n",
      "[Episode 500, Time Steps 25,586] Episode Reward:  47.0, Replay buffer: 25,586, Loss:  4.067, Epsilon: 0.53, Training Steps: 25,554, Elapsed Time: 00:00:21\n",
      "[Validation Episode Reward: [224. 294. 335.]] Average: 284.333\n",
      "[Episode 510, Time Steps 26,809] Episode Reward: 132.0, Replay buffer: 26,809, Loss:  3.105, Epsilon: 0.52, Training Steps: 26,777, Elapsed Time: 00:00:22\n",
      "[Episode 520, Time Steps 28,293] Episode Reward: 170.0, Replay buffer: 28,293, Loss:  2.601, Epsilon: 0.52, Training Steps: 28,261, Elapsed Time: 00:00:24\n",
      "[Episode 530, Time Steps 29,368] Episode Reward:  24.0, Replay buffer: 29,368, Loss: 80.070, Epsilon: 0.51, Training Steps: 29,336, Elapsed Time: 00:00:25\n",
      "[Episode 540, Time Steps 30,559] Episode Reward:  46.0, Replay buffer: 30,000, Loss:  5.185, Epsilon: 0.50, Training Steps: 30,527, Elapsed Time: 00:00:26\n",
      "[Episode 550, Time Steps 32,097] Episode Reward:  35.0, Replay buffer: 30,000, Loss:  2.758, Epsilon: 0.49, Training Steps: 32,065, Elapsed Time: 00:00:28\n",
      "[Validation Episode Reward: [260. 221. 400.]] Average: 293.667\n",
      "[Episode 560, Time Steps 33,479] Episode Reward: 203.0, Replay buffer: 30,000, Loss:  2.047, Epsilon: 0.48, Training Steps: 33,447, Elapsed Time: 00:00:29\n",
      "[Episode 570, Time Steps 35,123] Episode Reward: 300.0, Replay buffer: 30,000, Loss:  6.020, Epsilon: 0.47, Training Steps: 35,091, Elapsed Time: 00:00:31\n",
      "[Episode 580, Time Steps 36,515] Episode Reward: 168.0, Replay buffer: 30,000, Loss: 94.725, Epsilon: 0.47, Training Steps: 36,483, Elapsed Time: 00:00:32\n",
      "[Episode 590, Time Steps 37,687] Episode Reward:  46.0, Replay buffer: 30,000, Loss:  4.346, Epsilon: 0.46, Training Steps: 37,655, Elapsed Time: 00:00:34\n",
      "[Episode 600, Time Steps 39,002] Episode Reward: 300.0, Replay buffer: 30,000, Loss: 96.601, Epsilon: 0.45, Training Steps: 38,970, Elapsed Time: 00:00:35\n",
      "[Validation Episode Reward: [324. 312. 305.]] Average: 313.667\n",
      "[Episode 610, Time Steps 40,015] Episode Reward: 137.0, Replay buffer: 30,000, Loss:  1.505, Epsilon: 0.44, Training Steps: 39,983, Elapsed Time: 00:00:36\n",
      "[Episode 620, Time Steps 41,980] Episode Reward: 101.0, Replay buffer: 30,000, Loss: 117.639, Epsilon: 0.43, Training Steps: 41,948, Elapsed Time: 00:00:38\n",
      "[Episode 630, Time Steps 43,022] Episode Reward:  55.0, Replay buffer: 30,000, Loss: 15.650, Epsilon: 0.42, Training Steps: 42,990, Elapsed Time: 00:00:39\n",
      "[Episode 640, Time Steps 44,685] Episode Reward: 141.0, Replay buffer: 30,000, Loss:  4.702, Epsilon: 0.42, Training Steps: 44,653, Elapsed Time: 00:00:41\n",
      "[Episode 650, Time Steps 46,256] Episode Reward: 223.0, Replay buffer: 30,000, Loss:  4.884, Epsilon: 0.41, Training Steps: 46,224, Elapsed Time: 00:00:43\n",
      "[Validation Episode Reward: [500. 306. 310.]] Average: 372.000\n",
      "[Episode 660, Time Steps 48,185] Episode Reward:  26.0, Replay buffer: 30,000, Loss:  1.376, Epsilon: 0.40, Training Steps: 48,153, Elapsed Time: 00:00:45\n",
      "[Episode 670, Time Steps 50,348] Episode Reward: 116.0, Replay buffer: 30,000, Loss: 12.541, Epsilon: 0.39, Training Steps: 50,316, Elapsed Time: 00:00:47\n",
      "[Episode 680, Time Steps 52,529] Episode Reward: 183.0, Replay buffer: 30,000, Loss:  2.212, Epsilon: 0.38, Training Steps: 52,497, Elapsed Time: 00:00:49\n",
      "[Episode 690, Time Steps 54,900] Episode Reward:  81.0, Replay buffer: 30,000, Loss:  4.653, Epsilon: 0.37, Training Steps: 54,868, Elapsed Time: 00:00:52\n",
      "[Episode 700, Time Steps 57,446] Episode Reward: 275.0, Replay buffer: 30,000, Loss:  1.668, Epsilon: 0.37, Training Steps: 57,414, Elapsed Time: 00:00:55\n",
      "[Validation Episode Reward: [265. 383. 275.]] Average: 307.667\n",
      "[Episode 710, Time Steps 59,966] Episode Reward: 348.0, Replay buffer: 30,000, Loss:  1.748, Epsilon: 0.36, Training Steps: 59,934, Elapsed Time: 00:00:57\n",
      "[Episode 720, Time Steps 62,581] Episode Reward: 249.0, Replay buffer: 30,000, Loss:  1.059, Epsilon: 0.35, Training Steps: 62,549, Elapsed Time: 00:01:00\n",
      "[Episode 730, Time Steps 64,775] Episode Reward: 338.0, Replay buffer: 30,000, Loss:  1.698, Epsilon: 0.34, Training Steps: 64,743, Elapsed Time: 00:01:03\n",
      "[Episode 740, Time Steps 66,887] Episode Reward: 313.0, Replay buffer: 30,000, Loss:  2.548, Epsilon: 0.33, Training Steps: 66,855, Elapsed Time: 00:01:05\n",
      "[Episode 750, Time Steps 69,249] Episode Reward: 392.0, Replay buffer: 30,000, Loss:  2.209, Epsilon: 0.32, Training Steps: 69,217, Elapsed Time: 00:01:08\n",
      "[Validation Episode Reward: [427. 290. 330.]] Average: 349.000\n",
      "[Episode 760, Time Steps 71,540] Episode Reward:  65.0, Replay buffer: 30,000, Loss:  2.660, Epsilon: 0.31, Training Steps: 71,508, Elapsed Time: 00:01:10\n",
      "[Episode 770, Time Steps 73,759] Episode Reward: 291.0, Replay buffer: 30,000, Loss:  4.308, Epsilon: 0.31, Training Steps: 73,727, Elapsed Time: 00:01:13\n",
      "[Episode 780, Time Steps 76,211] Episode Reward: 254.0, Replay buffer: 30,000, Loss:  1.039, Epsilon: 0.30, Training Steps: 76,179, Elapsed Time: 00:01:15\n",
      "[Episode 790, Time Steps 79,057] Episode Reward: 285.0, Replay buffer: 30,000, Loss:  2.513, Epsilon: 0.29, Training Steps: 79,025, Elapsed Time: 00:01:18\n",
      "[Episode 800, Time Steps 82,043] Episode Reward: 294.0, Replay buffer: 30,000, Loss:  9.311, Epsilon: 0.28, Training Steps: 82,011, Elapsed Time: 00:01:21\n",
      "[Validation Episode Reward: [276. 265. 288.]] Average: 276.333\n",
      "[Episode 810, Time Steps 84,634] Episode Reward: 288.0, Replay buffer: 30,000, Loss:  2.343, Epsilon: 0.27, Training Steps: 84,602, Elapsed Time: 00:01:24\n",
      "[Episode 820, Time Steps 87,556] Episode Reward: 259.0, Replay buffer: 30,000, Loss: 35.648, Epsilon: 0.26, Training Steps: 87,524, Elapsed Time: 00:01:27\n",
      "[Episode 830, Time Steps 89,903] Episode Reward: 286.0, Replay buffer: 30,000, Loss:  1.837, Epsilon: 0.26, Training Steps: 89,871, Elapsed Time: 00:01:30\n",
      "[Episode 840, Time Steps 92,354] Episode Reward: 249.0, Replay buffer: 30,000, Loss:  1.702, Epsilon: 0.25, Training Steps: 92,322, Elapsed Time: 00:01:33\n",
      "[Episode 850, Time Steps 94,817] Episode Reward: 260.0, Replay buffer: 30,000, Loss: 16.958, Epsilon: 0.24, Training Steps: 94,785, Elapsed Time: 00:01:35\n",
      "[Validation Episode Reward: [264. 267. 277.]] Average: 269.333\n",
      "[Episode 860, Time Steps 97,820] Episode Reward: 250.0, Replay buffer: 30,000, Loss:  0.650, Epsilon: 0.23, Training Steps: 97,788, Elapsed Time: 00:01:38\n",
      "[Episode 870, Time Steps 100,693] Episode Reward: 280.0, Replay buffer: 30,000, Loss:  0.548, Epsilon: 0.22, Training Steps: 100,661, Elapsed Time: 00:01:41\n",
      "[Episode 880, Time Steps 103,937] Episode Reward: 389.0, Replay buffer: 30,000, Loss:  1.707, Epsilon: 0.21, Training Steps: 103,905, Elapsed Time: 00:01:45\n",
      "[Episode 890, Time Steps 106,651] Episode Reward: 291.0, Replay buffer: 30,000, Loss:  0.880, Epsilon: 0.21, Training Steps: 106,619, Elapsed Time: 00:01:48\n",
      "[Episode 900, Time Steps 109,304] Episode Reward: 250.0, Replay buffer: 30,000, Loss:  0.922, Epsilon: 0.20, Training Steps: 109,272, Elapsed Time: 00:01:50\n",
      "[Validation Episode Reward: [300. 271. 292.]] Average: 287.667\n",
      "[Episode 910, Time Steps 112,219] Episode Reward: 246.0, Replay buffer: 30,000, Loss:  0.460, Epsilon: 0.19, Training Steps: 112,187, Elapsed Time: 00:01:53\n",
      "[Episode 920, Time Steps 114,951] Episode Reward: 265.0, Replay buffer: 30,000, Loss:  0.346, Epsilon: 0.18, Training Steps: 114,919, Elapsed Time: 00:01:56\n",
      "[Episode 930, Time Steps 117,956] Episode Reward: 275.0, Replay buffer: 30,000, Loss:  0.068, Epsilon: 0.17, Training Steps: 117,924, Elapsed Time: 00:01:59\n",
      "[Episode 940, Time Steps 120,467] Episode Reward: 259.0, Replay buffer: 30,000, Loss:  0.633, Epsilon: 0.16, Training Steps: 120,435, Elapsed Time: 00:02:02\n",
      "[Episode 950, Time Steps 123,264] Episode Reward: 246.0, Replay buffer: 30,000, Loss:  0.349, Epsilon: 0.16, Training Steps: 123,232, Elapsed Time: 00:02:05\n",
      "[Validation Episode Reward: [252. 251. 248.]] Average: 250.333\n",
      "[Episode 960, Time Steps 125,941] Episode Reward: 260.0, Replay buffer: 30,000, Loss:  0.213, Epsilon: 0.15, Training Steps: 125,909, Elapsed Time: 00:02:08\n",
      "[Episode 970, Time Steps 128,760] Episode Reward: 315.0, Replay buffer: 30,000, Loss:  0.373, Epsilon: 0.14, Training Steps: 128,728, Elapsed Time: 00:02:11\n",
      "[Episode 980, Time Steps 131,884] Episode Reward: 316.0, Replay buffer: 30,000, Loss:  0.161, Epsilon: 0.13, Training Steps: 131,852, Elapsed Time: 00:02:14\n",
      "[Episode 990, Time Steps 134,565] Episode Reward: 285.0, Replay buffer: 30,000, Loss:  0.289, Epsilon: 0.12, Training Steps: 134,533, Elapsed Time: 00:02:17\n",
      "[Episode 1,000, Time Steps 137,408] Episode Reward: 263.0, Replay buffer: 30,000, Loss:  0.152, Epsilon: 0.11, Training Steps: 137,376, Elapsed Time: 00:02:20\n",
      "[Validation Episode Reward: [256. 267. 267.]] Average: 263.333\n",
      "[Episode 1,010, Time Steps 139,870] Episode Reward: 251.0, Replay buffer: 30,000, Loss:  0.158, Epsilon: 0.11, Training Steps: 139,838, Elapsed Time: 00:02:23\n",
      "[Episode 1,020, Time Steps 142,745] Episode Reward: 283.0, Replay buffer: 30,000, Loss:  0.299, Epsilon: 0.10, Training Steps: 142,713, Elapsed Time: 00:02:26\n",
      "[Episode 1,030, Time Steps 145,485] Episode Reward: 305.0, Replay buffer: 30,000, Loss:  0.079, Epsilon: 0.09, Training Steps: 145,453, Elapsed Time: 00:02:29\n",
      "[Episode 1,040, Time Steps 148,692] Episode Reward: 257.0, Replay buffer: 30,000, Loss:  0.068, Epsilon: 0.08, Training Steps: 148,660, Elapsed Time: 00:02:32\n",
      "[Episode 1,050, Time Steps 152,357] Episode Reward: 316.0, Replay buffer: 30,000, Loss:  0.466, Epsilon: 0.07, Training Steps: 152,325, Elapsed Time: 00:02:36\n",
      "[Validation Episode Reward: [283. 282. 306.]] Average: 290.333\n",
      "[Episode 1,060, Time Steps 155,673] Episode Reward: 396.0, Replay buffer: 30,000, Loss:  0.109, Epsilon: 0.06, Training Steps: 155,641, Elapsed Time: 00:02:39\n",
      "[Episode 1,070, Time Steps 159,167] Episode Reward: 368.0, Replay buffer: 30,000, Loss:  0.151, Epsilon: 0.06, Training Steps: 159,135, Elapsed Time: 00:02:43\n",
      "[Episode 1,080, Time Steps 162,873] Episode Reward: 387.0, Replay buffer: 30,000, Loss:  0.109, Epsilon: 0.05, Training Steps: 162,841, Elapsed Time: 00:02:47\n",
      "[Episode 1,090, Time Steps 166,354] Episode Reward: 337.0, Replay buffer: 30,000, Loss:  0.062, Epsilon: 0.04, Training Steps: 166,322, Elapsed Time: 00:02:51\n",
      "[Episode 1,100, Time Steps 170,902] Episode Reward: 500.0, Replay buffer: 30,000, Loss:  0.041, Epsilon: 0.03, Training Steps: 170,870, Elapsed Time: 00:02:56\n",
      "[Validation Episode Reward: [500. 500. 500.]] Average: 500.000\n",
      "Solved in 170,902 steps (170,870 training steps)!\n",
      "Total Training End : 00:02:56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Episode</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Training Steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>[TRAIN] Episode Reward</td><td>▁▁▁▁▁▁▂▁▂▂▂▁▁▁▁▂▁▃▃▂▄▁▁▃▆▂▅██▆▁▅▇▅▆▆▆▆▇▆</td></tr><tr><td>[TRAIN] Epsilon</td><td>███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>[TRAIN] Loss</td><td>▁▁▁▁▁▁▁▂▁▁▂▁▂▁▁▁▂▁██▂▃▃▂▂▂▁▁▁▃▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>[TRAIN] Replay buffer</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▄▄▅▅▆▆▇█████████████████████</td></tr><tr><td>[VALIDATION] Mean Episode Reward (3 Episodes)</td><td>▁▁▁▁▃▃▂██▆▆▆▆▄▄▄▄▆▆▅▅▅▆▆▇▇▆▆▆▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Episode</td><td>1100</td></tr><tr><td>Training Steps</td><td>170870</td></tr><tr><td>[TRAIN] Episode Reward</td><td>500.0</td></tr><tr><td>[TRAIN] Epsilon</td><td>0.03089</td></tr><tr><td>[TRAIN] Loss</td><td>0.04112</td></tr><tr><td>[TRAIN] Replay buffer</td><td>30000</td></tr><tr><td>[VALIDATION] Mean Episode Reward (3 Episodes)</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2023-11-07_08-05-02</strong> at: <a href='https://wandb.ai/gihwanjang/DQN_CartPole-v1/runs/25z3o3rv' target=\"_blank\">https://wandb.ai/gihwanjang/DQN_CartPole-v1/runs/25z3o3rv</a><br/> View job at <a href='https://wandb.ai/gihwanjang/DQN_CartPole-v1/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzIyMzU1NQ==/version_details/v0' target=\"_blank\">https://wandb.ai/gihwanjang/DQN_CartPole-v1/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzIyMzU1NQ==/version_details/v0</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_080505-25z3o3rv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympic",
   "language": "python",
   "name": "olympic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

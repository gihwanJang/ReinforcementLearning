{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16230158-bd81-45d6-ae8f-760058ff26f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c_policy_and_value.ipynb\n",
      "TORCH VERSION: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from shutil import copyfile\n",
    "\n",
    "import import_ipynb\n",
    "from c_policy_and_value import DEVICE, MODEL_DIR, Policy, Transition, Buffer, StateValueNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a433964-c97b-4c20-8f6d-42b8579b49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self, env, test_env, config, use_baseline, use_wandb):\n",
    "        self.env = env\n",
    "        self.test_env = test_env\n",
    "        self.use_baseline = use_baseline\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        self.env_name = config[\"env_name\"]\n",
    "\n",
    "        self.current_time = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "        if self.use_wandb:\n",
    "            self.wandb = wandb.init(\n",
    "                project=\"REINFORCE_{0}\".format(self.env_name),\n",
    "                name=self.current_time,\n",
    "                config=config\n",
    "            )\n",
    "\n",
    "        self.max_num_episodes = config[\"max_num_episodes\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.print_episode_interval = config[\"print_episode_interval\"]\n",
    "        self.train_num_episodes_before_next_test = config[\"train_num_episodes_before_next_test\"]\n",
    "        self.validation_num_episodes = config[\"validation_num_episodes\"]\n",
    "        self.episode_reward_avg_solved = config[\"episode_reward_avg_solved\"]\n",
    "\n",
    "        self.policy = Policy(n_features=3, n_actions=1)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        if self.use_baseline:\n",
    "            self.state_value_net = StateValueNet(n_features=3)\n",
    "            self.value_optimizer = optim.Adam(self.state_value_net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        self.buffer = Buffer()\n",
    "\n",
    "        self.time_steps = 0\n",
    "        self.training_time_steps = 0\n",
    "\n",
    "    def train_loop(self):\n",
    "        total_train_start_time = time.time()\n",
    "\n",
    "        validation_episode_reward_avg = -1500\n",
    "\n",
    "        is_terminated = False\n",
    "\n",
    "        for n_episode in range(1, self.max_num_episodes + 1):\n",
    "            episode_reward = 0\n",
    "\n",
    "            observation, _ = self.env.reset()\n",
    "\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                self.time_steps += 1\n",
    "\n",
    "                action = self.policy.get_action(observation)\n",
    "\n",
    "                next_observation, reward, terminated, truncated, _ = self.env.step(action * 2)\n",
    "\n",
    "                episode_reward += reward\n",
    "\n",
    "                transition = Transition(observation, action, next_observation, reward, terminated)\n",
    "\n",
    "                self.buffer.append(transition)\n",
    "\n",
    "                observation = next_observation\n",
    "                done = terminated or truncated\n",
    "\n",
    "            # TRAIN AFTER EPISODE DONE\n",
    "            policy_loss, avg_mu_v, avg_std_v, avg_action, avg_action_prob = self.train()\n",
    "            self.buffer.clear()\n",
    "\n",
    "            total_training_time = time.time() - total_train_start_time\n",
    "            total_training_time = time.strftime('%H:%M:%S', time.gmtime(total_training_time))\n",
    "\n",
    "            if n_episode % self.print_episode_interval == 0:\n",
    "                print(\n",
    "                    \"[Episode {:3,}, Steps {:6,}]\".format(n_episode, self.time_steps),\n",
    "                    \"Episode Reward: {:>9.3f},\".format(episode_reward),\n",
    "                    \"Policy Loss: {:>7.3f},\".format(policy_loss),\n",
    "                    \"Training Steps: {:5,}\".format(self.training_time_steps),\n",
    "                    \"Elapsed Time: {}\".format(total_training_time)\n",
    "                )\n",
    "\n",
    "            if n_episode % self.train_num_episodes_before_next_test == 0:\n",
    "                validation_episode_reward_lst, validation_episode_reward_avg = self.validate()\n",
    "\n",
    "                print(\"[Validation Episode Reward: {0}] Average: {1:.3f}\".format(\n",
    "                    validation_episode_reward_lst, validation_episode_reward_avg\n",
    "                ))\n",
    "\n",
    "                if validation_episode_reward_avg > self.episode_reward_avg_solved:\n",
    "                    print(\"Solved in {0:,} steps ({1:,} training steps)!\".format(\n",
    "                        self.time_steps, self.training_time_steps\n",
    "                    ))\n",
    "                    self.model_save(validation_episode_reward_avg)\n",
    "                    is_terminated = True\n",
    "\n",
    "            if self.use_wandb:\n",
    "                self.wandb.log({\n",
    "                    \"[VALIDATION] Mean Episode Reward ({0} Episodes)\".format(self.validation_num_episodes): validation_episode_reward_avg,\n",
    "                    \"[TRAIN] Episode Reward\": episode_reward,\n",
    "                    \"[TRAIN] Policy Loss\": policy_loss,\n",
    "                    \"[TRAIN] avg_mu_v\": avg_mu_v,\n",
    "                    \"[TRAIN] avg_std_v\": avg_std_v,\n",
    "                    \"[TRAIN] avg_action\": avg_action,\n",
    "                    \"[TRAIN] avg_action_prob\": avg_action_prob,\n",
    "                    \"Training Episode\": n_episode,\n",
    "                    \"Training Steps\": self.training_time_steps,\n",
    "                })\n",
    "\n",
    "            if is_terminated:\n",
    "                break\n",
    "\n",
    "        total_training_time = time.time() - total_train_start_time\n",
    "        total_training_time = time.strftime('%H:%M:%S', time.gmtime(total_training_time))\n",
    "        print(\"Total Training End : {}\".format(total_training_time))\n",
    "        self.wandb.finish()\n",
    "\n",
    "    def train(self):\n",
    "        self.training_time_steps += 1\n",
    "\n",
    "        observations, actions, next_observations, rewards, dones = self.buffer.get()\n",
    "\n",
    "        reversed_rewards = rewards.squeeze(dim=-1).cpu().numpy()[::-1]\n",
    "\n",
    "        G = 0.0\n",
    "        return_lst = []\n",
    "        for reward in reversed_rewards:\n",
    "            G = reward + self.gamma * G\n",
    "            return_lst.append(G)\n",
    "\n",
    "        returns = torch.tensor(return_lst[::-1], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        mu_v, std_v = self.policy.forward(observations)\n",
    "        dist = Normal(loc=mu_v, scale=std_v)\n",
    "        action_log_probs = dist.log_prob(value=actions).squeeze(dim=-1)  # natural log\n",
    "\n",
    "        #print(action_log_probs.mean(), torch.exp(action_log_probs).mean(), torch.normal(mu_v, std_v).mean())\n",
    "\n",
    "        if self.use_baseline:\n",
    "            values = self.state_value_net(observations).squeeze(dim=-1)\n",
    "            v_loss = F.mse_loss(values, returns)\n",
    "            self.value_optimizer.zero_grad()\n",
    "            v_loss.backward()\n",
    "            self.value_optimizer.step()\n",
    "\n",
    "            returns_baseline = returns - values.detach()\n",
    "            # normalization\n",
    "            returns_baseline = (returns_baseline - torch.mean(returns_baseline)) / (torch.std(returns_baseline) + 1e-7)\n",
    "            log_pi_returns = action_log_probs * returns_baseline\n",
    "            log_pi_returns_sum = log_pi_returns.sum()\n",
    "            # print(\n",
    "            #     returns.shape, values.shape, returns_baseline.shape, action_log_probs.shape, log_pi_returns.shape,\n",
    "            #     log_pi_returns_sum.shape, \"!!!\"\n",
    "            # )\n",
    "        else:\n",
    "            # normalization\n",
    "            returns = (returns - torch.mean(returns)) / (torch.std(returns) + 1e-7)\n",
    "            log_pi_returns = action_log_probs * returns\n",
    "            log_pi_returns_sum = log_pi_returns.sum()\n",
    "            # print(\n",
    "            # returns.shape, action_log_probs.shape, log_pi_returns.shape, log_pi_returns_sum.shape, \"!!!\"\n",
    "            # )\n",
    "\n",
    "        policy_loss = -1.0 * log_pi_returns_sum\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return (\n",
    "            policy_loss.item(),\n",
    "            mu_v.mean().item(),\n",
    "            std_v.mean().item(),\n",
    "            actions.type(torch.float32).mean().item(),\n",
    "            action_log_probs.exp().mean().item()\n",
    "        )\n",
    "\n",
    "    def model_save(self, validation_episode_reward_avg):\n",
    "        filename = \"reinforce_{0}_{1:4.1f}_{2}.pth\".format(\n",
    "            self.env_name, validation_episode_reward_avg, self.current_time\n",
    "        )\n",
    "        torch.save(self.policy.state_dict(), os.path.join(MODEL_DIR, filename))\n",
    "\n",
    "        copyfile(\n",
    "            src=os.path.join(MODEL_DIR, filename),\n",
    "            dst=os.path.join(MODEL_DIR, \"reinforce_{0}_latest.pth\".format(self.env_name))\n",
    "        )\n",
    "\n",
    "    def validate(self):\n",
    "        episode_reward_lst = np.zeros(shape=(self.validation_num_episodes,), dtype=float)\n",
    "\n",
    "        for i in range(self.validation_num_episodes):\n",
    "            episode_reward = 0\n",
    "\n",
    "            observation, _ = self.test_env.reset()\n",
    "\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                # action = self.policy.get_action(observation)\n",
    "                action = self.policy.get_action(observation, exploration=False)\n",
    "\n",
    "                next_observation, reward, terminated, truncated, _ = self.test_env.step(action * 2)\n",
    "\n",
    "                episode_reward += reward\n",
    "                observation = next_observation\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episode_reward_lst[i] = episode_reward\n",
    "\n",
    "        return episode_reward_lst, np.average(episode_reward_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37587e92-fa65-43f5-bff2-4f1a106b5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ENV_NAME = \"Pendulum-v1\"\n",
    "\n",
    "    # env\n",
    "    env = gym.make(ENV_NAME)\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "\n",
    "    config = {\n",
    "        \"env_name\": ENV_NAME,                       # 환경의 이름\n",
    "        \"max_num_episodes\": 200_000,                # 훈련을 위한 최대 에피소드 횟수\n",
    "        \"learning_rate\": 0.0003,                    # 학습율\n",
    "        \"gamma\": 0.99,                              # 감가율\n",
    "        \"print_episode_interval\": 20,               # Episode 통계 출력에 관한 에피소드 간격\n",
    "        \"train_num_episodes_before_next_test\": 100,                  # 검증 사이 마다 각 훈련 episode 간격\n",
    "        \"validation_num_episodes\": 3,               # 검증에 수행하는 에피소드 횟수\n",
    "        \"episode_reward_avg_solved\": -200,          # 훈련 종료를 위한 테스트 에피소드 리워드의 Average\n",
    "    }\n",
    "\n",
    "    use_wandb = True\n",
    "    reinforce = REINFORCE(\n",
    "        env=env, test_env=test_env, config=config, use_baseline=True, use_wandb=use_wandb\n",
    "    )\n",
    "    reinforce.train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e8e8d0-6fde-417b-9275-43823a623f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgihwan319\u001b[0m (\u001b[33mgihwanjang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_04_PG_REINFORCE/wandb/run-20231107_220439-8qrs9bj1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gihwanjang/REINFORCE_Pendulum-v1/runs/8qrs9bj1' target=\"_blank\">2023-11-07_22-04-38</a></strong> to <a href='https://wandb.ai/gihwanjang/REINFORCE_Pendulum-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gihwanjang/REINFORCE_Pendulum-v1' target=\"_blank\">https://wandb.ai/gihwanjang/REINFORCE_Pendulum-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gihwanjang/REINFORCE_Pendulum-v1/runs/8qrs9bj1' target=\"_blank\">https://wandb.ai/gihwanjang/REINFORCE_Pendulum-v1/runs/8qrs9bj1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode  20, Steps  4,000] Episode Reward: -1324.373, Policy Loss:  -0.519, Training Steps:    20 Elapsed Time: 00:00:00\n",
      "[Episode  40, Steps  8,000] Episode Reward: -1455.970, Policy Loss:   0.207, Training Steps:    40 Elapsed Time: 00:00:01\n",
      "[Episode  60, Steps 12,000] Episode Reward: -1653.249, Policy Loss:   0.592, Training Steps:    60 Elapsed Time: 00:00:01\n",
      "[Episode  80, Steps 16,000] Episode Reward:  -905.093, Policy Loss:   0.061, Training Steps:    80 Elapsed Time: 00:00:02\n",
      "[Episode 100, Steps 20,000] Episode Reward: -1070.467, Policy Loss:  -0.964, Training Steps:   100 Elapsed Time: 00:00:02\n",
      "[Validation Episode Reward: [-1478.03290436 -1681.59971208 -1877.88138813]] Average: -1679.171\n",
      "[Episode 120, Steps 24,000] Episode Reward: -1030.746, Policy Loss:   0.520, Training Steps:   120 Elapsed Time: 00:00:03\n",
      "[Episode 140, Steps 28,000] Episode Reward: -1187.997, Policy Loss:  -0.116, Training Steps:   140 Elapsed Time: 00:00:03\n",
      "[Episode 160, Steps 32,000] Episode Reward: -1040.865, Policy Loss:  -1.188, Training Steps:   160 Elapsed Time: 00:00:04\n",
      "[Episode 180, Steps 36,000] Episode Reward: -1491.949, Policy Loss:  -0.099, Training Steps:   180 Elapsed Time: 00:00:04\n",
      "[Episode 200, Steps 40,000] Episode Reward: -1416.450, Policy Loss:   0.565, Training Steps:   200 Elapsed Time: 00:00:05\n",
      "[Validation Episode Reward: [-1141.75572153 -1075.45622342 -1038.2316469 ]] Average: -1085.148\n",
      "[Episode 220, Steps 44,000] Episode Reward: -1278.061, Policy Loss:   0.239, Training Steps:   220 Elapsed Time: 00:00:05\n",
      "[Episode 240, Steps 48,000] Episode Reward: -1064.952, Policy Loss:  -1.463, Training Steps:   240 Elapsed Time: 00:00:06\n",
      "[Episode 260, Steps 52,000] Episode Reward: -1509.882, Policy Loss:  -0.724, Training Steps:   260 Elapsed Time: 00:00:06\n",
      "[Episode 280, Steps 56,000] Episode Reward: -1665.874, Policy Loss:  -0.703, Training Steps:   280 Elapsed Time: 00:00:06\n",
      "[Episode 300, Steps 60,000] Episode Reward: -1343.511, Policy Loss:   0.640, Training Steps:   300 Elapsed Time: 00:00:07\n",
      "[Validation Episode Reward: [-1433.27399557 -1898.59770846 -1334.61074349]] Average: -1555.494\n",
      "[Episode 320, Steps 64,000] Episode Reward: -1457.861, Policy Loss:  -0.252, Training Steps:   320 Elapsed Time: 00:00:07\n",
      "[Episode 340, Steps 68,000] Episode Reward: -1086.528, Policy Loss:  -0.725, Training Steps:   340 Elapsed Time: 00:00:08\n",
      "[Episode 360, Steps 72,000] Episode Reward: -1514.278, Policy Loss:  -0.083, Training Steps:   360 Elapsed Time: 00:00:08\n",
      "[Episode 380, Steps 76,000] Episode Reward: -1123.540, Policy Loss:  -2.273, Training Steps:   380 Elapsed Time: 00:00:09\n",
      "[Episode 400, Steps 80,000] Episode Reward: -1481.570, Policy Loss:  -0.112, Training Steps:   400 Elapsed Time: 00:00:09\n",
      "[Validation Episode Reward: [-1311.75172236 -1813.31197663 -1341.81549209]] Average: -1488.960\n",
      "[Episode 420, Steps 84,000] Episode Reward: -1528.260, Policy Loss:   0.792, Training Steps:   420 Elapsed Time: 00:00:10\n",
      "[Episode 440, Steps 88,000] Episode Reward: -1537.732, Policy Loss:  -0.762, Training Steps:   440 Elapsed Time: 00:00:10\n",
      "[Episode 460, Steps 92,000] Episode Reward: -1171.801, Policy Loss:  -2.066, Training Steps:   460 Elapsed Time: 00:00:11\n",
      "[Episode 480, Steps 96,000] Episode Reward: -1499.793, Policy Loss:  -0.260, Training Steps:   480 Elapsed Time: 00:00:11\n",
      "[Episode 500, Steps 100,000] Episode Reward: -1265.330, Policy Loss:  -2.353, Training Steps:   500 Elapsed Time: 00:00:12\n",
      "[Validation Episode Reward: [-1407.87595819 -1585.26922523 -1373.6394547 ]] Average: -1455.595\n",
      "[Episode 520, Steps 104,000] Episode Reward: -1415.955, Policy Loss:  -0.345, Training Steps:   520 Elapsed Time: 00:00:12\n",
      "[Episode 540, Steps 108,000] Episode Reward: -1510.085, Policy Loss:  -0.607, Training Steps:   540 Elapsed Time: 00:00:13\n",
      "[Episode 560, Steps 112,000] Episode Reward:  -904.190, Policy Loss:  -0.803, Training Steps:   560 Elapsed Time: 00:00:13\n",
      "[Episode 580, Steps 116,000] Episode Reward:  -871.811, Policy Loss:  -1.152, Training Steps:   580 Elapsed Time: 00:00:14\n",
      "[Episode 600, Steps 120,000] Episode Reward: -1095.722, Policy Loss:  -0.640, Training Steps:   600 Elapsed Time: 00:00:14\n",
      "[Validation Episode Reward: [ -907.51908383 -1278.42548131 -1311.61947475]] Average: -1165.855\n",
      "[Episode 620, Steps 124,000] Episode Reward: -1284.539, Policy Loss:  -0.631, Training Steps:   620 Elapsed Time: 00:00:15\n",
      "[Episode 640, Steps 128,000] Episode Reward: -1232.331, Policy Loss:  -0.961, Training Steps:   640 Elapsed Time: 00:00:15\n",
      "[Episode 660, Steps 132,000] Episode Reward: -1280.540, Policy Loss:   0.205, Training Steps:   660 Elapsed Time: 00:00:16\n",
      "[Episode 680, Steps 136,000] Episode Reward:  -976.916, Policy Loss:  -1.550, Training Steps:   680 Elapsed Time: 00:00:16\n",
      "[Episode 700, Steps 140,000] Episode Reward: -1312.348, Policy Loss:  -1.347, Training Steps:   700 Elapsed Time: 00:00:17\n",
      "[Validation Episode Reward: [-1106.04507364 -1247.71151528 -1212.76181263]] Average: -1188.839\n",
      "[Episode 720, Steps 144,000] Episode Reward: -1660.534, Policy Loss:  -0.315, Training Steps:   720 Elapsed Time: 00:00:17\n",
      "[Episode 740, Steps 148,000] Episode Reward: -1101.916, Policy Loss:  -1.213, Training Steps:   740 Elapsed Time: 00:00:18\n",
      "[Episode 760, Steps 152,000] Episode Reward:  -883.461, Policy Loss:  -2.128, Training Steps:   760 Elapsed Time: 00:00:18\n",
      "[Episode 780, Steps 156,000] Episode Reward: -1494.226, Policy Loss:  -0.509, Training Steps:   780 Elapsed Time: 00:00:19\n",
      "[Episode 800, Steps 160,000] Episode Reward: -1409.502, Policy Loss:   0.623, Training Steps:   800 Elapsed Time: 00:00:19\n",
      "[Validation Episode Reward: [-1041.31327083 -1367.71326263  -902.30561858]] Average: -1103.777\n",
      "[Episode 820, Steps 164,000] Episode Reward:  -870.815, Policy Loss:  -1.524, Training Steps:   820 Elapsed Time: 00:00:20\n",
      "[Episode 840, Steps 168,000] Episode Reward: -1111.430, Policy Loss:  -0.232, Training Steps:   840 Elapsed Time: 00:00:20\n",
      "[Episode 860, Steps 172,000] Episode Reward: -1018.452, Policy Loss:  -2.193, Training Steps:   860 Elapsed Time: 00:00:21\n",
      "[Episode 880, Steps 176,000] Episode Reward: -1494.561, Policy Loss:  -0.414, Training Steps:   880 Elapsed Time: 00:00:21\n",
      "[Episode 900, Steps 180,000] Episode Reward:  -916.925, Policy Loss:  -0.727, Training Steps:   900 Elapsed Time: 00:00:21\n",
      "[Validation Episode Reward: [ -774.24825052 -1239.09165543 -1184.99056433]] Average: -1066.110\n",
      "[Episode 920, Steps 184,000] Episode Reward: -1080.488, Policy Loss:  -1.902, Training Steps:   920 Elapsed Time: 00:00:22\n",
      "[Episode 940, Steps 188,000] Episode Reward:  -970.649, Policy Loss:  -1.066, Training Steps:   940 Elapsed Time: 00:00:22\n",
      "[Episode 960, Steps 192,000] Episode Reward: -1363.667, Policy Loss:  -1.028, Training Steps:   960 Elapsed Time: 00:00:23\n",
      "[Episode 980, Steps 196,000] Episode Reward:  -878.664, Policy Loss:  -1.661, Training Steps:   980 Elapsed Time: 00:00:23\n",
      "[Episode 1,000, Steps 200,000] Episode Reward: -1081.570, Policy Loss:  -1.231, Training Steps: 1,000 Elapsed Time: 00:00:24\n",
      "[Validation Episode Reward: [-1301.58277407 -1524.50654033  -966.57752402]] Average: -1264.222\n",
      "[Episode 1,020, Steps 204,000] Episode Reward:  -648.626, Policy Loss:  -0.352, Training Steps: 1,020 Elapsed Time: 00:00:24\n",
      "[Episode 1,040, Steps 208,000] Episode Reward:  -743.207, Policy Loss:  -1.365, Training Steps: 1,040 Elapsed Time: 00:00:25\n",
      "[Episode 1,060, Steps 212,000] Episode Reward: -1085.474, Policy Loss:  -0.969, Training Steps: 1,060 Elapsed Time: 00:00:25\n",
      "[Episode 1,080, Steps 216,000] Episode Reward: -1077.584, Policy Loss:  -1.247, Training Steps: 1,080 Elapsed Time: 00:00:26\n",
      "[Episode 1,100, Steps 220,000] Episode Reward:  -972.737, Policy Loss:  -5.033, Training Steps: 1,100 Elapsed Time: 00:00:26\n",
      "[Validation Episode Reward: [-1161.34851195 -1505.92653409 -1007.73695943]] Average: -1225.004\n",
      "[Episode 1,120, Steps 224,000] Episode Reward: -1513.454, Policy Loss:  -2.807, Training Steps: 1,120 Elapsed Time: 00:00:27\n",
      "[Episode 1,140, Steps 228,000] Episode Reward: -1017.880, Policy Loss:  -2.030, Training Steps: 1,140 Elapsed Time: 00:00:27\n",
      "[Episode 1,160, Steps 232,000] Episode Reward:  -640.111, Policy Loss:  -0.346, Training Steps: 1,160 Elapsed Time: 00:00:28\n",
      "[Episode 1,180, Steps 236,000] Episode Reward: -1371.500, Policy Loss:  -1.554, Training Steps: 1,180 Elapsed Time: 00:00:28\n",
      "[Episode 1,200, Steps 240,000] Episode Reward:  -905.421, Policy Loss:  -1.372, Training Steps: 1,200 Elapsed Time: 00:00:29\n",
      "[Validation Episode Reward: [ -645.66273054 -1500.17147577 -1035.23666303]] Average: -1060.357\n",
      "[Episode 1,220, Steps 244,000] Episode Reward: -1373.757, Policy Loss:  -2.347, Training Steps: 1,220 Elapsed Time: 00:00:29\n",
      "[Episode 1,240, Steps 248,000] Episode Reward: -1380.271, Policy Loss:  -2.479, Training Steps: 1,240 Elapsed Time: 00:00:30\n",
      "[Episode 1,260, Steps 252,000] Episode Reward: -1119.929, Policy Loss:  -1.999, Training Steps: 1,260 Elapsed Time: 00:00:30\n",
      "[Episode 1,280, Steps 256,000] Episode Reward:  -884.461, Policy Loss:  -2.959, Training Steps: 1,280 Elapsed Time: 00:00:31\n",
      "[Episode 1,300, Steps 260,000] Episode Reward: -1360.872, Policy Loss:   0.110, Training Steps: 1,300 Elapsed Time: 00:00:31\n",
      "[Validation Episode Reward: [  -17.49893883 -1039.92206346 -1057.24935118]] Average: -704.890\n",
      "[Episode 1,320, Steps 264,000] Episode Reward: -1188.491, Policy Loss:  -2.555, Training Steps: 1,320 Elapsed Time: 00:00:32\n",
      "[Episode 1,340, Steps 268,000] Episode Reward: -1009.135, Policy Loss:  -1.206, Training Steps: 1,340 Elapsed Time: 00:00:32\n",
      "[Episode 1,360, Steps 272,000] Episode Reward: -1358.550, Policy Loss:  -0.292, Training Steps: 1,360 Elapsed Time: 00:00:33\n",
      "[Episode 1,380, Steps 276,000] Episode Reward:  -797.910, Policy Loss:  -1.026, Training Steps: 1,380 Elapsed Time: 00:00:33\n",
      "[Episode 1,400, Steps 280,000] Episode Reward: -1443.963, Policy Loss:   1.683, Training Steps: 1,400 Elapsed Time: 00:00:34\n",
      "[Validation Episode Reward: [   -8.10555927    -5.2498639  -1092.17172382]] Average: -368.509\n",
      "[Episode 1,420, Steps 284,000] Episode Reward: -1332.308, Policy Loss:  -0.911, Training Steps: 1,420 Elapsed Time: 00:00:34\n",
      "[Episode 1,440, Steps 288,000] Episode Reward:  -986.069, Policy Loss:  -3.363, Training Steps: 1,440 Elapsed Time: 00:00:35\n",
      "[Episode 1,460, Steps 292,000] Episode Reward: -1739.283, Policy Loss:   1.447, Training Steps: 1,460 Elapsed Time: 00:00:35\n",
      "[Episode 1,480, Steps 296,000] Episode Reward: -1316.081, Policy Loss:   1.998, Training Steps: 1,480 Elapsed Time: 00:00:36\n",
      "[Episode 1,500, Steps 300,000] Episode Reward:  -991.615, Policy Loss:  -4.393, Training Steps: 1,500 Elapsed Time: 00:00:36\n",
      "[Validation Episode Reward: [-9.87155297e+02 -4.66616474e+00 -2.10927408e-01]] Average: -330.677\n",
      "[Episode 1,520, Steps 304,000] Episode Reward:  -875.671, Policy Loss:   1.572, Training Steps: 1,520 Elapsed Time: 00:00:37\n",
      "[Episode 1,540, Steps 308,000] Episode Reward: -1639.362, Policy Loss:  -1.761, Training Steps: 1,540 Elapsed Time: 00:00:37\n",
      "[Episode 1,560, Steps 312,000] Episode Reward: -1670.462, Policy Loss:   0.298, Training Steps: 1,560 Elapsed Time: 00:00:38\n",
      "[Episode 1,580, Steps 316,000] Episode Reward:  -988.803, Policy Loss:  -4.304, Training Steps: 1,580 Elapsed Time: 00:00:38\n",
      "[Episode 1,600, Steps 320,000] Episode Reward:  -987.339, Policy Loss:  -2.393, Training Steps: 1,600 Elapsed Time: 00:00:38\n",
      "[Validation Episode Reward: [-1507.40574752  -762.75239144 -1161.79077488]] Average: -1143.983\n",
      "[Episode 1,620, Steps 324,000] Episode Reward:  -979.085, Policy Loss:  -3.294, Training Steps: 1,620 Elapsed Time: 00:00:39\n",
      "[Episode 1,640, Steps 328,000] Episode Reward: -1454.482, Policy Loss:   1.837, Training Steps: 1,640 Elapsed Time: 00:00:39\n",
      "[Episode 1,660, Steps 332,000] Episode Reward: -1509.147, Policy Loss:   0.328, Training Steps: 1,660 Elapsed Time: 00:00:40\n",
      "[Episode 1,680, Steps 336,000] Episode Reward: -1266.770, Policy Loss:  -4.661, Training Steps: 1,680 Elapsed Time: 00:00:40\n",
      "[Episode 1,700, Steps 340,000] Episode Reward: -1107.865, Policy Loss:  -2.276, Training Steps: 1,700 Elapsed Time: 00:00:41\n",
      "[Validation Episode Reward: [-263.92860545 -383.44944372 -651.50934942]] Average: -432.962\n",
      "[Episode 1,720, Steps 344,000] Episode Reward: -1634.843, Policy Loss:  -2.115, Training Steps: 1,720 Elapsed Time: 00:00:41\n",
      "[Episode 1,740, Steps 348,000] Episode Reward: -1166.208, Policy Loss:  -1.806, Training Steps: 1,740 Elapsed Time: 00:00:42\n",
      "[Episode 1,760, Steps 352,000] Episode Reward: -1112.929, Policy Loss:  -0.779, Training Steps: 1,760 Elapsed Time: 00:00:42\n",
      "[Episode 1,780, Steps 356,000] Episode Reward:  -503.679, Policy Loss:  -1.427, Training Steps: 1,780 Elapsed Time: 00:00:43\n",
      "[Episode 1,800, Steps 360,000] Episode Reward: -1213.050, Policy Loss:   0.561, Training Steps: 1,800 Elapsed Time: 00:00:43\n",
      "[Validation Episode Reward: [-1.47003391e+03 -1.22117888e+03 -1.08313252e-01]] Average: -897.107\n",
      "[Episode 1,820, Steps 364,000] Episode Reward:  -876.224, Policy Loss:  -1.937, Training Steps: 1,820 Elapsed Time: 00:00:44\n",
      "[Episode 1,840, Steps 368,000] Episode Reward: -1506.748, Policy Loss:  -2.989, Training Steps: 1,840 Elapsed Time: 00:00:44\n",
      "[Episode 1,860, Steps 372,000] Episode Reward: -1097.559, Policy Loss:  -1.296, Training Steps: 1,860 Elapsed Time: 00:00:45\n",
      "[Episode 1,880, Steps 376,000] Episode Reward: -1343.072, Policy Loss:  -1.328, Training Steps: 1,880 Elapsed Time: 00:00:45\n",
      "[Episode 1,900, Steps 380,000] Episode Reward: -1358.949, Policy Loss:  -1.146, Training Steps: 1,900 Elapsed Time: 00:00:46\n",
      "[Validation Episode Reward: [-1274.1626856  -1127.75990595 -1450.5109035 ]] Average: -1284.144\n",
      "[Episode 1,920, Steps 384,000] Episode Reward: -1682.345, Policy Loss:  -1.553, Training Steps: 1,920 Elapsed Time: 00:00:46\n",
      "[Episode 1,940, Steps 388,000] Episode Reward: -1552.569, Policy Loss:   0.118, Training Steps: 1,940 Elapsed Time: 00:00:47\n",
      "[Episode 1,960, Steps 392,000] Episode Reward:  -857.204, Policy Loss:  -0.366, Training Steps: 1,960 Elapsed Time: 00:00:47\n",
      "[Episode 1,980, Steps 396,000] Episode Reward:  -649.633, Policy Loss:  -0.972, Training Steps: 1,980 Elapsed Time: 00:00:48\n",
      "[Episode 2,000, Steps 400,000] Episode Reward: -1198.903, Policy Loss:   1.041, Training Steps: 2,000 Elapsed Time: 00:00:48\n",
      "[Validation Episode Reward: [-1029.95125096 -1322.48010477  -882.20694479]] Average: -1078.213\n",
      "[Episode 2,020, Steps 404,000] Episode Reward: -1422.388, Policy Loss:   1.905, Training Steps: 2,020 Elapsed Time: 00:00:49\n",
      "[Episode 2,040, Steps 408,000] Episode Reward: -1512.229, Policy Loss:  -0.469, Training Steps: 2,040 Elapsed Time: 00:00:49\n",
      "[Episode 2,060, Steps 412,000] Episode Reward: -1361.240, Policy Loss:  -0.035, Training Steps: 2,060 Elapsed Time: 00:00:50\n",
      "[Episode 2,080, Steps 416,000] Episode Reward:  -861.906, Policy Loss:  -3.989, Training Steps: 2,080 Elapsed Time: 00:00:50\n",
      "[Episode 2,100, Steps 420,000] Episode Reward: -1401.152, Policy Loss:  -0.889, Training Steps: 2,100 Elapsed Time: 00:00:51\n",
      "[Validation Episode Reward: [  -4.28763889 -130.14249254 -526.42745487]] Average: -220.286\n",
      "[Episode 2,120, Steps 424,000] Episode Reward: -1319.428, Policy Loss:  -1.272, Training Steps: 2,120 Elapsed Time: 00:00:51\n",
      "[Episode 2,140, Steps 428,000] Episode Reward:  -857.695, Policy Loss:  -2.335, Training Steps: 2,140 Elapsed Time: 00:00:52\n",
      "[Episode 2,160, Steps 432,000] Episode Reward:  -984.997, Policy Loss:  -0.413, Training Steps: 2,160 Elapsed Time: 00:00:52\n",
      "[Episode 2,180, Steps 436,000] Episode Reward: -1311.851, Policy Loss:   0.002, Training Steps: 2,180 Elapsed Time: 00:00:53\n",
      "[Episode 2,200, Steps 440,000] Episode Reward: -1367.638, Policy Loss:   2.125, Training Steps: 2,200 Elapsed Time: 00:00:53\n",
      "[Validation Episode Reward: [-1398.18035438 -1143.69292163 -1320.7147945 ]] Average: -1287.529\n",
      "[Episode 2,220, Steps 444,000] Episode Reward: -1465.070, Policy Loss:   0.326, Training Steps: 2,220 Elapsed Time: 00:00:54\n",
      "[Episode 2,240, Steps 448,000] Episode Reward:  -863.109, Policy Loss:  -1.565, Training Steps: 2,240 Elapsed Time: 00:00:54\n",
      "[Episode 2,260, Steps 452,000] Episode Reward: -1567.964, Policy Loss:   1.238, Training Steps: 2,260 Elapsed Time: 00:00:55\n",
      "[Episode 2,280, Steps 456,000] Episode Reward:  -985.610, Policy Loss:   0.164, Training Steps: 2,280 Elapsed Time: 00:00:55\n",
      "[Episode 2,300, Steps 460,000] Episode Reward: -1102.493, Policy Loss:  -0.247, Training Steps: 2,300 Elapsed Time: 00:00:56\n",
      "[Validation Episode Reward: [-1.01623509e+03 -8.92387626e+02 -3.34761924e-01]] Average: -636.319\n",
      "[Episode 2,320, Steps 464,000] Episode Reward: -1202.974, Policy Loss:  -1.279, Training Steps: 2,320 Elapsed Time: 00:00:56\n",
      "[Episode 2,340, Steps 468,000] Episode Reward:  -865.244, Policy Loss:  -1.398, Training Steps: 2,340 Elapsed Time: 00:00:57\n",
      "[Episode 2,360, Steps 472,000] Episode Reward:  -932.469, Policy Loss:  -4.780, Training Steps: 2,360 Elapsed Time: 00:00:57\n",
      "[Episode 2,380, Steps 476,000] Episode Reward: -1481.637, Policy Loss:  -3.543, Training Steps: 2,380 Elapsed Time: 00:00:58\n",
      "[Episode 2,400, Steps 480,000] Episode Reward:  -758.782, Policy Loss:   0.507, Training Steps: 2,400 Elapsed Time: 00:00:58\n",
      "[Validation Episode Reward: [-1519.24055108    -5.5603944   -130.17655944]] Average: -551.659\n",
      "[Episode 2,420, Steps 484,000] Episode Reward: -1430.579, Policy Loss:  -1.356, Training Steps: 2,420 Elapsed Time: 00:00:58\n",
      "[Episode 2,440, Steps 488,000] Episode Reward: -1223.789, Policy Loss:   0.807, Training Steps: 2,440 Elapsed Time: 00:00:59\n",
      "[Episode 2,460, Steps 492,000] Episode Reward:  -781.533, Policy Loss:  -3.550, Training Steps: 2,460 Elapsed Time: 00:00:59\n",
      "[Episode 2,480, Steps 496,000] Episode Reward: -1044.853, Policy Loss:  -1.879, Training Steps: 2,480 Elapsed Time: 00:01:00\n",
      "[Episode 2,500, Steps 500,000] Episode Reward: -1152.353, Policy Loss:  -0.548, Training Steps: 2,500 Elapsed Time: 00:01:00\n",
      "[Validation Episode Reward: [-127.59659253 -392.60529714 -258.80285267]] Average: -259.668\n",
      "[Episode 2,520, Steps 504,000] Episode Reward:  -998.314, Policy Loss:  -2.892, Training Steps: 2,520 Elapsed Time: 00:01:01\n",
      "[Episode 2,540, Steps 508,000] Episode Reward: -1219.542, Policy Loss:  -1.312, Training Steps: 2,540 Elapsed Time: 00:01:01\n",
      "[Episode 2,560, Steps 512,000] Episode Reward: -1337.779, Policy Loss:  -0.193, Training Steps: 2,560 Elapsed Time: 00:01:02\n",
      "[Episode 2,580, Steps 516,000] Episode Reward:  -989.366, Policy Loss:  -0.101, Training Steps: 2,580 Elapsed Time: 00:01:02\n",
      "[Episode 2,600, Steps 520,000] Episode Reward: -1032.265, Policy Loss:   3.130, Training Steps: 2,600 Elapsed Time: 00:01:03\n",
      "[Validation Episode Reward: [-1.88522402e-01 -1.50280972e+03 -9.00224565e+02]] Average: -801.074\n",
      "[Episode 2,620, Steps 524,000] Episode Reward:  -761.913, Policy Loss:  -1.674, Training Steps: 2,620 Elapsed Time: 00:01:03\n",
      "[Episode 2,640, Steps 528,000] Episode Reward: -1018.549, Policy Loss:  -0.898, Training Steps: 2,640 Elapsed Time: 00:01:04\n",
      "[Episode 2,660, Steps 532,000] Episode Reward:  -768.222, Policy Loss:   0.979, Training Steps: 2,660 Elapsed Time: 00:01:04\n",
      "[Episode 2,680, Steps 536,000] Episode Reward: -1241.755, Policy Loss:   0.280, Training Steps: 2,680 Elapsed Time: 00:01:05\n",
      "[Episode 2,700, Steps 540,000] Episode Reward: -1444.831, Policy Loss:  -0.956, Training Steps: 2,700 Elapsed Time: 00:01:05\n",
      "[Validation Episode Reward: [-891.37248921 -828.26134042   -1.51866631]] Average: -573.717\n",
      "[Episode 2,720, Steps 544,000] Episode Reward: -1097.842, Policy Loss:  -0.551, Training Steps: 2,720 Elapsed Time: 00:01:06\n",
      "[Episode 2,740, Steps 548,000] Episode Reward: -1659.919, Policy Loss:  -1.368, Training Steps: 2,740 Elapsed Time: 00:01:06\n",
      "[Episode 2,760, Steps 552,000] Episode Reward: -1108.976, Policy Loss:   3.631, Training Steps: 2,760 Elapsed Time: 00:01:07\n",
      "[Episode 2,780, Steps 556,000] Episode Reward:  -840.833, Policy Loss:  -1.279, Training Steps: 2,780 Elapsed Time: 00:01:07\n",
      "[Episode 2,800, Steps 560,000] Episode Reward: -1026.107, Policy Loss:  -0.169, Training Steps: 2,800 Elapsed Time: 00:01:08\n",
      "[Validation Episode Reward: [-7.73550902e+02 -1.50001049e+03 -9.94063015e-01]] Average: -758.185\n",
      "[Episode 2,820, Steps 564,000] Episode Reward: -1658.251, Policy Loss:   2.033, Training Steps: 2,820 Elapsed Time: 00:01:08\n",
      "[Episode 2,840, Steps 568,000] Episode Reward: -1307.456, Policy Loss:  -3.940, Training Steps: 2,840 Elapsed Time: 00:01:09\n",
      "[Episode 2,860, Steps 572,000] Episode Reward: -1079.554, Policy Loss:  -1.255, Training Steps: 2,860 Elapsed Time: 00:01:09\n",
      "[Episode 2,880, Steps 576,000] Episode Reward:  -841.748, Policy Loss:  -1.778, Training Steps: 2,880 Elapsed Time: 00:01:10\n",
      "[Episode 2,900, Steps 580,000] Episode Reward:  -739.940, Policy Loss:   0.585, Training Steps: 2,900 Elapsed Time: 00:01:10\n",
      "[Validation Episode Reward: [   -1.85378371 -1495.13351487 -1492.79378145]] Average: -996.594\n",
      "[Episode 2,920, Steps 584,000] Episode Reward:  -848.968, Policy Loss:  -0.210, Training Steps: 2,920 Elapsed Time: 00:01:11\n",
      "[Episode 2,940, Steps 588,000] Episode Reward: -1074.868, Policy Loss:  -0.766, Training Steps: 2,940 Elapsed Time: 00:01:11\n",
      "[Episode 2,960, Steps 592,000] Episode Reward:  -967.912, Policy Loss:   0.268, Training Steps: 2,960 Elapsed Time: 00:01:12\n",
      "[Episode 2,980, Steps 596,000] Episode Reward:  -981.294, Policy Loss:  -1.899, Training Steps: 2,980 Elapsed Time: 00:01:12\n",
      "[Episode 3,000, Steps 600,000] Episode Reward: -1498.947, Policy Loss:  -2.350, Training Steps: 3,000 Elapsed Time: 00:01:13\n",
      "[Validation Episode Reward: [-124.59196912   -3.89293129 -651.64322779]] Average: -260.043\n",
      "[Episode 3,020, Steps 604,000] Episode Reward: -1108.459, Policy Loss:  -0.177, Training Steps: 3,020 Elapsed Time: 00:01:13\n",
      "[Episode 3,040, Steps 608,000] Episode Reward: -1365.776, Policy Loss:   0.597, Training Steps: 3,040 Elapsed Time: 00:01:13\n",
      "[Episode 3,060, Steps 612,000] Episode Reward:  -905.632, Policy Loss:  -1.066, Training Steps: 3,060 Elapsed Time: 00:01:14\n",
      "[Episode 3,080, Steps 616,000] Episode Reward: -1101.575, Policy Loss:  -2.854, Training Steps: 3,080 Elapsed Time: 00:01:14\n",
      "[Episode 3,100, Steps 620,000] Episode Reward:  -887.220, Policy Loss:  -1.943, Training Steps: 3,100 Elapsed Time: 00:01:15\n",
      "[Validation Episode Reward: [-1512.94612152  -263.81741213  -250.99275473]] Average: -675.919\n",
      "[Episode 3,120, Steps 624,000] Episode Reward:  -976.171, Policy Loss:   1.601, Training Steps: 3,120 Elapsed Time: 00:01:15\n",
      "[Episode 3,140, Steps 628,000] Episode Reward:  -992.480, Policy Loss:  -2.309, Training Steps: 3,140 Elapsed Time: 00:01:16\n",
      "[Episode 3,160, Steps 632,000] Episode Reward:  -934.297, Policy Loss:  -2.443, Training Steps: 3,160 Elapsed Time: 00:01:16\n",
      "[Episode 3,180, Steps 636,000] Episode Reward: -1013.649, Policy Loss:  -0.315, Training Steps: 3,180 Elapsed Time: 00:01:17\n",
      "[Episode 3,200, Steps 640,000] Episode Reward: -1010.616, Policy Loss:   0.112, Training Steps: 3,200 Elapsed Time: 00:01:17\n",
      "[Validation Episode Reward: [ -127.87391348 -1496.60912708 -1498.14749272]] Average: -1040.877\n",
      "[Episode 3,220, Steps 644,000] Episode Reward:  -914.575, Policy Loss:  -0.454, Training Steps: 3,220 Elapsed Time: 00:01:18\n",
      "[Episode 3,240, Steps 648,000] Episode Reward:  -636.609, Policy Loss:   1.438, Training Steps: 3,240 Elapsed Time: 00:01:18\n",
      "[Episode 3,260, Steps 652,000] Episode Reward:  -646.103, Policy Loss:  -2.384, Training Steps: 3,260 Elapsed Time: 00:01:19\n",
      "[Episode 3,280, Steps 656,000] Episode Reward: -1646.265, Policy Loss:  -0.326, Training Steps: 3,280 Elapsed Time: 00:01:19\n",
      "[Episode 3,300, Steps 660,000] Episode Reward: -1092.658, Policy Loss:  -3.388, Training Steps: 3,300 Elapsed Time: 00:01:20\n",
      "[Validation Episode Reward: [-260.84175117 -130.81497972 -380.42180696]] Average: -257.360\n",
      "[Episode 3,320, Steps 664,000] Episode Reward: -1688.319, Policy Loss:  -3.630, Training Steps: 3,320 Elapsed Time: 00:01:20\n",
      "[Episode 3,340, Steps 668,000] Episode Reward: -1043.864, Policy Loss:   0.126, Training Steps: 3,340 Elapsed Time: 00:01:21\n",
      "[Episode 3,360, Steps 672,000] Episode Reward:  -820.044, Policy Loss:  -1.248, Training Steps: 3,360 Elapsed Time: 00:01:21\n",
      "[Episode 3,380, Steps 676,000] Episode Reward: -1594.774, Policy Loss:   0.585, Training Steps: 3,380 Elapsed Time: 00:01:22\n",
      "[Episode 3,400, Steps 680,000] Episode Reward:  -859.360, Policy Loss:   2.365, Training Steps: 3,400 Elapsed Time: 00:01:22\n",
      "[Validation Episode Reward: [-258.70992849   -1.85850953 -259.53390584]] Average: -173.367\n",
      "Solved in 680,000 steps (3,400 training steps)!\n",
      "Total Training End : 00:01:22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Episode</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Training Steps</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>[TRAIN] Episode Reward</td><td>▆▁▄▄▅▅▄▁▆▃▅▃▁▆▅▄▅▆▃▄▆▄▁▄▇▃▆▃▆▅▅▅▅▆▇▃▇▆█▆</td></tr><tr><td>[TRAIN] Policy Loss</td><td>█▆▆▅▄█▅▅▅▆▅▅▄▄▆▄▃▁█▆▃▅▅▅▂▃▅▆▇▄▅▇▄▄▅▆█▆▂▅</td></tr><tr><td>[TRAIN] avg_action</td><td>▄█▇▇▆▅▆▅▆▃█▆▃▆▇▄▆▆▆▅▅▇▁▆▅▆█▄▅▆▆▇▅▆█▂▆▆▅▇</td></tr><tr><td>[TRAIN] avg_action_prob</td><td>▁▁▁▁▁▂▂▂▃▄▄▅▅▆▆▇▇▇██▇██▇█▇▇██▇███▇█▇█▇▇▇</td></tr><tr><td>[TRAIN] avg_mu_v</td><td>▄▆▆▆▅▆▆▄▆▄▆▅▁▅▅▅▆▆▅▅▅▅▁▅▅▅▇▅▅▅▆▆▅▅▇▂▆▅█▆</td></tr><tr><td>[TRAIN] avg_std_v</td><td>█████▇▆▆▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>[VALIDATION] Mean Episode Reward (3 Episodes)</td><td>▂▂▁▄▂▂▂▃▃▄▄▄▃▃▄▆▇▇▇▄▇▅▃▃▄█▃▆▆██▅▆▅▄█▆▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Episode</td><td>3400</td></tr><tr><td>Training Steps</td><td>3400</td></tr><tr><td>[TRAIN] Episode Reward</td><td>-859.35987</td></tr><tr><td>[TRAIN] Policy Loss</td><td>2.36467</td></tr><tr><td>[TRAIN] avg_action</td><td>-0.06</td></tr><tr><td>[TRAIN] avg_action_prob</td><td>0.18334</td></tr><tr><td>[TRAIN] avg_mu_v</td><td>-0.07785</td></tr><tr><td>[TRAIN] avg_std_v</td><td>2.0</td></tr><tr><td>[VALIDATION] Mean Episode Reward (3 Episodes)</td><td>-173.36745</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2023-11-07_22-04-38</strong> at: <a href='https://wandb.ai/gihwanjang/REINFORCE_Pendulum-v1/runs/8qrs9bj1' target=\"_blank\">https://wandb.ai/gihwanjang/REINFORCE_Pendulum-v1/runs/8qrs9bj1</a><br/> View job at <a href='https://wandb.ai/gihwanjang/REINFORCE_Pendulum-v1/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzQwMTg3Nw==/version_details/v0' target=\"_blank\">https://wandb.ai/gihwanjang/REINFORCE_Pendulum-v1/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzQwMTg3Nw==/version_details/v0</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_220439-8qrs9bj1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab65449-a6c1-4b82-85b9-ac090b877650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympic",
   "language": "python",
   "name": "olympic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

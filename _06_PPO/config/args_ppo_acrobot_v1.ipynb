{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad5043fb-620e-409e-95fb-26cd0e2de12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4225fc63-2d20-4701-b53b-605c3406520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from e_utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from e_utils import boolean_argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c81ae1e-822e-4965-83d9-ddb0a420a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    \"\"\"PPOAgent.\n",
    "        Parameters:\n",
    "            device: cpu or gpu acelator.\n",
    "            make_env: factory that produce environment.\n",
    "            continuous: True of environments with continuous action space.\n",
    "            obs_dim: dimension od observaion.\n",
    "            act_dim: dimension of action.\n",
    "            gamma: coef for discount factor.\n",
    "            lamda: coef for general adversial estimator (GAE).\n",
    "            entropy_coef: coef of weighting entropy in objective loss.\n",
    "            epsilon: clipping range for actor objective loss.\n",
    "            actor_lr: learnig rate for actor optimizer.\n",
    "            critic_lr: learnig rate for critic optimizer.\n",
    "            value_range: clipping range for critic objective loss.\n",
    "            rollout_len: num t-steps per one rollout.\n",
    "            total_rollouts: num rollouts.\n",
    "            num_epochs: num weights updation iteration for one policy update.\n",
    "            batch_size: data batch size for weights updating\n",
    "            actor: model for predction action.\n",
    "            critic: model for prediction state values.\n",
    "            plot_interval: interval for plotting train history.\n",
    "            solved_reward: desired reward.\n",
    "            plot_interval: plot history log every plot_interval rollouts.\n",
    "            path2save_train_history: path to save training history logs.\n",
    "            \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--env_name', type=str, default='Acrobot-v1', help='name of environment')\n",
    "\n",
    "    # device\n",
    "    parser.add_argument('--device', type=str, default='cpu', help='cpu or gpu acelator')\n",
    "\n",
    "    # coefficients\n",
    "    parser.add_argument('--gamma', type=float, default=0.99, help='coef for discount factor')\n",
    "    parser.add_argument('--lamda', type=float, default=0.95, help='coef for general adversial estimator (GAE)')\n",
    "    parser.add_argument('--entropy_coef', type=float, default=0.003, help='coef for general adversial estimator (GAE)')\n",
    "    parser.add_argument('--epsilon', type=float, default=0.12, help='clipping range for actor objective loss')\n",
    "    parser.add_argument('--value_range', type=float, default=0.5, help='clipping range for critic objective loss')\n",
    "\n",
    "    # other hyperparameters\n",
    "    parser.add_argument('--rollout_len', type=int, default=64, help='num t-steps per one rollout')\n",
    "    parser.add_argument('--total_rollouts', type=int, default=4000, help='num rollouts')\n",
    "    parser.add_argument('--num_epochs', type=int, default=12,\n",
    "                        help='num weights updation iteration for one policy update')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='data batch size for weights updating')\n",
    "\n",
    "    # agent net\n",
    "    parser.add_argument('--obs_dim', type=int, default=6, help='dimension od observaion')\n",
    "    parser.add_argument('--continuous', type=boolean_argument, default=False,\n",
    "                        help='True of environments with continuous action space')\n",
    "    parser.add_argument('--act_dim', type=int, default=3, help='dimension of action')\n",
    "\n",
    "    # agent nets optimizers\n",
    "    parser.add_argument('--actor_lr', type=float, default=1e-4, help='learning rate for actor optimizer')\n",
    "    parser.add_argument('--critic_lr', type=float, default=5e-4, help='learning rate for actor optimizer')\n",
    "\n",
    "    # etc.\n",
    "    parser.add_argument('--is_evaluate', type=boolean_argument, default=False, help='for evaluation')\n",
    "    parser.add_argument('--solved_reward', type=int, default=-80, help='desired reward')\n",
    "    parser.add_argument('--plot_interval', type=int, default=10, help='interval for plotting train history')\n",
    "    parser.add_argument('--print_episode_interval', type=int, default=10, help='interval for printing train history')\n",
    "\n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a0e59-6fb0-47cb-bf78-7f0594082f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympic",
   "language": "python",
   "name": "olympic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8751fdb8-85c1-4f89-844c-10ad51ce4a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c_ppo_utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import import_ipynb\n",
    "from a_actor_critic import ContinuousActor, DiscreteActor, Critic, init_weights, Memory\n",
    "from c_ppo_utils import get_gae, trajectories_data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0c0e23-e19e-4e54-98e7-3a399319b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(object):\n",
    "    \"\"\"PPOAgent.\n",
    "    Parameters:\n",
    "        device: cpu or gpu acelator.\n",
    "        make_env: factory that produce environment.\n",
    "        continuous: True of environments with continuous action space.\n",
    "        obs_dim: dimension od observaion.\n",
    "        act_dim: dimension of action.\n",
    "        gamma: coef for discount factor.\n",
    "        lamda: coef for general adversial estimator (GAE).\n",
    "        entropy_coef: coef of weighting entropy in objective loss.\n",
    "        epsilon: clipping range for actor objective loss.\n",
    "        actor_lr: learnig rate for actor optimizer.\n",
    "        critic_lr: learnig rate for critic optimizer.\n",
    "        value_range: clipping range for critic objective loss.\n",
    "        rollout_len: num t-steps per one rollout.\n",
    "        total_rollouts: num rollouts.\n",
    "        num_epochs: num weights updation iteration for one policy update.\n",
    "        batch_size: data batch size for weights updating\n",
    "        actor: model for predction action.\n",
    "        critic: model for prediction state values.\n",
    "        plot_interval: interval for plotting train history.\n",
    "        solved_reward: desired reward.\n",
    "        plot_interval: plot history log every plot_interval rollouts.\n",
    "        path2save_train_history: path to save training history logs.\n",
    "        \"\"\"\n",
    "    def __init__(self, make_env, args):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        \"\"\"\n",
    "        self.device = args.device\n",
    "        print(\"device:\", self.device)\n",
    "        self.env = make_env(args.env_name, render_mode=\"human\" if args.is_evaluate else None)\n",
    "\n",
    "        # coeffs\n",
    "        self.gamma = args.gamma\n",
    "        self.lamda = args.lamda\n",
    "        self.entropy_coef = args.entropy_coef\n",
    "        self.epsilon = args.epsilon\n",
    "        self.value_range = args.value_range\n",
    "        \n",
    "        # other hyperparameters\n",
    "        self.rollout_len = args.rollout_len\n",
    "        self.total_rollouts = args.total_rollouts\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        # agent nets\n",
    "        self.obs_dim = args.obs_dim\n",
    "\n",
    "        if args.continuous:\n",
    "            self.actor = ContinuousActor(self.obs_dim, args.act_dim).apply(\n",
    "                init_weights).to(self.device)\n",
    "        else:\n",
    "            self.actor = DiscreteActor(self.obs_dim, args.act_dim).apply(\n",
    "                init_weights).to(self.device)\n",
    "\n",
    "        self.critic = Critic(self.obs_dim).apply(init_weights).to(self.device)\n",
    "\n",
    "        # agent nets optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=args.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=args.critic_lr)\n",
    "\n",
    "        # memory of trajectory (s, a, r ...)\n",
    "        self.memory = Memory()\n",
    "\n",
    "        # memory of the train history\n",
    "        self.actor_loss_history = []\n",
    "        self.critic_loss_history = []\n",
    "        self.scores = []\n",
    "\n",
    "        self.is_evaluate = args.is_evaluate\n",
    "        self.solved_reward = args.solved_reward\n",
    "        self.plot_interval = args.plot_interval\n",
    "        self.print_episode_interval = args.print_episode_interval\n",
    "        self.path2save_train_history = args.path2save_train_history\n",
    "\n",
    "    def _get_action(self, state: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Get action from actor, and if not test -  \n",
    "        get state value from critic, collect elements of trajectory.\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action, dist = self.actor(state)\n",
    "\n",
    "        if not self.is_evaluate:\n",
    "            value = self.critic(state)\n",
    "           \n",
    "            # collect elements of trajectory\n",
    "            self.memory.states.append(state)\n",
    "            self.memory.actions.append(action)\n",
    "            self.memory.log_probs.append(dist.log_prob(action))\n",
    "            self.memory.values.append(value)\n",
    "\n",
    "        return list(action.detach().cpu().numpy()).pop()\n",
    "            \n",
    "    def _step(self, action: float):\n",
    "        \"\"\"\n",
    "        Make action in enviroment chosen by current policy,\n",
    "        if not evaluate - collect elements of trajectory.\n",
    "        \"\"\"\n",
    "        #print(action)\n",
    "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        if any([terminated, truncated]):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # add fake dim to match dimension with batch size\n",
    "        next_state = np.reshape(next_state, (1, -1)).astype(np.float64)\n",
    "        reward = np.reshape(reward, (1, -1)).astype(np.float64)\n",
    "        done = np.reshape(done, (1, -1))\n",
    "\n",
    "        if not self.is_evaluate:\n",
    "            # convert np.ndarray return from enviroment to torch tensor. \n",
    "            # collect elements of trajectory.\n",
    "            self.memory.rewards.append(torch.FloatTensor(reward).to(self.device))\n",
    "            self.memory.is_terminals.append(torch.FloatTensor(1 - done).to(self.device))\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Interaction process in enviroment for collect trajectory,\n",
    "        train process by agent nets after each rollout.\n",
    "        \"\"\"\n",
    "        total_train_start_time = time.time()\n",
    "\n",
    "        score = 0\n",
    "        state, _ = self.env.reset()\n",
    "        state = np.reshape(state, (1, -1))\n",
    "        num_episode = 0\n",
    "        time_step = 0\n",
    "        episode_reward_list = []\n",
    "        episode_reward = 0\n",
    "        print_episode_flag = False\n",
    "\n",
    "        for step_ in range(self.total_rollouts):\n",
    "            for _ in range(self.rollout_len):\n",
    "                action = self._get_action(state)\n",
    "                next_state, reward, done = self._step(action)\n",
    "\n",
    "                state = next_state\n",
    "                score += reward[0][0]\n",
    "                episode_reward += reward[0][0]\n",
    "\n",
    "                if done[0][0]:\n",
    "                    self.scores.append(score)\n",
    "                    score = 0\n",
    "                    state, _ = self.env.reset()\n",
    "                    state = np.reshape(state, (1, -1))\n",
    "                    num_episode += 1\n",
    "                    episode_reward_list.append(episode_reward)\n",
    "                    episode_reward = 0\n",
    "                    print_episode_flag = True\n",
    "\n",
    "                time_step += 1\n",
    "\n",
    "                total_training_time = time.time() - total_train_start_time\n",
    "                total_training_time = time.strftime('%H:%M:%S', time.gmtime(total_training_time))\n",
    "                if num_episode % self.print_episode_interval == 0 and print_episode_flag:\n",
    "                    print(\n",
    "                        \"[Episode {:3,}, Steps {:6,}]\".format(num_episode, time_step),\n",
    "                        \"Episode Reward: {:>9.3f},\".format(np.mean(episode_reward_list)),\n",
    "                        \"Elapsed Time: {}\".format(total_training_time)\n",
    "                    )\n",
    "                    print_episode_flag = False\n",
    "\n",
    "            if (step_ % self.plot_interval == 0):\n",
    "                self._plot_train_history()\n",
    "\n",
    "            # if we have achieved the desired score - stop the process.\n",
    "            if self.solved_reward is not None:\n",
    "                if np.mean(self.scores[-10:]) > self.solved_reward:\n",
    "                    print(\"It's solved!\")\n",
    "                    break\n",
    "\n",
    "            value = self.critic(torch.FloatTensor(next_state))\n",
    "            self.memory.values.append(value)\n",
    "            # update policy\n",
    "            self._update_weights()\n",
    "\n",
    "        self._save_train_history()\n",
    "        self.env.close()\n",
    "\n",
    "    def _update_weights(self):\n",
    "\n",
    "        returns = get_gae(\n",
    "            self.memory.rewards,\n",
    "            self.memory.values,\n",
    "            self.memory.is_terminals,\n",
    "            self.gamma,\n",
    "            self.lamda,\n",
    "        )\n",
    "        actor_losses, critic_losses = [], []\n",
    "\n",
    "        # flattening a list of torch.tensors into vectors\n",
    "        states = torch.cat(self.memory.states).view(-1, self.obs_dim)\n",
    "        actions = torch.cat(self.memory.actions)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        log_probs = torch.cat(self.memory.log_probs).detach()\n",
    "        values = torch.cat(self.memory.values).detach()\n",
    "        advantages = returns - values[:-1]\n",
    "\n",
    "        for state, action, return_, old_log_prob, old_value, advantage in trajectories_data_generator(\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            returns=returns,\n",
    "            log_probs=log_probs,\n",
    "            values=values,\n",
    "            advantages=advantages,\n",
    "            batch_size=self.batch_size,\n",
    "            num_epochs=self.num_epochs,\n",
    "            ):\n",
    "\n",
    "            # compute ratio (pi_theta / pi_theta__old)\n",
    "            _, dist = self.actor(state)\n",
    "            cur_log_prob = dist.log_prob(action)\n",
    "            ratio = torch.exp(cur_log_prob - old_log_prob)\n",
    "\n",
    "            # compute entropy\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            # compute actor loss\n",
    "            loss =  advantage * ratio\n",
    "            clipped_loss = (\n",
    "                torch.clamp(ratio, 1. - self.epsilon, 1. + self.epsilon)\n",
    "                 * advantage\n",
    "                )\n",
    "            actor_loss = (\n",
    "                -torch.mean(torch.min(loss, clipped_loss))\n",
    "                - entropy * self.entropy_coef)\n",
    "            \n",
    "            # critic loss, uncoment for clipped value loss too.\n",
    "            cur_value = self.critic(state)\n",
    "\n",
    "            critic_loss = (return_ - cur_value).pow(2).mean()\n",
    "\n",
    "            # actor optimizer step\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # critic optimizer step\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            actor_losses.append(actor_loss.item())\n",
    "            critic_losses.append(critic_loss.item())\n",
    "\n",
    "        # clean memory of trajectory\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "        # write mean losses in train history logs\n",
    "        actor_loss = sum(actor_losses) / len(actor_losses)\n",
    "        critic_loss = sum(critic_losses) / len(critic_losses)\n",
    "        self.actor_loss_history.append(actor_loss)\n",
    "        self.critic_loss_history.append(critic_loss)\n",
    "\n",
    "    def _plot_train_history(self):\n",
    "        data = [self.scores, self.actor_loss_history, self.critic_loss_history]\n",
    "        labels = [f\"score {np.mean(self.scores[-10:])}\",\n",
    "                  f\"actor loss {np.mean(self.actor_loss_history[-10:])}\", \n",
    "                  f\"critic loss {np.mean(self.critic_loss_history[-10:])}\",\n",
    "                  ]\n",
    "        clear_output(True)\n",
    "        with plt.style.context(\"seaborn-bright\"):\n",
    "            fig, axes = plt.subplots(3, 1, figsize=(6, 8))\n",
    "            for i, ax in enumerate(axes):\n",
    "                ax.plot(data[i], c=\"crimson\")\n",
    "                ax.set_title(labels[i])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def _save_train_history(self):\n",
    "        \"\"\"writing model weights and training logs to files.\"\"\"\n",
    "        torch.save(self.actor.state_dict(),\n",
    "                   f\"{self.path2save_train_history}/actor.pth\")\n",
    "        torch.save(self.critic.state_dict(),\n",
    "                   f\"{self.path2save_train_history}/critic.pth\")\n",
    "        \n",
    "        pd.DataFrame({\"actor loss\": self.actor_loss_history, \n",
    "                      \"critic loss\": self.critic_loss_history}\n",
    "                     ).to_csv(f\"{self.path2save_train_history}/loss_logs.csv\")\n",
    "\n",
    "        pd.DataFrame(\n",
    "            data=self.scores, columns=[\"scores\"]\n",
    "            ).to_csv(f\"{self.path2save_train_history}/score_logs.csv\")\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.is_evaluate = True\n",
    "\n",
    "        state, _ = self.env.reset()\n",
    "        state = np.reshape(state, (1, -1))\n",
    "        done = False\n",
    "\n",
    "        for _ in range(self.rollout_len):\n",
    "            while not done:\n",
    "                action = self._get_action(state)\n",
    "                next_state, reward, done = self._step(action)\n",
    "                state = next_state\n",
    "                state = np.reshape(state, (1, -1))\n",
    "\n",
    "            self.env.close()\n",
    "\n",
    "    def load_predtrain_model(self,\n",
    "                             actor_weights_path: str,\n",
    "                             critic_weights_path: str):\n",
    "\n",
    "        self.actor.load_state_dict(torch.load(actor_weights_path))\n",
    "        self.critic.load_state_dict(torch.load(critic_weights_path))\n",
    "        print(\"Predtrain models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df438f34-a47f-4883-9f71-7bb6c2368059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympic",
   "language": "python",
   "name": "olympic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

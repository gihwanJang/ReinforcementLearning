{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7d8094-4b51-4074-8cca-c9d6cbe8467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c_actor_and_critic.ipynb\n",
      "TORCH VERSION: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from shutil import copyfile\n",
    "\n",
    "import import_ipynb\n",
    "from c_actor_and_critic import MODEL_DIR, Actor, Critic, Transition, Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d41665d-6788-4dc2-8d46-270019537349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self, env, test_env, config, use_wandb):\n",
    "        self.env = env\n",
    "        self.test_env = test_env\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "        self.env_name = config[\"env_name\"]\n",
    "\n",
    "        self.current_time = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "        if self.use_wandb:\n",
    "            self.wandb = wandb.init(\n",
    "                project=\"A2C_{0}\".format(self.env_name),\n",
    "                name=self.current_time,\n",
    "                config=config\n",
    "            )\n",
    "\n",
    "        self.max_num_episodes = config[\"max_num_episodes\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.entropy_beta = config[\"entropy_beta\"]\n",
    "        self.print_episode_interval = config[\"print_episode_interval\"]\n",
    "        self.train_num_episodes_before_next_test = config[\"train_num_episodes_before_next_test\"]\n",
    "        self.validation_num_episodes = config[\"validation_num_episodes\"]\n",
    "        self.episode_reward_avg_solved = config[\"episode_reward_avg_solved\"]\n",
    "\n",
    "        self.actor = Actor(n_features=3, n_actions=1)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        self.critic = Critic(n_features=3)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        self.buffer = Buffer()\n",
    "\n",
    "        self.time_steps = 0\n",
    "        self.training_time_steps = 0\n",
    "\n",
    "    def train_loop(self):\n",
    "        total_train_start_time = time.time()\n",
    "\n",
    "        validation_episode_reward_avg = -1500\n",
    "        policy_loss = avg_mu_v = avg_std_v = avg_action = avg_action_prob = 0.0\n",
    "\n",
    "        is_terminated = False\n",
    "\n",
    "        for n_episode in range(1, self.max_num_episodes + 1):\n",
    "            episode_reward = 0\n",
    "\n",
    "            observation, _ = self.env.reset()\n",
    "\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                self.time_steps += 1\n",
    "\n",
    "                action = self.actor.get_action(observation)\n",
    "\n",
    "                next_observation, reward, terminated, truncated, _ = self.env.step(action * 2)\n",
    "\n",
    "                episode_reward += reward\n",
    "\n",
    "                transition = Transition(observation, action, next_observation, reward, terminated)\n",
    "\n",
    "                self.buffer.append(transition)\n",
    "\n",
    "                observation = next_observation\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if self.time_steps % self.batch_size == 0:\n",
    "                    policy_loss, avg_mu_v, avg_std_v, avg_action, avg_action_prob = self.train()\n",
    "                    self.buffer.clear()\n",
    "\n",
    "            total_training_time = time.time() - total_train_start_time\n",
    "            total_training_time = time.strftime('%H:%M:%S', time.gmtime(total_training_time))\n",
    "\n",
    "            if n_episode % self.print_episode_interval == 0:\n",
    "                print(\n",
    "                    \"[Episode {:3,}, Steps {:6,}]\".format(n_episode, self.time_steps),\n",
    "                    \"Episode Reward: {:>9.3f},\".format(episode_reward),\n",
    "                    \"Policy Loss: {:>7.3f},\".format(policy_loss),\n",
    "                    \"Training Steps: {:5,}, \".format(self.training_time_steps),\n",
    "                    \"Elapsed Time: {}\".format(total_training_time)\n",
    "                )\n",
    "\n",
    "            if n_episode % self.train_num_episodes_before_next_test == 0:\n",
    "                validation_episode_reward_lst, validation_episode_reward_avg = self.validate()\n",
    "\n",
    "                print(\"[Validation Episode Reward: {0}] Average: {1:.3f}\".format(\n",
    "                    validation_episode_reward_lst, validation_episode_reward_avg\n",
    "                ))\n",
    "\n",
    "                if validation_episode_reward_avg > self.episode_reward_avg_solved:\n",
    "                    print(\"Solved in {0:,} steps ({1:,} training steps)!\".format(\n",
    "                        self.time_steps, self.training_time_steps\n",
    "                    ))\n",
    "                    self.model_save(validation_episode_reward_avg)\n",
    "                    is_terminated = True\n",
    "\n",
    "            if self.use_wandb:\n",
    "                self.wandb.log({\n",
    "                    \"[VALIDATION] Mean Episode Reward ({0} Episodes)\".format(self.validation_num_episodes): validation_episode_reward_avg,\n",
    "                    \"[TRAIN] Episode Reward\": episode_reward,\n",
    "                    \"[TRAIN] Policy Loss\": policy_loss,\n",
    "                    \"[TRAIN] avg_mu_v\": avg_mu_v,\n",
    "                    \"[TRAIN] avg_std_v\": avg_std_v,\n",
    "                    \"[TRAIN] avg_action\": avg_action,\n",
    "                    \"[TRAIN] avg_action_prob\": avg_action_prob,\n",
    "                    \"Training Episode\": n_episode,\n",
    "                    \"Training Steps\": self.training_time_steps,\n",
    "                })\n",
    "\n",
    "            if is_terminated:\n",
    "                break\n",
    "\n",
    "        total_training_time = time.time() - total_train_start_time\n",
    "        total_training_time = time.strftime('%H:%M:%S', time.gmtime(total_training_time))\n",
    "        print(\"Total Training End : {}\".format(total_training_time))\n",
    "        self.wandb.finish()\n",
    "\n",
    "    def train(self):\n",
    "        self.training_time_steps += 1\n",
    "\n",
    "        observations, actions, next_observations, rewards, dones = self.buffer.get()\n",
    "\n",
    "        ### CRITIC UPDATE\n",
    "        values = self.critic(observations).squeeze(dim=-1)\n",
    "        next_values = self.critic(next_observations).squeeze(dim=-1)\n",
    "        next_values[dones] = 0.0\n",
    "        q_values = rewards.squeeze(dim=-1) + self.gamma * next_values\n",
    "        advantages = q_values - values\n",
    "        critic_loss = F.mse_loss(q_values.detach(), values)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        ### ACTOR UPDATE\n",
    "        advantages = (advantages - torch.mean(advantages)) / (torch.std(advantages) + 1e-7)\n",
    "        mu_v, std_v = self.actor.forward(observations)\n",
    "        dist = Normal(loc=mu_v, scale=std_v)\n",
    "        action_log_probs = dist.log_prob(value=actions).squeeze(dim=-1)  # natural log\n",
    "\n",
    "        log_pi_advantages = action_log_probs * advantages.detach()\n",
    "        log_pi_advantages_sum = log_pi_advantages.sum()\n",
    "\n",
    "        entropy = dist.entropy().squeeze(dim=-1)\n",
    "        entropy_sum = entropy.sum()\n",
    "\n",
    "        # print(\n",
    "        #     q_values.shape, values.shape, advantages.shape, values.shape, action_log_probs.shape, log_pi_advantages.shape,\n",
    "        #     entropy.shape, entropy_sum.shape, log_pi_advantages_sum.shape, \"!!!\"\n",
    "        # )\n",
    "\n",
    "        actor_loss = -1.0 * log_pi_advantages_sum - 1.0 * entropy_sum * self.entropy_beta\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return (\n",
    "            actor_loss.item(),\n",
    "            mu_v.mean().item(),\n",
    "            std_v.mean().item(),\n",
    "            actions.type(torch.float32).mean().item(),\n",
    "            action_log_probs.exp().mean().item()\n",
    "        )\n",
    "\n",
    "    def model_save(self, validation_episode_reward_avg):\n",
    "        filename = \"a2c_{0}_{1:4.1f}_{2}.pth\".format(\n",
    "            self.env_name, validation_episode_reward_avg, self.current_time\n",
    "        )\n",
    "        torch.save(self.actor.state_dict(), os.path.join(MODEL_DIR, filename))\n",
    "\n",
    "        copyfile(\n",
    "            src=os.path.join(MODEL_DIR, filename),\n",
    "            dst=os.path.join(MODEL_DIR, \"a2c_{0}_latest.pth\".format(self.env_name))\n",
    "        )\n",
    "\n",
    "    def validate(self):\n",
    "        episode_reward_lst = np.zeros(shape=(self.validation_num_episodes,), dtype=float)\n",
    "\n",
    "        for i in range(self.validation_num_episodes):\n",
    "            episode_reward = 0\n",
    "\n",
    "            observation, _ = self.test_env.reset()\n",
    "\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                # action = self.actor.get_action(observation)\n",
    "                action = self.actor.get_action(observation, exploration=False)\n",
    "\n",
    "                next_observation, reward, terminated, truncated, _ = self.test_env.step(action * 2)\n",
    "\n",
    "                episode_reward += reward\n",
    "                observation = next_observation\n",
    "                done = terminated or truncated\n",
    "\n",
    "            episode_reward_lst[i] = episode_reward\n",
    "\n",
    "        return episode_reward_lst, np.average(episode_reward_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5036d36d-a8a7-46d6-9253-c95c456516a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ENV_NAME = \"Pendulum-v1\"\n",
    "\n",
    "    # env\n",
    "    env = gym.make(ENV_NAME)\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "\n",
    "    config = {\n",
    "        \"env_name\": ENV_NAME,                       # 환경의 이름\n",
    "        \"max_num_episodes\": 200_000,                # 훈련을 위한 최대 에피소드 횟수\n",
    "        \"batch_size\": 32,                           # 훈련시 배치에서 한번에 가져오는 랜덤 배치 사이즈\n",
    "        \"learning_rate\": 0.0003,                    # 학습율\n",
    "        \"gamma\": 0.99,                              # 감가율\n",
    "        \"entropy_beta\": 0.05,                     # 엔트로피 가중치\n",
    "        \"print_episode_interval\": 20,               # Episode 통계 출력에 관한 에피소드 간격\n",
    "        \"train_num_episodes_before_next_test\": 100,                  # 검증 사이 마다 각 훈련 episode 간격\n",
    "        \"validation_num_episodes\": 3,               # 검증에 수행하는 에피소드 횟수\n",
    "        \"episode_reward_avg_solved\": -50,          # 훈련 종료를 위한 테스트 에피소드 리워드의 Average\n",
    "    }\n",
    "\n",
    "    use_wandb = True\n",
    "    a2c = A2C(\n",
    "        env=env, test_env=test_env, config=config, use_wandb=use_wandb\n",
    "    )\n",
    "    a2c.train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32d091d-9d36-42aa-a5dc-0042de09f3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgihwan319\u001b[0m (\u001b[33mgihwanjang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jang-gihwan/Desktop/DeepLearing/ReinforcementLearning/_05_A2C/wandb/run-20231108_090115-pkbk0y78</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gihwanjang/A2C_Pendulum-v1/runs/pkbk0y78' target=\"_blank\">2023-11-08_09-01-13</a></strong> to <a href='https://wandb.ai/gihwanjang/A2C_Pendulum-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gihwanjang/A2C_Pendulum-v1' target=\"_blank\">https://wandb.ai/gihwanjang/A2C_Pendulum-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gihwanjang/A2C_Pendulum-v1/runs/pkbk0y78' target=\"_blank\">https://wandb.ai/gihwanjang/A2C_Pendulum-v1/runs/pkbk0y78</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode  20, Steps  4,000] Episode Reward: -1565.021, Policy Loss:  -4.873, Training Steps:   125,  Elapsed Time: 00:00:00\n",
      "[Episode  40, Steps  8,000] Episode Reward: -1505.066, Policy Loss:  -5.995, Training Steps:   250,  Elapsed Time: 00:00:01\n",
      "[Episode  60, Steps 12,000] Episode Reward: -1718.928, Policy Loss:  -6.901, Training Steps:   375,  Elapsed Time: 00:00:02\n",
      "[Episode  80, Steps 16,000] Episode Reward: -1541.775, Policy Loss:  -6.550, Training Steps:   500,  Elapsed Time: 00:00:02\n",
      "[Episode 100, Steps 20,000] Episode Reward: -1342.154, Policy Loss:  -6.890, Training Steps:   625,  Elapsed Time: 00:00:03\n",
      "[Validation Episode Reward: [-1493.10223051 -1495.26630298 -1518.30558928]] Average: -1502.225\n",
      "[Episode 120, Steps 24,000] Episode Reward: -1575.584, Policy Loss:  -7.623, Training Steps:   750,  Elapsed Time: 00:00:04\n",
      "[Episode 140, Steps 28,000] Episode Reward: -1169.373, Policy Loss:  -5.061, Training Steps:   875,  Elapsed Time: 00:00:04\n",
      "[Episode 160, Steps 32,000] Episode Reward: -1686.730, Policy Loss:  -6.193, Training Steps: 1,000,  Elapsed Time: 00:00:05\n",
      "[Episode 180, Steps 36,000] Episode Reward: -1516.364, Policy Loss:  -7.078, Training Steps: 1,125,  Elapsed Time: 00:00:06\n",
      "[Episode 200, Steps 40,000] Episode Reward: -1533.822, Policy Loss:  -5.541, Training Steps: 1,250,  Elapsed Time: 00:00:07\n",
      "[Validation Episode Reward: [-1531.79510433 -1492.78171072 -1485.63355876]] Average: -1503.403\n",
      "[Episode 220, Steps 44,000] Episode Reward: -1439.521, Policy Loss:  -7.468, Training Steps: 1,375,  Elapsed Time: 00:00:07\n",
      "[Episode 240, Steps 48,000] Episode Reward: -1697.096, Policy Loss:  -6.121, Training Steps: 1,500,  Elapsed Time: 00:00:08\n",
      "[Episode 260, Steps 52,000] Episode Reward: -1212.810, Policy Loss:  -4.999, Training Steps: 1,625,  Elapsed Time: 00:00:09\n",
      "[Episode 280, Steps 56,000] Episode Reward: -1651.377, Policy Loss:  -4.603, Training Steps: 1,750,  Elapsed Time: 00:00:09\n",
      "[Episode 300, Steps 60,000] Episode Reward: -1588.740, Policy Loss:  -5.120, Training Steps: 1,875,  Elapsed Time: 00:00:10\n",
      "[Validation Episode Reward: [-1339.14977869 -1384.99259181 -1307.22037356]] Average: -1343.788\n",
      "[Episode 320, Steps 64,000] Episode Reward: -1101.311, Policy Loss:  -5.664, Training Steps: 2,000,  Elapsed Time: 00:00:11\n",
      "[Episode 340, Steps 68,000] Episode Reward: -1095.393, Policy Loss:  -4.167, Training Steps: 2,125,  Elapsed Time: 00:00:11\n",
      "[Episode 360, Steps 72,000] Episode Reward: -1088.366, Policy Loss:  -5.019, Training Steps: 2,250,  Elapsed Time: 00:00:12\n",
      "[Episode 380, Steps 76,000] Episode Reward: -1371.819, Policy Loss:  -4.521, Training Steps: 2,375,  Elapsed Time: 00:00:13\n",
      "[Episode 400, Steps 80,000] Episode Reward: -1129.719, Policy Loss:  -2.312, Training Steps: 2,500,  Elapsed Time: 00:00:13\n",
      "[Validation Episode Reward: [-1411.02370556 -1199.02950431 -1272.59915305]] Average: -1294.217\n",
      "[Episode 420, Steps 84,000] Episode Reward: -1337.399, Policy Loss:  -4.706, Training Steps: 2,625,  Elapsed Time: 00:00:14\n",
      "[Episode 440, Steps 88,000] Episode Reward: -1036.116, Policy Loss:  -4.265, Training Steps: 2,750,  Elapsed Time: 00:00:15\n",
      "[Episode 460, Steps 92,000] Episode Reward: -1190.990, Policy Loss:  -2.134, Training Steps: 2,875,  Elapsed Time: 00:00:16\n",
      "[Episode 480, Steps 96,000] Episode Reward: -1167.259, Policy Loss:  -1.123, Training Steps: 3,000,  Elapsed Time: 00:00:16\n",
      "[Episode 500, Steps 100,000] Episode Reward: -1088.675, Policy Loss:  -1.892, Training Steps: 3,125,  Elapsed Time: 00:00:17\n",
      "[Validation Episode Reward: [-1625.64249362 -1622.78020838 -1595.35599239]] Average: -1614.593\n",
      "[Episode 520, Steps 104,000] Episode Reward: -1326.304, Policy Loss:  -1.537, Training Steps: 3,250,  Elapsed Time: 00:00:18\n",
      "[Episode 540, Steps 108,000] Episode Reward: -1267.293, Policy Loss:  -1.984, Training Steps: 3,375,  Elapsed Time: 00:00:18\n",
      "[Episode 560, Steps 112,000] Episode Reward: -1243.994, Policy Loss:  -2.435, Training Steps: 3,500,  Elapsed Time: 00:00:19\n",
      "[Episode 580, Steps 116,000] Episode Reward: -1066.850, Policy Loss:  -1.423, Training Steps: 3,625,  Elapsed Time: 00:00:20\n",
      "[Episode 600, Steps 120,000] Episode Reward: -1139.205, Policy Loss:  -4.254, Training Steps: 3,750,  Elapsed Time: 00:00:20\n",
      "[Validation Episode Reward: [ -901.83363923 -1008.551092   -1008.54234957]] Average: -972.976\n",
      "[Episode 620, Steps 124,000] Episode Reward: -1402.389, Policy Loss:  -3.576, Training Steps: 3,875,  Elapsed Time: 00:00:21\n",
      "[Episode 640, Steps 128,000] Episode Reward: -1377.696, Policy Loss:  -4.963, Training Steps: 4,000,  Elapsed Time: 00:00:22\n",
      "[Episode 660, Steps 132,000] Episode Reward: -1684.007, Policy Loss:  -3.944, Training Steps: 4,125,  Elapsed Time: 00:00:23\n",
      "[Episode 680, Steps 136,000] Episode Reward: -1565.139, Policy Loss:  -3.756, Training Steps: 4,250,  Elapsed Time: 00:00:23\n",
      "[Episode 700, Steps 140,000] Episode Reward: -1467.427, Policy Loss:  -4.840, Training Steps: 4,375,  Elapsed Time: 00:00:24\n",
      "[Validation Episode Reward: [-1637.27856365 -1617.38017349 -1063.88472258]] Average: -1439.514\n",
      "[Episode 720, Steps 144,000] Episode Reward: -1272.573, Policy Loss:  -5.531, Training Steps: 4,500,  Elapsed Time: 00:00:25\n",
      "[Episode 740, Steps 148,000] Episode Reward: -1574.437, Policy Loss:  -5.287, Training Steps: 4,625,  Elapsed Time: 00:00:25\n",
      "[Episode 760, Steps 152,000] Episode Reward: -1449.726, Policy Loss:  -6.433, Training Steps: 4,750,  Elapsed Time: 00:00:26\n",
      "[Episode 780, Steps 156,000] Episode Reward: -1536.489, Policy Loss:  -4.785, Training Steps: 4,875,  Elapsed Time: 00:00:27\n",
      "[Episode 800, Steps 160,000] Episode Reward: -1409.053, Policy Loss:  -5.047, Training Steps: 5,000,  Elapsed Time: 00:00:28\n",
      "[Validation Episode Reward: [-1164.06251562 -1202.65744136 -1006.50398534]] Average: -1124.408\n",
      "[Episode 820, Steps 164,000] Episode Reward: -1103.055, Policy Loss:  -6.000, Training Steps: 5,125,  Elapsed Time: 00:00:28\n",
      "[Episode 840, Steps 168,000] Episode Reward: -1208.738, Policy Loss:  -3.568, Training Steps: 5,250,  Elapsed Time: 00:00:29\n",
      "[Episode 860, Steps 172,000] Episode Reward: -1100.661, Policy Loss:  -3.390, Training Steps: 5,375,  Elapsed Time: 00:00:30\n",
      "[Episode 880, Steps 176,000] Episode Reward: -1312.597, Policy Loss:  -4.491, Training Steps: 5,500,  Elapsed Time: 00:00:30\n",
      "[Episode 900, Steps 180,000] Episode Reward: -1393.793, Policy Loss:  -5.215, Training Steps: 5,625,  Elapsed Time: 00:00:31\n",
      "[Validation Episode Reward: [-1206.17902189 -1279.32498671 -1385.5452928 ]] Average: -1290.350\n",
      "[Episode 920, Steps 184,000] Episode Reward: -1563.631, Policy Loss:  -7.321, Training Steps: 5,750,  Elapsed Time: 00:00:32\n",
      "[Episode 940, Steps 188,000] Episode Reward: -1493.269, Policy Loss:  -5.769, Training Steps: 5,875,  Elapsed Time: 00:00:32\n",
      "[Episode 960, Steps 192,000] Episode Reward:  -924.848, Policy Loss:  -4.421, Training Steps: 6,000,  Elapsed Time: 00:00:33\n",
      "[Episode 980, Steps 196,000] Episode Reward: -1465.672, Policy Loss:  -6.137, Training Steps: 6,125,  Elapsed Time: 00:00:34\n",
      "[Episode 1,000, Steps 200,000] Episode Reward: -1268.944, Policy Loss:  -5.148, Training Steps: 6,250,  Elapsed Time: 00:00:34\n",
      "[Validation Episode Reward: [-1068.39622834 -1052.30752987 -1032.41809388]] Average: -1051.041\n",
      "[Episode 1,020, Steps 204,000] Episode Reward: -1249.926, Policy Loss:  -3.302, Training Steps: 6,375,  Elapsed Time: 00:00:35\n",
      "[Episode 1,040, Steps 208,000] Episode Reward: -1219.658, Policy Loss:  -3.828, Training Steps: 6,500,  Elapsed Time: 00:00:36\n",
      "[Episode 1,060, Steps 212,000] Episode Reward: -1478.815, Policy Loss:  -2.745, Training Steps: 6,625,  Elapsed Time: 00:00:37\n",
      "[Episode 1,080, Steps 216,000] Episode Reward: -1475.173, Policy Loss:  -1.569, Training Steps: 6,750,  Elapsed Time: 00:00:37\n",
      "[Episode 1,100, Steps 220,000] Episode Reward: -1434.187, Policy Loss:  -6.995, Training Steps: 6,875,  Elapsed Time: 00:00:38\n",
      "[Validation Episode Reward: [-1340.30709438 -1069.66948334  -703.06416971]] Average: -1037.680\n",
      "[Episode 1,120, Steps 224,000] Episode Reward: -1326.347, Policy Loss:  -4.934, Training Steps: 7,000,  Elapsed Time: 00:00:39\n",
      "[Episode 1,140, Steps 228,000] Episode Reward: -1194.861, Policy Loss:  -4.548, Training Steps: 7,125,  Elapsed Time: 00:00:39\n",
      "[Episode 1,160, Steps 232,000] Episode Reward: -1100.902, Policy Loss:  -4.767, Training Steps: 7,250,  Elapsed Time: 00:00:40\n",
      "[Episode 1,180, Steps 236,000] Episode Reward: -1184.623, Policy Loss:  -4.116, Training Steps: 7,375,  Elapsed Time: 00:00:41\n",
      "[Episode 1,200, Steps 240,000] Episode Reward: -1238.227, Policy Loss:  -4.216, Training Steps: 7,500,  Elapsed Time: 00:00:41\n",
      "[Validation Episode Reward: [-1378.6486785  -1321.94363575 -1377.58548259]] Average: -1359.393\n",
      "[Episode 1,220, Steps 244,000] Episode Reward: -1086.943, Policy Loss:  -4.977, Training Steps: 7,625,  Elapsed Time: 00:00:42\n",
      "[Episode 1,240, Steps 248,000] Episode Reward: -1289.413, Policy Loss:  -4.637, Training Steps: 7,750,  Elapsed Time: 00:00:43\n",
      "[Episode 1,260, Steps 252,000] Episode Reward: -1216.471, Policy Loss:  -6.027, Training Steps: 7,875,  Elapsed Time: 00:00:44\n",
      "[Episode 1,280, Steps 256,000] Episode Reward: -1163.532, Policy Loss:  -6.071, Training Steps: 8,000,  Elapsed Time: 00:00:44\n",
      "[Episode 1,300, Steps 260,000] Episode Reward: -1138.596, Policy Loss:  -6.721, Training Steps: 8,125,  Elapsed Time: 00:00:45\n",
      "[Validation Episode Reward: [-1.13654229e+03 -4.62752583e-01 -1.07762961e+03]] Average: -738.212\n",
      "[Episode 1,320, Steps 264,000] Episode Reward: -1220.119, Policy Loss:  -3.914, Training Steps: 8,250,  Elapsed Time: 00:00:46\n",
      "[Episode 1,340, Steps 268,000] Episode Reward: -1217.297, Policy Loss:  -5.793, Training Steps: 8,375,  Elapsed Time: 00:00:46\n",
      "[Episode 1,360, Steps 272,000] Episode Reward: -1102.003, Policy Loss:  -2.052, Training Steps: 8,500,  Elapsed Time: 00:00:47\n",
      "[Episode 1,380, Steps 276,000] Episode Reward: -1177.642, Policy Loss:  -4.569, Training Steps: 8,625,  Elapsed Time: 00:00:48\n",
      "[Episode 1,400, Steps 280,000] Episode Reward:  -648.795, Policy Loss:  -3.437, Training Steps: 8,750,  Elapsed Time: 00:00:48\n",
      "[Validation Episode Reward: [-931.09566025 -826.02973944 -925.86432582]] Average: -894.330\n",
      "[Episode 1,420, Steps 284,000] Episode Reward:  -982.510, Policy Loss:  -4.794, Training Steps: 8,875,  Elapsed Time: 00:00:49\n",
      "[Episode 1,440, Steps 288,000] Episode Reward: -1271.490, Policy Loss:  -5.061, Training Steps: 9,000,  Elapsed Time: 00:00:50\n",
      "[Episode 1,460, Steps 292,000] Episode Reward: -1235.637, Policy Loss:  -4.811, Training Steps: 9,125,  Elapsed Time: 00:00:51\n",
      "[Episode 1,480, Steps 296,000] Episode Reward:  -952.718, Policy Loss:  -5.547, Training Steps: 9,250,  Elapsed Time: 00:00:51\n",
      "[Episode 1,500, Steps 300,000] Episode Reward:  -909.457, Policy Loss:  -5.238, Training Steps: 9,375,  Elapsed Time: 00:00:52\n",
      "[Validation Episode Reward: [-904.85568722 -930.52273281 -873.04678992]] Average: -902.808\n",
      "[Episode 1,520, Steps 304,000] Episode Reward:  -754.966, Policy Loss:  -6.944, Training Steps: 9,500,  Elapsed Time: 00:00:53\n",
      "[Episode 1,540, Steps 308,000] Episode Reward: -1327.518, Policy Loss:  -3.689, Training Steps: 9,625,  Elapsed Time: 00:00:53\n",
      "[Episode 1,560, Steps 312,000] Episode Reward: -1154.906, Policy Loss:  -4.537, Training Steps: 9,750,  Elapsed Time: 00:00:54\n",
      "[Episode 1,580, Steps 316,000] Episode Reward:  -784.031, Policy Loss:  -4.317, Training Steps: 9,875,  Elapsed Time: 00:00:55\n",
      "[Episode 1,600, Steps 320,000] Episode Reward:  -944.416, Policy Loss:  -4.521, Training Steps: 10,000,  Elapsed Time: 00:00:55\n",
      "[Validation Episode Reward: [-1296.10505701  -997.10055842 -1254.03707595]] Average: -1182.414\n",
      "[Episode 1,620, Steps 324,000] Episode Reward: -1101.329, Policy Loss:  -4.932, Training Steps: 10,125,  Elapsed Time: 00:00:56\n",
      "[Episode 1,640, Steps 328,000] Episode Reward: -1226.818, Policy Loss:  -5.245, Training Steps: 10,250,  Elapsed Time: 00:00:57\n",
      "[Episode 1,660, Steps 332,000] Episode Reward: -1362.662, Policy Loss:  -3.896, Training Steps: 10,375,  Elapsed Time: 00:00:58\n",
      "[Episode 1,680, Steps 336,000] Episode Reward: -1113.435, Policy Loss:  -6.325, Training Steps: 10,500,  Elapsed Time: 00:00:58\n",
      "[Episode 1,700, Steps 340,000] Episode Reward: -1014.901, Policy Loss:  -4.736, Training Steps: 10,625,  Elapsed Time: 00:00:59\n",
      "[Validation Episode Reward: [-777.79972931 -772.8732942  -703.3784477 ]] Average: -751.350\n",
      "[Episode 1,720, Steps 344,000] Episode Reward:  -986.413, Policy Loss:  -4.889, Training Steps: 10,750,  Elapsed Time: 00:01:00\n",
      "[Episode 1,740, Steps 348,000] Episode Reward:  -966.717, Policy Loss:  -5.951, Training Steps: 10,875,  Elapsed Time: 00:01:00\n",
      "[Episode 1,760, Steps 352,000] Episode Reward:  -763.446, Policy Loss:  -5.103, Training Steps: 11,000,  Elapsed Time: 00:01:01\n",
      "[Episode 1,780, Steps 356,000] Episode Reward: -1218.943, Policy Loss:  -5.508, Training Steps: 11,125,  Elapsed Time: 00:01:02\n",
      "[Episode 1,800, Steps 360,000] Episode Reward:  -784.290, Policy Loss:  -4.764, Training Steps: 11,250,  Elapsed Time: 00:01:02\n",
      "[Validation Episode Reward: [-1216.92986922 -1216.3004697  -1222.26538178]] Average: -1218.499\n",
      "[Episode 1,820, Steps 364,000] Episode Reward: -1120.921, Policy Loss:  -4.479, Training Steps: 11,375,  Elapsed Time: 00:01:03\n",
      "[Episode 1,840, Steps 368,000] Episode Reward: -1192.292, Policy Loss:  -5.344, Training Steps: 11,500,  Elapsed Time: 00:01:04\n",
      "[Episode 1,860, Steps 372,000] Episode Reward: -1127.790, Policy Loss:  -5.696, Training Steps: 11,625,  Elapsed Time: 00:01:05\n",
      "[Episode 1,880, Steps 376,000] Episode Reward:  -903.605, Policy Loss:  -4.634, Training Steps: 11,750,  Elapsed Time: 00:01:05\n",
      "[Episode 1,900, Steps 380,000] Episode Reward: -1119.289, Policy Loss:  -5.039, Training Steps: 11,875,  Elapsed Time: 00:01:06\n",
      "[Validation Episode Reward: [ -961.48611249 -1087.36519882 -1044.74934359]] Average: -1031.200\n",
      "[Episode 1,920, Steps 384,000] Episode Reward:  -916.723, Policy Loss:  -3.183, Training Steps: 12,000,  Elapsed Time: 00:01:07\n",
      "[Episode 1,940, Steps 388,000] Episode Reward:  -977.748, Policy Loss:  -4.209, Training Steps: 12,125,  Elapsed Time: 00:01:07\n",
      "[Episode 1,960, Steps 392,000] Episode Reward: -1131.622, Policy Loss:  -4.898, Training Steps: 12,250,  Elapsed Time: 00:01:08\n",
      "[Episode 1,980, Steps 396,000] Episode Reward:  -971.762, Policy Loss:  -4.193, Training Steps: 12,375,  Elapsed Time: 00:01:09\n",
      "[Episode 2,000, Steps 400,000] Episode Reward: -1040.449, Policy Loss:  -5.616, Training Steps: 12,500,  Elapsed Time: 00:01:09\n",
      "[Validation Episode Reward: [ -129.21145036 -1078.08605215  -124.32297815]] Average: -443.873\n",
      "[Episode 2,020, Steps 404,000] Episode Reward:  -930.954, Policy Loss:  -5.970, Training Steps: 12,625,  Elapsed Time: 00:01:10\n",
      "[Episode 2,040, Steps 408,000] Episode Reward:  -875.797, Policy Loss:  -3.437, Training Steps: 12,750,  Elapsed Time: 00:01:11\n",
      "[Episode 2,060, Steps 412,000] Episode Reward:  -893.547, Policy Loss:  -5.693, Training Steps: 12,875,  Elapsed Time: 00:01:12\n",
      "[Episode 2,080, Steps 416,000] Episode Reward:  -897.797, Policy Loss:  -2.504, Training Steps: 13,000,  Elapsed Time: 00:01:12\n",
      "[Episode 2,100, Steps 420,000] Episode Reward:  -830.365, Policy Loss:  -4.402, Training Steps: 13,125,  Elapsed Time: 00:01:13\n",
      "[Validation Episode Reward: [ -239.25279107  -140.70433484 -1238.23092027]] Average: -539.396\n",
      "[Episode 2,120, Steps 424,000] Episode Reward:  -982.342, Policy Loss:  -6.255, Training Steps: 13,250,  Elapsed Time: 00:01:14\n",
      "[Episode 2,140, Steps 428,000] Episode Reward:  -630.814, Policy Loss:  -5.031, Training Steps: 13,375,  Elapsed Time: 00:01:14\n",
      "[Episode 2,160, Steps 432,000] Episode Reward:  -701.867, Policy Loss:  -5.762, Training Steps: 13,500,  Elapsed Time: 00:01:15\n",
      "[Episode 2,180, Steps 436,000] Episode Reward: -1254.972, Policy Loss:  -7.124, Training Steps: 13,625,  Elapsed Time: 00:01:16\n",
      "[Episode 2,200, Steps 440,000] Episode Reward:  -687.162, Policy Loss:  -6.196, Training Steps: 13,750,  Elapsed Time: 00:01:16\n",
      "[Validation Episode Reward: [  -1.05906997 -132.43300027 -243.74037811]] Average: -125.744\n",
      "[Episode 2,220, Steps 444,000] Episode Reward:  -770.494, Policy Loss:  -4.562, Training Steps: 13,875,  Elapsed Time: 00:01:17\n",
      "[Episode 2,240, Steps 448,000] Episode Reward:  -879.450, Policy Loss:  -3.696, Training Steps: 14,000,  Elapsed Time: 00:01:18\n",
      "[Episode 2,260, Steps 452,000] Episode Reward:  -874.231, Policy Loss:  -5.513, Training Steps: 14,125,  Elapsed Time: 00:01:19\n",
      "[Episode 2,280, Steps 456,000] Episode Reward:  -966.156, Policy Loss:  -6.248, Training Steps: 14,250,  Elapsed Time: 00:01:19\n",
      "[Episode 2,300, Steps 460,000] Episode Reward:  -749.475, Policy Loss:  -6.845, Training Steps: 14,375,  Elapsed Time: 00:01:20\n",
      "[Validation Episode Reward: [-8.42887097e-02 -1.23007419e+02 -1.30679308e+02]] Average: -84.590\n",
      "[Episode 2,320, Steps 464,000] Episode Reward:  -861.640, Policy Loss:  -6.047, Training Steps: 14,500,  Elapsed Time: 00:01:21\n",
      "[Episode 2,340, Steps 468,000] Episode Reward:  -975.750, Policy Loss:  -4.792, Training Steps: 14,625,  Elapsed Time: 00:01:21\n",
      "[Episode 2,360, Steps 472,000] Episode Reward:  -883.538, Policy Loss:  -7.143, Training Steps: 14,750,  Elapsed Time: 00:01:22\n",
      "[Episode 2,380, Steps 476,000] Episode Reward:  -907.479, Policy Loss:  -5.293, Training Steps: 14,875,  Elapsed Time: 00:01:23\n",
      "[Episode 2,400, Steps 480,000] Episode Reward:  -895.743, Policy Loss:  -4.775, Training Steps: 15,000,  Elapsed Time: 00:01:23\n",
      "[Validation Episode Reward: [ -238.27843155 -1078.04641602  -125.61023215]] Average: -480.645\n",
      "[Episode 2,420, Steps 484,000] Episode Reward:  -750.513, Policy Loss:  -5.067, Training Steps: 15,125,  Elapsed Time: 00:01:24\n",
      "[Episode 2,440, Steps 488,000] Episode Reward:  -884.695, Policy Loss:  -4.946, Training Steps: 15,250,  Elapsed Time: 00:01:25\n",
      "[Episode 2,460, Steps 492,000] Episode Reward:  -864.673, Policy Loss:  -5.390, Training Steps: 15,375,  Elapsed Time: 00:01:25\n",
      "[Episode 2,480, Steps 496,000] Episode Reward:  -843.464, Policy Loss:  -4.987, Training Steps: 15,500,  Elapsed Time: 00:01:26\n",
      "[Episode 2,500, Steps 500,000] Episode Reward:  -584.867, Policy Loss:  -5.041, Training Steps: 15,625,  Elapsed Time: 00:01:27\n",
      "[Validation Episode Reward: [-250.35780236 -344.47211045 -133.32012656]] Average: -242.717\n",
      "[Episode 2,520, Steps 504,000] Episode Reward:  -880.909, Policy Loss:  -1.967, Training Steps: 15,750,  Elapsed Time: 00:01:28\n",
      "[Episode 2,540, Steps 508,000] Episode Reward:  -762.742, Policy Loss:  -7.275, Training Steps: 15,875,  Elapsed Time: 00:01:28\n",
      "[Episode 2,560, Steps 512,000] Episode Reward:  -977.692, Policy Loss:  -5.332, Training Steps: 16,000,  Elapsed Time: 00:01:29\n",
      "[Episode 2,580, Steps 516,000] Episode Reward: -1037.104, Policy Loss:  -5.770, Training Steps: 16,125,  Elapsed Time: 00:01:30\n",
      "[Episode 2,600, Steps 520,000] Episode Reward:  -642.970, Policy Loss:  -6.229, Training Steps: 16,250,  Elapsed Time: 00:01:30\n",
      "[Validation Episode Reward: [-132.08682405   -2.27441567 -128.95029294]] Average: -87.771\n",
      "[Episode 2,620, Steps 524,000] Episode Reward:  -772.274, Policy Loss:  -7.035, Training Steps: 16,375,  Elapsed Time: 00:01:31\n",
      "[Episode 2,640, Steps 528,000] Episode Reward:  -876.711, Policy Loss:  -4.186, Training Steps: 16,500,  Elapsed Time: 00:01:32\n",
      "[Episode 2,660, Steps 532,000] Episode Reward:  -874.951, Policy Loss:  -6.919, Training Steps: 16,625,  Elapsed Time: 00:01:32\n",
      "[Episode 2,680, Steps 536,000] Episode Reward:  -639.006, Policy Loss:  -7.091, Training Steps: 16,750,  Elapsed Time: 00:01:33\n",
      "[Episode 2,700, Steps 540,000] Episode Reward:  -921.508, Policy Loss:  -4.898, Training Steps: 16,875,  Elapsed Time: 00:01:34\n",
      "[Validation Episode Reward: [-252.86559903 -120.60192117 -269.41811405]] Average: -214.295\n",
      "[Episode 2,720, Steps 544,000] Episode Reward:  -548.264, Policy Loss:  -4.749, Training Steps: 17,000,  Elapsed Time: 00:01:35\n",
      "[Episode 2,740, Steps 548,000] Episode Reward: -1005.676, Policy Loss:  -5.510, Training Steps: 17,125,  Elapsed Time: 00:01:35\n",
      "[Episode 2,760, Steps 552,000] Episode Reward:  -892.191, Policy Loss:  -5.025, Training Steps: 17,250,  Elapsed Time: 00:01:36\n",
      "[Episode 2,780, Steps 556,000] Episode Reward:  -765.113, Policy Loss:  -6.402, Training Steps: 17,375,  Elapsed Time: 00:01:37\n",
      "[Episode 2,800, Steps 560,000] Episode Reward:  -896.465, Policy Loss:  -5.633, Training Steps: 17,500,  Elapsed Time: 00:01:37\n",
      "[Validation Episode Reward: [-281.86507432 -362.42986454 -122.51855286]] Average: -255.604\n",
      "[Episode 2,820, Steps 564,000] Episode Reward:  -863.771, Policy Loss:  -5.873, Training Steps: 17,625,  Elapsed Time: 00:01:38\n",
      "[Episode 2,840, Steps 568,000] Episode Reward:  -749.906, Policy Loss:  -4.263, Training Steps: 17,750,  Elapsed Time: 00:01:39\n",
      "[Episode 2,860, Steps 572,000] Episode Reward:  -976.200, Policy Loss:  -6.567, Training Steps: 17,875,  Elapsed Time: 00:01:39\n",
      "[Episode 2,880, Steps 576,000] Episode Reward:  -678.181, Policy Loss:  -3.382, Training Steps: 18,000,  Elapsed Time: 00:01:40\n",
      "[Episode 2,900, Steps 580,000] Episode Reward: -1170.401, Policy Loss:  -5.963, Training Steps: 18,125,  Elapsed Time: 00:01:41\n",
      "[Validation Episode Reward: [ -125.25469574 -1062.82299912  -121.85127058]] Average: -436.643\n",
      "[Episode 2,920, Steps 584,000] Episode Reward:  -646.502, Policy Loss:  -6.151, Training Steps: 18,250,  Elapsed Time: 00:01:42\n",
      "[Episode 2,940, Steps 588,000] Episode Reward:  -746.890, Policy Loss:  -5.702, Training Steps: 18,375,  Elapsed Time: 00:01:42\n",
      "[Episode 2,960, Steps 592,000] Episode Reward:  -944.725, Policy Loss:  -6.470, Training Steps: 18,500,  Elapsed Time: 00:01:43\n",
      "[Episode 2,980, Steps 596,000] Episode Reward:  -646.132, Policy Loss:  -4.840, Training Steps: 18,625,  Elapsed Time: 00:01:44\n",
      "[Episode 3,000, Steps 600,000] Episode Reward:  -863.952, Policy Loss:  -5.632, Training Steps: 18,750,  Elapsed Time: 00:01:44\n",
      "[Validation Episode Reward: [-115.02892817 -241.6087348  -119.20026467]] Average: -158.613\n",
      "[Episode 3,020, Steps 604,000] Episode Reward:  -866.316, Policy Loss:  -3.269, Training Steps: 18,875,  Elapsed Time: 00:01:45\n",
      "[Episode 3,040, Steps 608,000] Episode Reward:  -861.281, Policy Loss:  -3.253, Training Steps: 19,000,  Elapsed Time: 00:01:46\n",
      "[Episode 3,060, Steps 612,000] Episode Reward:  -767.626, Policy Loss:  -3.903, Training Steps: 19,125,  Elapsed Time: 00:01:46\n",
      "[Episode 3,080, Steps 616,000] Episode Reward:  -839.623, Policy Loss:  -6.756, Training Steps: 19,250,  Elapsed Time: 00:01:47\n",
      "[Episode 3,100, Steps 620,000] Episode Reward: -1023.697, Policy Loss:  -4.436, Training Steps: 19,375,  Elapsed Time: 00:01:48\n",
      "[Validation Episode Reward: [-130.67868709 -125.26236975 -129.6392769 ]] Average: -128.527\n",
      "[Episode 3,120, Steps 624,000] Episode Reward:  -728.192, Policy Loss:  -5.022, Training Steps: 19,500,  Elapsed Time: 00:01:49\n",
      "[Episode 3,140, Steps 628,000] Episode Reward:  -892.556, Policy Loss:  -4.514, Training Steps: 19,625,  Elapsed Time: 00:01:49\n",
      "[Episode 3,160, Steps 632,000] Episode Reward: -1193.705, Policy Loss:  -5.096, Training Steps: 19,750,  Elapsed Time: 00:01:50\n",
      "[Episode 3,180, Steps 636,000] Episode Reward:  -507.874, Policy Loss:  -7.380, Training Steps: 19,875,  Elapsed Time: 00:01:51\n",
      "[Episode 3,200, Steps 640,000] Episode Reward:  -907.519, Policy Loss:  -5.420, Training Steps: 20,000,  Elapsed Time: 00:01:51\n",
      "[Validation Episode Reward: [-131.41648221 -128.73044396 -260.6105922 ]] Average: -173.586\n",
      "[Episode 3,220, Steps 644,000] Episode Reward: -1009.746, Policy Loss:  -5.422, Training Steps: 20,125,  Elapsed Time: 00:01:52\n",
      "[Episode 3,240, Steps 648,000] Episode Reward: -1191.259, Policy Loss:  -3.591, Training Steps: 20,250,  Elapsed Time: 00:01:53\n",
      "[Episode 3,260, Steps 652,000] Episode Reward:  -382.108, Policy Loss:  -6.766, Training Steps: 20,375,  Elapsed Time: 00:01:53\n",
      "[Episode 3,280, Steps 656,000] Episode Reward:  -510.384, Policy Loss:  -6.766, Training Steps: 20,500,  Elapsed Time: 00:01:54\n",
      "[Episode 3,300, Steps 660,000] Episode Reward:  -629.344, Policy Loss:  -4.691, Training Steps: 20,625,  Elapsed Time: 00:01:55\n",
      "[Validation Episode Reward: [-131.13202721 -127.57526533 -348.83469246]] Average: -202.514\n",
      "[Episode 3,320, Steps 664,000] Episode Reward:  -774.965, Policy Loss:  -3.942, Training Steps: 20,750,  Elapsed Time: 00:01:56\n",
      "[Episode 3,340, Steps 668,000] Episode Reward:  -636.031, Policy Loss:  -6.887, Training Steps: 20,875,  Elapsed Time: 00:01:56\n",
      "[Episode 3,360, Steps 672,000] Episode Reward:  -935.331, Policy Loss:  -3.766, Training Steps: 21,000,  Elapsed Time: 00:01:57\n",
      "[Episode 3,380, Steps 676,000] Episode Reward:  -633.547, Policy Loss:  -5.504, Training Steps: 21,125,  Elapsed Time: 00:01:58\n",
      "[Episode 3,400, Steps 680,000] Episode Reward:  -741.593, Policy Loss:  -5.031, Training Steps: 21,250,  Elapsed Time: 00:01:58\n",
      "[Validation Episode Reward: [-362.1488899  -120.24848787 -117.31541254]] Average: -199.904\n",
      "[Episode 3,420, Steps 684,000] Episode Reward:  -504.051, Policy Loss:  -6.243, Training Steps: 21,375,  Elapsed Time: 00:01:59\n",
      "[Episode 3,440, Steps 688,000] Episode Reward:  -984.777, Policy Loss:  -5.213, Training Steps: 21,500,  Elapsed Time: 00:02:00\n",
      "[Episode 3,460, Steps 692,000] Episode Reward: -1156.716, Policy Loss:  -5.849, Training Steps: 21,625,  Elapsed Time: 00:02:00\n",
      "[Episode 3,480, Steps 696,000] Episode Reward:  -888.347, Policy Loss:  -4.942, Training Steps: 21,750,  Elapsed Time: 00:02:01\n",
      "[Episode 3,500, Steps 700,000] Episode Reward:  -849.585, Policy Loss:  -7.012, Training Steps: 21,875,  Elapsed Time: 00:02:02\n",
      "[Validation Episode Reward: [-268.00331307 -122.46166728 -119.70477014]] Average: -170.057\n",
      "[Episode 3,520, Steps 704,000] Episode Reward:  -753.359, Policy Loss:  -5.077, Training Steps: 22,000,  Elapsed Time: 00:02:03\n",
      "[Episode 3,540, Steps 708,000] Episode Reward:  -734.302, Policy Loss:  -6.247, Training Steps: 22,125,  Elapsed Time: 00:02:03\n",
      "[Episode 3,560, Steps 712,000] Episode Reward:  -764.211, Policy Loss:  -5.296, Training Steps: 22,250,  Elapsed Time: 00:02:04\n",
      "[Episode 3,580, Steps 716,000] Episode Reward:  -677.807, Policy Loss:  -7.350, Training Steps: 22,375,  Elapsed Time: 00:02:05\n",
      "[Episode 3,600, Steps 720,000] Episode Reward:  -780.001, Policy Loss:  -4.402, Training Steps: 22,500,  Elapsed Time: 00:02:05\n",
      "[Validation Episode Reward: [  -1.76781232 -114.68810225 -127.9776941 ]] Average: -81.478\n",
      "[Episode 3,620, Steps 724,000] Episode Reward:  -515.415, Policy Loss:  -6.280, Training Steps: 22,625,  Elapsed Time: 00:02:06\n",
      "[Episode 3,640, Steps 728,000] Episode Reward:  -743.485, Policy Loss:  -5.535, Training Steps: 22,750,  Elapsed Time: 00:02:07\n",
      "[Episode 3,660, Steps 732,000] Episode Reward: -1015.426, Policy Loss:  -3.309, Training Steps: 22,875,  Elapsed Time: 00:02:07\n",
      "[Episode 3,680, Steps 736,000] Episode Reward:  -497.592, Policy Loss:  -7.525, Training Steps: 23,000,  Elapsed Time: 00:02:08\n",
      "[Episode 3,700, Steps 740,000] Episode Reward:  -626.355, Policy Loss:  -6.667, Training Steps: 23,125,  Elapsed Time: 00:02:09\n",
      "[Validation Episode Reward: [  -1.18325121 -229.79209228 -129.94232991]] Average: -120.306\n",
      "[Episode 3,720, Steps 744,000] Episode Reward:  -865.999, Policy Loss:  -6.700, Training Steps: 23,250,  Elapsed Time: 00:02:09\n",
      "[Episode 3,740, Steps 748,000] Episode Reward:  -905.905, Policy Loss:  -4.037, Training Steps: 23,375,  Elapsed Time: 00:02:10\n",
      "[Episode 3,760, Steps 752,000] Episode Reward:  -653.243, Policy Loss:  -5.112, Training Steps: 23,500,  Elapsed Time: 00:02:11\n",
      "[Episode 3,780, Steps 756,000] Episode Reward:  -493.489, Policy Loss:  -7.247, Training Steps: 23,625,  Elapsed Time: 00:02:12\n",
      "[Episode 3,800, Steps 760,000] Episode Reward:  -603.629, Policy Loss:  -3.733, Training Steps: 23,750,  Elapsed Time: 00:02:12\n",
      "[Validation Episode Reward: [  -2.10388397 -117.80461143 -261.26496287]] Average: -127.058\n",
      "[Episode 3,820, Steps 764,000] Episode Reward:  -619.111, Policy Loss:  -4.276, Training Steps: 23,875,  Elapsed Time: 00:02:13\n",
      "[Episode 3,840, Steps 768,000] Episode Reward:  -625.735, Policy Loss:  -4.992, Training Steps: 24,000,  Elapsed Time: 00:02:14\n",
      "[Episode 3,860, Steps 772,000] Episode Reward: -1056.769, Policy Loss:  -5.389, Training Steps: 24,125,  Elapsed Time: 00:02:14\n",
      "[Episode 3,880, Steps 776,000] Episode Reward:  -623.086, Policy Loss:  -6.646, Training Steps: 24,250,  Elapsed Time: 00:02:15\n",
      "[Episode 3,900, Steps 780,000] Episode Reward:  -735.992, Policy Loss:  -3.450, Training Steps: 24,375,  Elapsed Time: 00:02:16\n",
      "[Validation Episode Reward: [   -3.95855495 -1054.6633516   -117.13805362]] Average: -391.920\n",
      "[Episode 3,920, Steps 784,000] Episode Reward:  -613.677, Policy Loss:  -4.847, Training Steps: 24,500,  Elapsed Time: 00:02:16\n",
      "[Episode 3,940, Steps 788,000] Episode Reward:  -959.466, Policy Loss:  -4.889, Training Steps: 24,625,  Elapsed Time: 00:02:17\n",
      "[Episode 3,960, Steps 792,000] Episode Reward:  -819.124, Policy Loss:  -6.518, Training Steps: 24,750,  Elapsed Time: 00:02:18\n",
      "[Episode 3,980, Steps 796,000] Episode Reward:  -686.789, Policy Loss:  -5.262, Training Steps: 24,875,  Elapsed Time: 00:02:19\n",
      "[Episode 4,000, Steps 800,000] Episode Reward:  -741.805, Policy Loss:  -3.624, Training Steps: 25,000,  Elapsed Time: 00:02:19\n",
      "[Validation Episode Reward: [  -1.20643197 -129.42533894 -126.85342125]] Average: -85.828\n",
      "[Episode 4,020, Steps 804,000] Episode Reward:  -873.973, Policy Loss:  -4.161, Training Steps: 25,125,  Elapsed Time: 00:02:20\n",
      "[Episode 4,040, Steps 808,000] Episode Reward:  -827.100, Policy Loss:  -6.981, Training Steps: 25,250,  Elapsed Time: 00:02:21\n",
      "[Episode 4,060, Steps 812,000] Episode Reward:  -707.589, Policy Loss:  -5.746, Training Steps: 25,375,  Elapsed Time: 00:02:21\n",
      "[Episode 4,080, Steps 816,000] Episode Reward:  -637.674, Policy Loss:  -6.376, Training Steps: 25,500,  Elapsed Time: 00:02:22\n",
      "[Episode 4,100, Steps 820,000] Episode Reward:  -907.521, Policy Loss:  -4.589, Training Steps: 25,625,  Elapsed Time: 00:02:23\n",
      "[Validation Episode Reward: [-241.7330544  -239.96255662 -124.80327475]] Average: -202.166\n",
      "[Episode 4,120, Steps 824,000] Episode Reward:  -724.107, Policy Loss:  -4.962, Training Steps: 25,750,  Elapsed Time: 00:02:23\n",
      "[Episode 4,140, Steps 828,000] Episode Reward:  -638.997, Policy Loss:  -7.569, Training Steps: 25,875,  Elapsed Time: 00:02:24\n",
      "[Episode 4,160, Steps 832,000] Episode Reward:  -505.591, Policy Loss:  -4.551, Training Steps: 26,000,  Elapsed Time: 00:02:25\n",
      "[Episode 4,180, Steps 836,000] Episode Reward:  -958.594, Policy Loss:  -6.544, Training Steps: 26,125,  Elapsed Time: 00:02:26\n",
      "[Episode 4,200, Steps 840,000] Episode Reward:  -630.768, Policy Loss:  -5.784, Training Steps: 26,250,  Elapsed Time: 00:02:26\n",
      "[Validation Episode Reward: [-120.85045853 -119.79059044 -119.25196046]] Average: -119.964\n",
      "[Episode 4,220, Steps 844,000] Episode Reward:  -775.732, Policy Loss:  -5.496, Training Steps: 26,375,  Elapsed Time: 00:02:27\n",
      "[Episode 4,240, Steps 848,000] Episode Reward:  -639.483, Policy Loss:  -7.954, Training Steps: 26,500,  Elapsed Time: 00:02:28\n",
      "[Episode 4,260, Steps 852,000] Episode Reward:  -867.526, Policy Loss:  -5.544, Training Steps: 26,625,  Elapsed Time: 00:02:28\n",
      "[Episode 4,280, Steps 856,000] Episode Reward:  -387.980, Policy Loss:  -3.812, Training Steps: 26,750,  Elapsed Time: 00:02:29\n",
      "[Episode 4,300, Steps 860,000] Episode Reward:  -256.817, Policy Loss:  -6.318, Training Steps: 26,875,  Elapsed Time: 00:02:30\n",
      "[Validation Episode Reward: [-119.83570092 -131.00101898   -3.38909724]] Average: -84.742\n",
      "[Episode 4,320, Steps 864,000] Episode Reward:  -631.331, Policy Loss:  -4.375, Training Steps: 27,000,  Elapsed Time: 00:02:30\n",
      "[Episode 4,340, Steps 868,000] Episode Reward:  -512.152, Policy Loss:  -1.967, Training Steps: 27,125,  Elapsed Time: 00:02:31\n",
      "[Episode 4,360, Steps 872,000] Episode Reward:  -716.232, Policy Loss:  -5.652, Training Steps: 27,250,  Elapsed Time: 00:02:32\n",
      "[Episode 4,380, Steps 876,000] Episode Reward:  -759.196, Policy Loss:  -6.708, Training Steps: 27,375,  Elapsed Time: 00:02:33\n",
      "[Episode 4,400, Steps 880,000] Episode Reward:  -754.349, Policy Loss:  -6.107, Training Steps: 27,500,  Elapsed Time: 00:02:33\n",
      "[Validation Episode Reward: [-238.61255002 -117.6592225  -117.47352671]] Average: -157.915\n",
      "[Episode 4,420, Steps 884,000] Episode Reward:  -385.526, Policy Loss:  -5.636, Training Steps: 27,625,  Elapsed Time: 00:02:34\n",
      "[Episode 4,440, Steps 888,000] Episode Reward:  -844.822, Policy Loss:  -5.786, Training Steps: 27,750,  Elapsed Time: 00:02:35\n",
      "[Episode 4,460, Steps 892,000] Episode Reward:  -499.328, Policy Loss:  -6.559, Training Steps: 27,875,  Elapsed Time: 00:02:35\n",
      "[Episode 4,480, Steps 896,000] Episode Reward:  -748.113, Policy Loss:  -4.201, Training Steps: 28,000,  Elapsed Time: 00:02:36\n",
      "[Episode 4,500, Steps 900,000] Episode Reward:  -901.483, Policy Loss:  -6.045, Training Steps: 28,125,  Elapsed Time: 00:02:37\n",
      "[Validation Episode Reward: [-124.19663327 -131.35959385   -1.54677121]] Average: -85.701\n",
      "[Episode 4,520, Steps 904,000] Episode Reward:  -915.064, Policy Loss:  -8.137, Training Steps: 28,250,  Elapsed Time: 00:02:37\n",
      "[Episode 4,540, Steps 908,000] Episode Reward:  -748.849, Policy Loss:  -6.764, Training Steps: 28,375,  Elapsed Time: 00:02:38\n",
      "[Episode 4,560, Steps 912,000] Episode Reward:  -603.495, Policy Loss:  -5.231, Training Steps: 28,500,  Elapsed Time: 00:02:39\n",
      "[Episode 4,580, Steps 916,000] Episode Reward:  -987.134, Policy Loss:  -6.762, Training Steps: 28,625,  Elapsed Time: 00:02:39\n",
      "[Episode 4,600, Steps 920,000] Episode Reward:  -509.831, Policy Loss:  -5.769, Training Steps: 28,750,  Elapsed Time: 00:02:40\n",
      "[Validation Episode Reward: [-238.01994747 -129.95855823 -245.39616783]] Average: -204.458\n",
      "[Episode 4,620, Steps 924,000] Episode Reward:  -367.236, Policy Loss:  -6.263, Training Steps: 28,875,  Elapsed Time: 00:02:41\n",
      "[Episode 4,640, Steps 928,000] Episode Reward:  -847.323, Policy Loss:  -6.517, Training Steps: 29,000,  Elapsed Time: 00:02:42\n",
      "[Episode 4,660, Steps 932,000] Episode Reward:  -632.766, Policy Loss:  -6.298, Training Steps: 29,125,  Elapsed Time: 00:02:42\n",
      "[Episode 4,680, Steps 936,000] Episode Reward:  -854.267, Policy Loss:  -4.054, Training Steps: 29,250,  Elapsed Time: 00:02:43\n",
      "[Episode 4,700, Steps 940,000] Episode Reward:  -503.362, Policy Loss:  -5.664, Training Steps: 29,375,  Elapsed Time: 00:02:44\n",
      "[Validation Episode Reward: [-116.08673847   -1.81229928 -124.157773  ]] Average: -80.686\n",
      "[Episode 4,720, Steps 944,000] Episode Reward:  -617.725, Policy Loss:  -6.119, Training Steps: 29,500,  Elapsed Time: 00:02:44\n",
      "[Episode 4,740, Steps 948,000] Episode Reward:  -520.511, Policy Loss:  -4.608, Training Steps: 29,625,  Elapsed Time: 00:02:45\n",
      "[Episode 4,760, Steps 952,000] Episode Reward: -1074.119, Policy Loss:  -5.809, Training Steps: 29,750,  Elapsed Time: 00:02:46\n",
      "[Episode 4,780, Steps 956,000] Episode Reward:  -980.880, Policy Loss:  -5.779, Training Steps: 29,875,  Elapsed Time: 00:02:46\n",
      "[Episode 4,800, Steps 960,000] Episode Reward:  -978.479, Policy Loss:  -6.135, Training Steps: 30,000,  Elapsed Time: 00:02:47\n",
      "[Validation Episode Reward: [-125.7793976 -345.6125428 -368.2433568]] Average: -279.878\n",
      "[Episode 4,820, Steps 964,000] Episode Reward:  -674.943, Policy Loss:  -3.581, Training Steps: 30,125,  Elapsed Time: 00:02:48\n",
      "[Episode 4,840, Steps 968,000] Episode Reward:  -629.833, Policy Loss:  -5.920, Training Steps: 30,250,  Elapsed Time: 00:02:49\n",
      "[Episode 4,860, Steps 972,000] Episode Reward:  -818.036, Policy Loss:  -4.884, Training Steps: 30,375,  Elapsed Time: 00:02:49\n",
      "[Episode 4,880, Steps 976,000] Episode Reward:  -749.791, Policy Loss:  -4.743, Training Steps: 30,500,  Elapsed Time: 00:02:50\n",
      "[Episode 4,900, Steps 980,000] Episode Reward:  -628.483, Policy Loss:  -5.212, Training Steps: 30,625,  Elapsed Time: 00:02:51\n",
      "[Validation Episode Reward: [-118.11576276 -128.05154051 -124.72721939]] Average: -123.632\n",
      "[Episode 4,920, Steps 984,000] Episode Reward:  -775.484, Policy Loss:  -5.215, Training Steps: 30,750,  Elapsed Time: 00:02:51\n",
      "[Episode 4,940, Steps 988,000] Episode Reward:  -911.370, Policy Loss:  -5.243, Training Steps: 30,875,  Elapsed Time: 00:02:52\n",
      "[Episode 4,960, Steps 992,000] Episode Reward: -1076.277, Policy Loss:  -4.020, Training Steps: 31,000,  Elapsed Time: 00:02:53\n",
      "[Episode 4,980, Steps 996,000] Episode Reward:  -391.622, Policy Loss:  -8.667, Training Steps: 31,125,  Elapsed Time: 00:02:53\n",
      "[Episode 5,000, Steps 1,000,000] Episode Reward:  -725.074, Policy Loss:  -4.983, Training Steps: 31,250,  Elapsed Time: 00:02:54\n",
      "[Validation Episode Reward: [-391.14318384 -124.83401289 -236.73239649]] Average: -250.903\n",
      "[Episode 5,020, Steps 1,004,000] Episode Reward:  -580.584, Policy Loss:  -5.690, Training Steps: 31,375,  Elapsed Time: 00:02:55\n",
      "[Episode 5,040, Steps 1,008,000] Episode Reward:  -772.162, Policy Loss:  -4.064, Training Steps: 31,500,  Elapsed Time: 00:02:56\n",
      "[Episode 5,060, Steps 1,012,000] Episode Reward:  -870.573, Policy Loss:  -4.272, Training Steps: 31,625,  Elapsed Time: 00:02:56\n",
      "[Episode 5,080, Steps 1,016,000] Episode Reward:  -894.859, Policy Loss:  -6.024, Training Steps: 31,750,  Elapsed Time: 00:02:57\n",
      "[Episode 5,100, Steps 1,020,000] Episode Reward:  -385.966, Policy Loss:  -5.854, Training Steps: 31,875,  Elapsed Time: 00:02:58\n",
      "[Validation Episode Reward: [-125.09452108 -237.66334307 -128.56264784]] Average: -163.774\n",
      "[Episode 5,120, Steps 1,024,000] Episode Reward:  -754.139, Policy Loss:  -6.306, Training Steps: 32,000,  Elapsed Time: 00:02:58\n",
      "[Episode 5,140, Steps 1,028,000] Episode Reward:  -853.901, Policy Loss:  -6.854, Training Steps: 32,125,  Elapsed Time: 00:02:59\n",
      "[Episode 5,160, Steps 1,032,000] Episode Reward:  -872.889, Policy Loss:  -7.118, Training Steps: 32,250,  Elapsed Time: 00:03:00\n",
      "[Episode 5,180, Steps 1,036,000] Episode Reward:  -512.781, Policy Loss:  -5.660, Training Steps: 32,375,  Elapsed Time: 00:03:00\n",
      "[Episode 5,200, Steps 1,040,000] Episode Reward:  -529.023, Policy Loss:  -6.284, Training Steps: 32,500,  Elapsed Time: 00:03:01\n",
      "[Validation Episode Reward: [-329.13459474 -233.76911549 -232.97605596]] Average: -265.293\n",
      "[Episode 5,220, Steps 1,044,000] Episode Reward:  -630.657, Policy Loss:  -6.541, Training Steps: 32,625,  Elapsed Time: 00:03:02\n",
      "[Episode 5,240, Steps 1,048,000] Episode Reward:  -522.397, Policy Loss:  -4.919, Training Steps: 32,750,  Elapsed Time: 00:03:03\n",
      "[Episode 5,260, Steps 1,052,000] Episode Reward:  -502.231, Policy Loss:  -7.318, Training Steps: 32,875,  Elapsed Time: 00:03:03\n",
      "[Episode 5,280, Steps 1,056,000] Episode Reward:  -878.784, Policy Loss:  -5.196, Training Steps: 33,000,  Elapsed Time: 00:03:04\n",
      "[Episode 5,300, Steps 1,060,000] Episode Reward:  -744.374, Policy Loss:  -5.668, Training Steps: 33,125,  Elapsed Time: 00:03:05\n",
      "[Validation Episode Reward: [  -2.26689432 -127.45944869 -238.40010875]] Average: -122.709\n",
      "[Episode 5,320, Steps 1,064,000] Episode Reward:  -632.595, Policy Loss:  -5.408, Training Steps: 33,250,  Elapsed Time: 00:03:05\n",
      "[Episode 5,340, Steps 1,068,000] Episode Reward:  -560.781, Policy Loss:  -4.626, Training Steps: 33,375,  Elapsed Time: 00:03:06\n",
      "[Episode 5,360, Steps 1,072,000] Episode Reward:  -972.118, Policy Loss:  -5.687, Training Steps: 33,500,  Elapsed Time: 00:03:07\n",
      "[Episode 5,380, Steps 1,076,000] Episode Reward:  -995.023, Policy Loss:  -4.407, Training Steps: 33,625,  Elapsed Time: 00:03:07\n",
      "[Episode 5,400, Steps 1,080,000] Episode Reward:  -622.809, Policy Loss:  -5.845, Training Steps: 33,750,  Elapsed Time: 00:03:08\n",
      "[Validation Episode Reward: [-119.0111867    -2.63993639 -129.40929019]] Average: -83.687\n",
      "[Episode 5,420, Steps 1,084,000] Episode Reward:  -501.551, Policy Loss:  -6.737, Training Steps: 33,875,  Elapsed Time: 00:03:09\n",
      "[Episode 5,440, Steps 1,088,000] Episode Reward:  -855.923, Policy Loss:  -6.389, Training Steps: 34,000,  Elapsed Time: 00:03:10\n",
      "[Episode 5,460, Steps 1,092,000] Episode Reward:  -812.106, Policy Loss:  -6.637, Training Steps: 34,125,  Elapsed Time: 00:03:10\n",
      "[Episode 5,480, Steps 1,096,000] Episode Reward:  -758.280, Policy Loss:  -6.023, Training Steps: 34,250,  Elapsed Time: 00:03:11\n",
      "[Episode 5,500, Steps 1,100,000] Episode Reward:  -978.589, Policy Loss:  -4.811, Training Steps: 34,375,  Elapsed Time: 00:03:12\n",
      "[Validation Episode Reward: [-118.57330714 -122.34182971   -3.88806814]] Average: -81.601\n",
      "[Episode 5,520, Steps 1,104,000] Episode Reward:  -501.395, Policy Loss:  -7.198, Training Steps: 34,500,  Elapsed Time: 00:03:12\n",
      "[Episode 5,540, Steps 1,108,000] Episode Reward:  -830.064, Policy Loss:  -6.405, Training Steps: 34,625,  Elapsed Time: 00:03:13\n",
      "[Episode 5,560, Steps 1,112,000] Episode Reward:  -756.247, Policy Loss:  -6.099, Training Steps: 34,750,  Elapsed Time: 00:03:14\n",
      "[Episode 5,580, Steps 1,116,000] Episode Reward:  -530.868, Policy Loss:  -6.656, Training Steps: 34,875,  Elapsed Time: 00:03:14\n",
      "[Episode 5,600, Steps 1,120,000] Episode Reward:  -537.548, Policy Loss:  -5.716, Training Steps: 35,000,  Elapsed Time: 00:03:15\n",
      "[Validation Episode Reward: [-233.56093486 -130.3019525  -126.20984902]] Average: -163.358\n",
      "[Episode 5,620, Steps 1,124,000] Episode Reward:  -775.724, Policy Loss:  -4.041, Training Steps: 35,125,  Elapsed Time: 00:03:16\n",
      "[Episode 5,640, Steps 1,128,000] Episode Reward: -1000.771, Policy Loss:  -4.432, Training Steps: 35,250,  Elapsed Time: 00:03:16\n",
      "[Episode 5,660, Steps 1,132,000] Episode Reward:  -617.809, Policy Loss:  -6.887, Training Steps: 35,375,  Elapsed Time: 00:03:17\n",
      "[Episode 5,680, Steps 1,136,000] Episode Reward:  -876.719, Policy Loss:  -6.125, Training Steps: 35,500,  Elapsed Time: 00:03:18\n",
      "[Episode 5,700, Steps 1,140,000] Episode Reward: -1066.638, Policy Loss:  -5.461, Training Steps: 35,625,  Elapsed Time: 00:03:18\n",
      "[Validation Episode Reward: [-118.40714159 -117.32503372 -127.43920559]] Average: -121.057\n",
      "[Episode 5,720, Steps 1,144,000] Episode Reward:  -504.339, Policy Loss:  -7.480, Training Steps: 35,750,  Elapsed Time: 00:03:19\n",
      "[Episode 5,740, Steps 1,148,000] Episode Reward:  -901.924, Policy Loss:  -6.125, Training Steps: 35,875,  Elapsed Time: 00:03:20\n",
      "[Episode 5,760, Steps 1,152,000] Episode Reward:  -834.410, Policy Loss:  -4.559, Training Steps: 36,000,  Elapsed Time: 00:03:21\n",
      "[Episode 5,780, Steps 1,156,000] Episode Reward:  -758.267, Policy Loss:  -5.776, Training Steps: 36,125,  Elapsed Time: 00:03:21\n",
      "[Episode 5,800, Steps 1,160,000] Episode Reward: -1004.831, Policy Loss:  -4.990, Training Steps: 36,250,  Elapsed Time: 00:03:22\n",
      "[Validation Episode Reward: [-121.89526807   -1.33691661 -127.53842965]] Average: -83.590\n",
      "[Episode 5,820, Steps 1,164,000] Episode Reward:  -638.662, Policy Loss:  -6.102, Training Steps: 36,375,  Elapsed Time: 00:03:23\n",
      "[Episode 5,840, Steps 1,168,000] Episode Reward:  -767.803, Policy Loss:  -5.474, Training Steps: 36,500,  Elapsed Time: 00:03:23\n",
      "[Episode 5,860, Steps 1,172,000] Episode Reward:  -652.600, Policy Loss:  -6.247, Training Steps: 36,625,  Elapsed Time: 00:03:24\n",
      "[Episode 5,880, Steps 1,176,000] Episode Reward:  -637.862, Policy Loss:  -2.937, Training Steps: 36,750,  Elapsed Time: 00:03:25\n",
      "[Episode 5,900, Steps 1,180,000] Episode Reward:  -379.317, Policy Loss:  -4.296, Training Steps: 36,875,  Elapsed Time: 00:03:25\n",
      "[Validation Episode Reward: [-124.27676864 -116.1388924  -127.22559574]] Average: -122.547\n",
      "[Episode 5,920, Steps 1,184,000] Episode Reward:  -633.877, Policy Loss:  -4.723, Training Steps: 37,000,  Elapsed Time: 00:03:26\n",
      "[Episode 5,940, Steps 1,188,000] Episode Reward:  -613.563, Policy Loss:  -5.895, Training Steps: 37,125,  Elapsed Time: 00:03:27\n",
      "[Episode 5,960, Steps 1,192,000] Episode Reward:  -612.862, Policy Loss:  -5.484, Training Steps: 37,250,  Elapsed Time: 00:03:28\n",
      "[Episode 5,980, Steps 1,196,000] Episode Reward:  -984.642, Policy Loss:  -4.452, Training Steps: 37,375,  Elapsed Time: 00:03:28\n",
      "[Episode 6,000, Steps 1,200,000] Episode Reward:  -751.030, Policy Loss:  -3.971, Training Steps: 37,500,  Elapsed Time: 00:03:29\n",
      "[Validation Episode Reward: [-122.16044802 -243.78453235 -230.40103517]] Average: -198.782\n",
      "[Episode 6,020, Steps 1,204,000] Episode Reward:  -638.042, Policy Loss:  -5.472, Training Steps: 37,625,  Elapsed Time: 00:03:30\n",
      "[Episode 6,040, Steps 1,208,000] Episode Reward:  -854.154, Policy Loss:  -6.114, Training Steps: 37,750,  Elapsed Time: 00:03:30\n",
      "[Episode 6,060, Steps 1,212,000] Episode Reward:  -500.484, Policy Loss:  -7.793, Training Steps: 37,875,  Elapsed Time: 00:03:31\n",
      "[Episode 6,080, Steps 1,216,000] Episode Reward:  -888.354, Policy Loss:  -5.779, Training Steps: 38,000,  Elapsed Time: 00:03:32\n",
      "[Episode 6,100, Steps 1,220,000] Episode Reward: -1111.479, Policy Loss:  -5.118, Training Steps: 38,125,  Elapsed Time: 00:03:32\n",
      "[Validation Episode Reward: [-121.82678188 -296.84418411 -127.40738568]] Average: -182.026\n",
      "[Episode 6,120, Steps 1,224,000] Episode Reward:  -753.069, Policy Loss:  -6.833, Training Steps: 38,250,  Elapsed Time: 00:03:33\n",
      "[Episode 6,140, Steps 1,228,000] Episode Reward:  -587.029, Policy Loss:  -7.498, Training Steps: 38,375,  Elapsed Time: 00:03:34\n",
      "[Episode 6,160, Steps 1,232,000] Episode Reward:  -958.338, Policy Loss:  -8.627, Training Steps: 38,500,  Elapsed Time: 00:03:35\n",
      "[Episode 6,180, Steps 1,236,000] Episode Reward:  -849.085, Policy Loss:  -6.524, Training Steps: 38,625,  Elapsed Time: 00:03:35\n",
      "[Episode 6,200, Steps 1,240,000] Episode Reward:  -893.847, Policy Loss:  -6.223, Training Steps: 38,750,  Elapsed Time: 00:03:36\n",
      "[Validation Episode Reward: [-119.92802545 -122.07711174 -346.72902928]] Average: -196.245\n",
      "[Episode 6,220, Steps 1,244,000] Episode Reward:  -736.564, Policy Loss:  -7.981, Training Steps: 38,875,  Elapsed Time: 00:03:37\n",
      "[Episode 6,240, Steps 1,248,000] Episode Reward:  -726.395, Policy Loss:  -7.237, Training Steps: 39,000,  Elapsed Time: 00:03:37\n",
      "[Episode 6,260, Steps 1,252,000] Episode Reward:  -777.333, Policy Loss:  -6.286, Training Steps: 39,125,  Elapsed Time: 00:03:38\n",
      "[Episode 6,280, Steps 1,256,000] Episode Reward:  -837.968, Policy Loss:  -6.869, Training Steps: 39,250,  Elapsed Time: 00:03:39\n",
      "[Episode 6,300, Steps 1,260,000] Episode Reward:  -635.583, Policy Loss:  -6.029, Training Steps: 39,375,  Elapsed Time: 00:03:39\n",
      "[Validation Episode Reward: [-124.29386722   -3.00101241 -233.00058299]] Average: -120.098\n",
      "[Episode 6,320, Steps 1,264,000] Episode Reward:  -711.085, Policy Loss:  -5.736, Training Steps: 39,500,  Elapsed Time: 00:03:40\n",
      "[Episode 6,340, Steps 1,268,000] Episode Reward:  -748.394, Policy Loss:  -6.721, Training Steps: 39,625,  Elapsed Time: 00:03:41\n",
      "[Episode 6,360, Steps 1,272,000] Episode Reward:  -635.118, Policy Loss:  -7.239, Training Steps: 39,750,  Elapsed Time: 00:03:41\n",
      "[Episode 6,380, Steps 1,276,000] Episode Reward: -1264.968, Policy Loss:  -5.728, Training Steps: 39,875,  Elapsed Time: 00:03:42\n",
      "[Episode 6,400, Steps 1,280,000] Episode Reward: -1007.335, Policy Loss:  -7.009, Training Steps: 40,000,  Elapsed Time: 00:03:43\n",
      "[Validation Episode Reward: [-317.54909269 -126.48557201 -128.0508772 ]] Average: -190.695\n",
      "[Episode 6,420, Steps 1,284,000] Episode Reward:  -577.246, Policy Loss:  -4.722, Training Steps: 40,125,  Elapsed Time: 00:03:44\n",
      "[Episode 6,440, Steps 1,288,000] Episode Reward:  -860.305, Policy Loss:  -5.516, Training Steps: 40,250,  Elapsed Time: 00:03:44\n",
      "[Episode 6,460, Steps 1,292,000] Episode Reward: -1000.961, Policy Loss:  -7.271, Training Steps: 40,375,  Elapsed Time: 00:03:45\n",
      "[Episode 6,480, Steps 1,296,000] Episode Reward:  -616.102, Policy Loss:  -4.177, Training Steps: 40,500,  Elapsed Time: 00:03:46\n",
      "[Episode 6,500, Steps 1,300,000] Episode Reward:  -386.205, Policy Loss:  -5.680, Training Steps: 40,625,  Elapsed Time: 00:03:46\n",
      "[Validation Episode Reward: [-230.68208077 -122.8583053  -232.58151429]] Average: -195.374\n",
      "[Episode 6,520, Steps 1,304,000] Episode Reward:  -909.622, Policy Loss:  -7.260, Training Steps: 40,750,  Elapsed Time: 00:03:47\n",
      "[Episode 6,540, Steps 1,308,000] Episode Reward:  -600.545, Policy Loss:  -6.528, Training Steps: 40,875,  Elapsed Time: 00:03:48\n",
      "[Episode 6,560, Steps 1,312,000] Episode Reward:  -534.811, Policy Loss:  -7.804, Training Steps: 41,000,  Elapsed Time: 00:03:48\n",
      "[Episode 6,580, Steps 1,316,000] Episode Reward:  -787.647, Policy Loss:  -4.039, Training Steps: 41,125,  Elapsed Time: 00:03:49\n",
      "[Episode 6,600, Steps 1,320,000] Episode Reward:  -629.935, Policy Loss:  -6.583, Training Steps: 41,250,  Elapsed Time: 00:03:50\n",
      "[Validation Episode Reward: [-128.97304612   -2.64091193 -237.9649391 ]] Average: -123.193\n",
      "[Episode 6,620, Steps 1,324,000] Episode Reward:  -508.440, Policy Loss:  -6.045, Training Steps: 41,375,  Elapsed Time: 00:03:51\n",
      "[Episode 6,640, Steps 1,328,000] Episode Reward:  -935.737, Policy Loss:  -7.608, Training Steps: 41,500,  Elapsed Time: 00:03:51\n",
      "[Episode 6,660, Steps 1,332,000] Episode Reward:  -959.663, Policy Loss:  -5.430, Training Steps: 41,625,  Elapsed Time: 00:03:52\n",
      "[Episode 6,680, Steps 1,336,000] Episode Reward:  -504.621, Policy Loss:  -5.509, Training Steps: 41,750,  Elapsed Time: 00:03:53\n",
      "[Episode 6,700, Steps 1,340,000] Episode Reward: -1146.499, Policy Loss:  -4.466, Training Steps: 41,875,  Elapsed Time: 00:03:53\n",
      "[Validation Episode Reward: [-117.3890412 -129.1309373 -125.5636513]] Average: -124.028\n",
      "[Episode 6,720, Steps 1,344,000] Episode Reward:  -732.844, Policy Loss:  -6.382, Training Steps: 42,000,  Elapsed Time: 00:03:54\n",
      "[Episode 6,740, Steps 1,348,000] Episode Reward:  -967.180, Policy Loss:  -6.948, Training Steps: 42,125,  Elapsed Time: 00:03:55\n",
      "[Episode 6,760, Steps 1,352,000] Episode Reward:  -510.094, Policy Loss:  -6.695, Training Steps: 42,250,  Elapsed Time: 00:03:55\n",
      "[Episode 6,780, Steps 1,356,000] Episode Reward:  -711.473, Policy Loss:  -7.367, Training Steps: 42,375,  Elapsed Time: 00:03:56\n",
      "[Episode 6,800, Steps 1,360,000] Episode Reward:  -918.700, Policy Loss:  -7.267, Training Steps: 42,500,  Elapsed Time: 00:03:57\n",
      "[Validation Episode Reward: [  -1.94781921 -120.62375662 -126.83879953]] Average: -83.137\n",
      "[Episode 6,820, Steps 1,364,000] Episode Reward:  -650.434, Policy Loss:  -5.596, Training Steps: 42,625,  Elapsed Time: 00:03:58\n",
      "[Episode 6,840, Steps 1,368,000] Episode Reward:  -873.202, Policy Loss:  -6.504, Training Steps: 42,750,  Elapsed Time: 00:03:58\n",
      "[Episode 6,860, Steps 1,372,000] Episode Reward:  -384.797, Policy Loss:  -5.061, Training Steps: 42,875,  Elapsed Time: 00:03:59\n",
      "[Episode 6,880, Steps 1,376,000] Episode Reward:  -637.160, Policy Loss:  -7.608, Training Steps: 43,000,  Elapsed Time: 00:04:00\n",
      "[Episode 6,900, Steps 1,380,000] Episode Reward:  -502.557, Policy Loss:  -7.232, Training Steps: 43,125,  Elapsed Time: 00:04:00\n",
      "[Validation Episode Reward: [-130.64754819 -126.94096315 -238.1210427 ]] Average: -165.237\n",
      "[Episode 6,920, Steps 1,384,000] Episode Reward:  -618.827, Policy Loss:  -5.741, Training Steps: 43,250,  Elapsed Time: 00:04:01\n",
      "[Episode 6,940, Steps 1,388,000] Episode Reward:  -755.065, Policy Loss:  -6.237, Training Steps: 43,375,  Elapsed Time: 00:04:02\n",
      "[Episode 6,960, Steps 1,392,000] Episode Reward: -1085.835, Policy Loss:  -6.437, Training Steps: 43,500,  Elapsed Time: 00:04:02\n",
      "[Episode 6,980, Steps 1,396,000] Episode Reward:  -740.878, Policy Loss:  -8.078, Training Steps: 43,625,  Elapsed Time: 00:04:03\n",
      "[Episode 7,000, Steps 1,400,000] Episode Reward:  -785.291, Policy Loss:  -6.697, Training Steps: 43,750,  Elapsed Time: 00:04:04\n",
      "[Validation Episode Reward: [-285.55430564 -326.17630608 -236.63999838]] Average: -282.790\n",
      "[Episode 7,020, Steps 1,404,000] Episode Reward:  -745.233, Policy Loss:  -6.939, Training Steps: 43,875,  Elapsed Time: 00:04:05\n",
      "[Episode 7,040, Steps 1,408,000] Episode Reward:  -554.111, Policy Loss:  -6.097, Training Steps: 44,000,  Elapsed Time: 00:04:05\n",
      "[Episode 7,060, Steps 1,412,000] Episode Reward:  -606.758, Policy Loss:  -7.605, Training Steps: 44,125,  Elapsed Time: 00:04:06\n",
      "[Episode 7,080, Steps 1,416,000] Episode Reward:  -633.257, Policy Loss:  -7.383, Training Steps: 44,250,  Elapsed Time: 00:04:07\n",
      "[Episode 7,100, Steps 1,420,000] Episode Reward:  -914.226, Policy Loss:  -4.984, Training Steps: 44,375,  Elapsed Time: 00:04:07\n",
      "[Validation Episode Reward: [-129.82055745 -123.930418   -117.1269551 ]] Average: -123.626\n",
      "[Episode 7,120, Steps 1,424,000] Episode Reward:  -944.322, Policy Loss:  -4.940, Training Steps: 44,500,  Elapsed Time: 00:04:08\n",
      "[Episode 7,140, Steps 1,428,000] Episode Reward:  -907.662, Policy Loss:  -3.345, Training Steps: 44,625,  Elapsed Time: 00:04:09\n",
      "[Episode 7,160, Steps 1,432,000] Episode Reward:  -912.819, Policy Loss:  -4.817, Training Steps: 44,750,  Elapsed Time: 00:04:09\n",
      "[Episode 7,180, Steps 1,436,000] Episode Reward:  -631.211, Policy Loss:  -6.007, Training Steps: 44,875,  Elapsed Time: 00:04:10\n",
      "[Episode 7,200, Steps 1,440,000] Episode Reward:  -854.656, Policy Loss:  -5.051, Training Steps: 45,000,  Elapsed Time: 00:04:11\n",
      "[Validation Episode Reward: [-236.52631058 -242.78918773 -123.03342424]] Average: -200.783\n",
      "[Episode 7,220, Steps 1,444,000] Episode Reward:  -420.505, Policy Loss:  -5.195, Training Steps: 45,125,  Elapsed Time: 00:04:11\n",
      "[Episode 7,240, Steps 1,448,000] Episode Reward:  -641.474, Policy Loss:  -6.043, Training Steps: 45,250,  Elapsed Time: 00:04:12\n",
      "[Episode 7,260, Steps 1,452,000] Episode Reward:  -742.687, Policy Loss:  -5.928, Training Steps: 45,375,  Elapsed Time: 00:04:13\n",
      "[Episode 7,280, Steps 1,456,000] Episode Reward:  -391.895, Policy Loss:  -7.692, Training Steps: 45,500,  Elapsed Time: 00:04:14\n",
      "[Episode 7,300, Steps 1,460,000] Episode Reward:  -971.310, Policy Loss:  -7.703, Training Steps: 45,625,  Elapsed Time: 00:04:14\n",
      "[Validation Episode Reward: [-117.45493778 -234.41922245 -125.5150704 ]] Average: -159.130\n",
      "[Episode 7,320, Steps 1,464,000] Episode Reward:  -646.539, Policy Loss:  -5.839, Training Steps: 45,750,  Elapsed Time: 00:04:15\n",
      "[Episode 7,340, Steps 1,468,000] Episode Reward:  -634.326, Policy Loss:  -7.199, Training Steps: 45,875,  Elapsed Time: 00:04:16\n",
      "[Episode 7,360, Steps 1,472,000] Episode Reward:  -496.851, Policy Loss:  -6.356, Training Steps: 46,000,  Elapsed Time: 00:04:16\n",
      "[Episode 7,380, Steps 1,476,000] Episode Reward:  -740.688, Policy Loss:  -7.562, Training Steps: 46,125,  Elapsed Time: 00:04:17\n",
      "[Episode 7,400, Steps 1,480,000] Episode Reward: -1108.729, Policy Loss:  -5.681, Training Steps: 46,250,  Elapsed Time: 00:04:18\n",
      "[Validation Episode Reward: [-115.91407128 -118.67917486 -347.67420631]] Average: -194.089\n",
      "[Episode 7,420, Steps 1,484,000] Episode Reward:  -506.596, Policy Loss:  -5.969, Training Steps: 46,375,  Elapsed Time: 00:04:18\n",
      "[Episode 7,440, Steps 1,488,000] Episode Reward:  -990.939, Policy Loss:  -5.035, Training Steps: 46,500,  Elapsed Time: 00:04:19\n",
      "[Episode 7,460, Steps 1,492,000] Episode Reward:  -487.268, Policy Loss:  -6.737, Training Steps: 46,625,  Elapsed Time: 00:04:20\n",
      "[Episode 7,480, Steps 1,496,000] Episode Reward:  -762.391, Policy Loss:  -4.252, Training Steps: 46,750,  Elapsed Time: 00:04:21\n",
      "[Episode 7,500, Steps 1,500,000] Episode Reward:  -633.646, Policy Loss:  -4.829, Training Steps: 46,875,  Elapsed Time: 00:04:21\n",
      "[Validation Episode Reward: [  -1.53950818   -1.74860291 -126.32680515]] Average: -43.205\n",
      "Solved in 1,500,000 steps (46,875 training steps)!\n",
      "Total Training End : 00:04:21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e59b7675c8a43c6ab1322d428fd9179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Training Steps</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>[TRAIN] Episode Reward</td><td>▂▁▄▄▂▆▄▃▄▄▆▇▇█▆▇▇▅█▆▇▇▇▅▇▅▆▅▅▇▆▆█▇█▇▅▆▇▅</td></tr><tr><td>[TRAIN] Policy Loss</td><td>▅▃██▃▅▅█▅▅▇▄▇▃▆▆▂▅▅▅▅▄▄▂▅▄▃▁▅▅▅▃▃▂▄▄▄▂▄▃</td></tr><tr><td>[TRAIN] avg_action</td><td>▄▆▅▁▇▆▄▅▁▄▅▆▇▇▄▅▅▅▄▆▄▃▆▄▄▅▂▄▅▄▅▅█▆▆▅▄▄▅▅</td></tr><tr><td>[TRAIN] avg_action_prob</td><td>▁▆▇█▆▅▆▇▇▆▇▇▇▇█▇▆▆▇▆█▆▇▆▆▇▇▇▇▅▆▇█▇▆█▇▆▆▇</td></tr><tr><td>[TRAIN] avg_mu_v</td><td>▃█▄▁█▃▁▆▁▅▅█▄▆▄▆▇▃▄▃▅▆▇▄▃▄▄▅▄▅▇▆█▆▆▅▄▆▅▆</td></tr><tr><td>[TRAIN] avg_std_v</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>[VALIDATION] Mean Episode Reward (3 Episodes)</td><td>▂▂▂▁▂▂▄▅▄▅▄▆█▆█▇██▇█████▇▇████████▇███▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Episode</td><td>7500</td></tr><tr><td>Training Steps</td><td>46875</td></tr><tr><td>[TRAIN] Episode Reward</td><td>-633.646</td></tr><tr><td>[TRAIN] Policy Loss</td><td>-4.82923</td></tr><tr><td>[TRAIN] avg_action</td><td>0.25</td></tr><tr><td>[TRAIN] avg_action_prob</td><td>0.17838</td></tr><tr><td>[TRAIN] avg_mu_v</td><td>-0.07534</td></tr><tr><td>[TRAIN] avg_std_v</td><td>2.0</td></tr><tr><td>[VALIDATION] Mean Episode Reward (3 Episodes)</td><td>-43.20497</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2023-11-08_09-01-13</strong> at: <a href='https://wandb.ai/gihwanjang/A2C_Pendulum-v1/runs/pkbk0y78' target=\"_blank\">https://wandb.ai/gihwanjang/A2C_Pendulum-v1/runs/pkbk0y78</a><br/> View job at <a href='https://wandb.ai/gihwanjang/A2C_Pendulum-v1/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzU3MjY1OQ==/version_details/v0' target=\"_blank\">https://wandb.ai/gihwanjang/A2C_Pendulum-v1/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMzU3MjY1OQ==/version_details/v0</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231108_090115-pkbk0y78/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympic",
   "language": "python",
   "name": "olympic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
